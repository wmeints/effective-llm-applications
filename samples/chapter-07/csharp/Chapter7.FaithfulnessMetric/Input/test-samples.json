[
  {
    "Question": "Does Semantic Kernel include a component that truncates older chat messages?",
    "Answer": "Yes, Semantic Kernel includes a component called \u0060ChatHistoryTruncationReducer\u0060 that truncates older chat messages to help manage the length of the chat history.",
    "Context": [
      {
        "Id": 38,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "1. First, we obtain an instance of the \u0060IChatCompletionService\u0060 from the kernel.\r\n2. After that, we ask the kernel to return a chat message based on our provided chat history.\r\n3. Finally, we print the content of the returned first chat message.\r\n\r\nWhere invoking a prompt returns just one result, the chat completion service returns an array of results. Don\u0027t get fooled by this array. You can\u0027t get multiple results anymore from Semantic Kernel, as many of the LLM providers no longer support returning various results for the same prompt. I expect they\u0027ll remove this feature soon after writing this.\r\n\r\nYou can serialize the chat history to JSON. So, you can store it in a database in serialized form and reuse it later to continue a conversation. However, I recommend you don\u0027t do this. For one, the chat history includes the system prompt, and you may want to update it later to provide extra instructions. Also, since the LLM executes the chat history content, you open the door to poisoning chat history data with malicious data that gets executed without your supervision.\r\n\r\nThe internal flow of the \u0060IChatCompletionService\u0060 is the same as it is for executing prompts (see [#s](#semantic-kernel-architecture)). The main difference is that you\u0027re dealing with a more extended prompt shaped like a conversation. Filters and functions work for chat-oriented applications as well as single prompts. However, the main goal of the \u0060IChatCompletionService\u0060 is to build chat-oriented applications.\r\n\r\nGenerating a response for a more extended conversation takes a lot of time because you need to transfer the whole chat history to the model, which will process it token by the token to produce a response. This is a slow process. It\u0027s good that Semantic Kernel supports streaming responses to prompts and chat scenarios.\r\n\r\nYou can ask \u0060IChatCompletionService\u0060 to stream the response by calling \u0060GetStreamingChatMessageContentsAsync\u0060 instead of \u0060GetChatMessageContentsAsync.\u0060 The streaming method returns an \u0060IAsyncEnumerable\u0060 object that you can iterate over using \u0060await foreach\u0060 as the following code demonstrates:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar chatCompletionService = kernel.Services\r\n\u00A0 \u00A0 .GetService\u003CIChatCompletionService\u003E();\r\n\r\nvar responseIterator = chatCompletionService!\r\n\u00A0 \u00A0 .GetStreamingChatMessageContentsAsync(chatHistory);\r\n\r\nawait foreach (var token in responseIterator)\r\n{\r\n    Console.Write(token.Content);\r\n}\r\n\u0060\u0060\u0060\r\n\r\nI\u0027ve included the complete source code in the [GitHub Repository][STREAMING_SAMPLE].\r\n\r\nNote that as conversations get longer, you\u0027ll run out of context space and need to decide how to keep the chat history within acceptable limits. You can truncate older messages from the history or summarize the earlier ones to keep a shorter version around.\r\n\r\nIn my experience, it works well to truncate older messages from the chat history until you have a consistent set of messages within your desired context window size. I\u0027ve never had to resort to summarizing the history. This is because, in chat scenarios, older messages become less relevant as the conversation continues. Sometimes, I\u0027ve seen the LLM flip out because I had unrelated content in the chat history. So, it\u0027s good to keep the chat history focused on the task at hand.\r\n\r\nSemantic Kernel includes two components to help you manage the length of the chat history:\r\n\r\n- \u0060ChatHistoryTruncationReducer\u0060 truncates older messages that aren\u0027t essential to the LLM.\r\n- \u0060ChatHistorySummarizationReducer\u0060 summarizes the non-essential chat messages.\r\nYou can learn more about using these history reducers in [the product documentation][HISTORY_REDUCERS]\r\n\r\nWorking with a chat history is more complex than a single prompt. But it\u0027s essential when you\u0027re building an assistant-like use case. Knowing you can mix and match prompts and chat in such a scenario is good. We\u0027ll explore this in #[s](#enhancing-llms-with-tools) when we look at using functions with Semantic Kernel.",
        "Embedding": []
      },
      {
        "Id": 20,
        "OriginalFileName": "Content\\03-getting-started-with-semantic-kernel.md",
        "Content": "{#getting-started-with-semantic-kernel} \r\n# Getting Started with Semantic Kernel\r\n\r\nIn [#s](#essential-llmops-knowledge), we covered essential LLMOps knowledge. In this\r\nchapter, we\u0027re finally going to talk about code. We\u0027ll start working with Semantic\r\nKernel. We\u0027ll cover the following topics:\r\n\r\n- Understanding Semantic Kernel\r\n- Setting up your development environment\r\n- Setting up a project with Semantic Kernel\r\n- Running a basic prompt with Semantic Kernel\r\n\r\nBy the end of the chapter, you\u0027ll have a solid foundation to start exploring how to\r\nwrite effective prompts and design applications using the design patterns in the\r\nupcoming chapters. Let\u0027s get started!\r\n\r\n## Understanding Semantic Kernel\r\n\r\nYou can build LLM-based applications by calling the LLM providers\u0027 API directly. Calling\r\nthe LLM provider directly through their API or package is the best way to access the\r\nlatest and greatest features. However, applications built this way can be challenging to\r\nmanage over the long term. You have to deal with a lot of boilerplate code, and if you\r\nneed to switch providers, you must rewrite a lot of code.\r\n\r\nA good framework will solve the boilerplate code for you while providing enough\r\nflexibility to implement your desired design patterns. Semantic Kernel is a flexible\r\nframework that provides enough value to make it worthwhile.\r\n\r\n### What is Semantic Kernel\r\n\r\nSemantic Kernel is an open-source framework from Microsoft that helps you build\r\nLLM-based applications. Its primary purpose is to abstract away the integration code\r\nneeded to talk to various LLM providers so you can easily switch. Microsoft focuses on\r\nproviding an enterprise-grade solution to build LLM-based applications. You can be sure\r\nthat the concepts from [#s](#essential-llmops-knowledge) are implemented in the\r\nframework.\r\n\r\nLike the other LLM-based application frameworks, the framework is relatively new and\r\nconstantly evolving. So, you should regularly update your code as the framework evolves.\r\nMicrosoft makes the updates relatively easy to apply as long as you don\u0027t use any\r\nexperimental features. Since Semantic Kernel reached version 1\u002B, Microsoft promises that\r\nthe non-experimental features are stable and won\u0027t receive breaking changes until 2.0\r\narrives.\r\n\r\nI\u0027ll point out which features are still experimental so you can make an informed choice\r\nregardless of whether you want to use them. You can also check the [product\r\ndocumentation][SEMANTIC_KERNEL_DOCS] to determine if a feature you want to use is\r\nexperimental.\r\n\r\nLet\u0027s look at the core concepts and the architecture of Semantic Kernel so you\u0027ll know\r\nwhat to expect when you start using it.\r\n\r\n{#core-concepts-and-architecture} \r\n### Core concepts and architecture\r\n\r\nThe architecture for Semantic Kernel is based on the concept of a Kernel that connects\r\nto functions, an LLM provider, and filters. [#s](#semantic-kernel-architecture) shows\r\nthe architecture of the framework.\r\n\r\n{#semantic-kernel-architecture} \r\n![Semantic Kernel Architecture](semantic-kernel-architecture.png)\r\n\r\nAfter trying multiple frameworks, I\u0027ve found that the Semantic Kernel fits quite well in\r\nthe mental model of an application built around an intelligent core that uses AI and\r\nlogic to fill in parts of the smart behavior. The kernel fills the core role, and the\r\nfunctions, filters, and other application logic provide the intelligent components\r\naround the core.\r\n\r\nThe mental model provided by Semantic Kernel allows you to isolate the AI portion of\r\nyour solution to a subsystem that can be replaced or upgraded without affecting\r\neverything else in the application. This is important because the architecture of\r\nLLM-based applications will change quickly over the next few years as we learn more\r\nabout LLMs\u0027 capabilities and limitations.\r\n\r\nIn [#s](#understanding-llms), we covered how LLMs generate a response. The kernel plays\r\nan essential role in this process. Let\u0027s explore what happens when you run a prompt",
        "Embedding": []
      },
      {
        "Id": 21,
        "OriginalFileName": "Content\\03-getting-started-with-semantic-kernel.md",
        "Content": "through the kernel.\r\n\r\n{#kernel-interactions} \r\n![Kernel interactions](kernel-interactions.png)\r\n\r\nWhen you ask Semantic Kernel to execute a prompt, it will determine what AI service to\r\nuse. An AI service in Semantic Kernel can be an LLM or another AI service that takes a\r\nprompt to generate a response. For example, with Semantic Kernel, you can:\r\n\r\n- Generate text and then translate it to audio (text-to-audio)\r\n- Generate an image from a prompt (text-to-image)\r\n- Process an image to extract meaning from it (image-to-text)\r\n- Process audio to text (audio-to-text)\r\n- Generate embeddings for a document (embedding-generation)\r\n\r\nYou should be aware that the other modalities (sound and images) are experimental at the\r\ntime of writing. I don\u0027t use them, and I will stick to text-only tasks for this book.\r\nBut it\u0027s good to know that you can do more than just process text.\r\n\r\nThe prompt is rendered after selecting the best AI service to execute a prompt. You can\r\nexecute plain prompts with Semantic Kernel. But you\u0027ll want to use templating whenever\r\nyou can. Templating allows you to store prompts in source control to version them and\r\nreuse them in multiple use cases or applications.\r\n\r\nWhen the prompt is rendered, it is sent to the LLM provider. In\r\n[#s](#llmops-rate-limits), we discussed that sometimes the LLM provider may not be\r\navailable, or you may have run out of quota. Semantic Kernel handles these failure modes\r\nfor you as much as possible.\r\n\r\nWhen the response comes back from the LLM, we need to process it. The monitoring data we\r\ndiscussed in [#s](#llmops-monitoring) is automatically generated so that you can export\r\nit through tools like Application Insights or OpenTelemetry. The generated telemetry\r\ncontains token usage information, the input prompt, and the generated response.\r\n\r\nYour code may be a simple chatbot or a complex workflow, and depending on the use case,\r\nyou\u0027ll need a different response. Depending on what you asked for, you may need\r\nstructured data or just plain text. Semantic Kernel will ensure that the data is\r\ndeserialized as necessary so you can work with it in your application.\r\n\r\nThe kernel allows you to inject filters at various points in the process. This is useful\r\nto, for example, filter out harmful responses (see [#s](#llmops-user-safety)) or remove\r\nPII data in the request (see [#s](#llmops-data-privacy)).\r\n\r\nWhile the kernel is the core of Semantic Kernel, the functions are the most essential\r\npart of the framework. Functions will provide the power needed to turn the LLM from a\r\nsimple text generator into the promised semantic intelligence of your application.\r\n\r\n{#llm-function-calling} \r\n### Calling Functions\r\n\r\nIn Semantic Kernel, you can connect functions to the kernel to provide additional\r\nfunctionality. Functions play an essential role in LLM-based applications because you\r\ncan use them to:\r\n\r\n- Perform actions based on the user\u0027s prompt.\r\n- Find relevant content to enhance the response to your prompt.\r\n\r\nWe will cover functions in great depth in [#s](#prompt-testing-and-monitoring) and\r\n[#s](#enhancing-llms-with-tools), but for now, it\u0027s important to understand a trick I\r\nlike to call the kernel loop.\r\n\r\nModern LLMs support function calling and can combine several calls to functions to\r\ncomplete a reasonably complex task if you design your application right.\r\n[#s](#function-calling-loop) demonstrates the pattern Semantic Kernel uses to turn the\r\nLLM into a planner using the LLM\u0027s function-calling capabilities. The function calling\r\nflow takes place when the rendered prompt is submitted at the end of the prompt\r\nexecution flow in [#s](#kernel-interactions).\r\n\r\n{#function-calling-loop} \r\n![Function calling loop](function-calling-loop.png)\r\n\r\nThis may come as a surprise: LLMs are trained as if they\u0027re chatbots, and that\u0027s good\r\neven if you don\u0027t use them as chatbots. Let me explain.\r\n\r\nThe concept of a conversation will sound familiar to you if you\u0027ve ever worked with",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "What is a drawback of using LLMs?",
    "Answer": "One drawback of using LLMs mentioned in the provided context is their **slow performance**. LLMs are inherently slow, and while you can make minor adjustments, such as increasing rate limits, their speed remains a significant bottleneck. This can impact the user experience, as users may have to wait longer for responses. Additionally, optimizing the rest of the application around the LLM is crucial to minimize this issue but won\u0027t eliminate the underlying limitation.",
    "Context": [
      {
        "Id": 17,
        "OriginalFileName": "Content\\02-essential-llmops-knowledge.md",
        "Content": "LLMs are. I could write an entire book about this topic, but I\u0027ll have to keep this\r\nshorter because we need to talk about other things. But let me give you the basics so\r\nyou know what you\u0027re getting yourself into.\r\n\r\nThere are three main topics when we talk about security in LLMs:\r\n\r\n- **Data privacy:** Customers and users will talk about data privacy because they\u0027re\r\nworried about the provider stealing their data to train the LLM.\r\n- **Application security:** This is about protecting your application from being hacked.\r\nThis is a big topic because LLMs are very powerful and can be used to generate malicious\r\ncontent.\r\n- **User safety:** This is about protecting your users from being exposed to harmful\r\ncontent generated by the LLM. LLMs can generate harmful content or misleading content\r\nthat can be used to manipulate people.\r\n\r\nLet me go over each of these topics to give you a direction on how to think about them.\r\n\r\n{#llmops-data-privacy} \r\n### Data privacy\r\n\r\nBefore you start using an LLM, you need to think about the data you\u0027re sending to it.\r\n\r\nThe [Samsung incident][SAMSUNG_INCIDENT] teaches us an important lesson: If your data is\r\ngoing to be used for training, it is likely to end up on someone else\u0027s desk at some\r\npoint in the future. Hackers are actively probing with prompts to find out what data is\r\nbeing used to train the LLM.\r\n\r\nSometimes, it\u0027s better not to use an LLM if you\u0027re worried about company secrets being\r\nleaked. For most cases, however, it\u0027s good enough to agree with the LLM provider and ask\r\nit not to use your data for training. All LLM Providers offer this option today and have\r\na section about it in their legal agreements.\r\n\r\nIt is essential to understand that security is a trade-off. You can\u0027t have 100% security\r\nbecause then you wouldn\u0027t use LLMs in the first place and miss out on their value.\r\nHowever, you can\u0027t wholly live without security, either. Have an active discussion with\r\nyour client about the data you\u0027re sending to the LLM and the risks involved, and then\r\ndecide based on that.\r\n\r\nThere\u0027s another closely related topic to data privacy that I want to address here. In\r\nincreasingly increasing countries, some laws require you to be careful with personal\r\nidentifiable information (PII). For example, in Europe, you need to comply with GDPR. In\r\nCalifornia, you need to comply with CCPA. These laws require you to be careful with the\r\ndata you\u0027re processing.\r\n\r\nManaging personal data is a topic you should address alongside the other security\r\nrequirements when planning an LLM-based application.\r\n\r\nMany cloud providers offer standard tools to filter out PII from text. For example,\r\nAzure has a tool called [Text Analytics][PII_FILTER] that can filter out PII from text.\r\nThis way, you can avoid sending PII to the LLM or storing it in your application.\r\nConsider it an extra safety measure on top of ensuring people know they\u0027re not supposed\r\nto send in PII.\r\n\r\n{#llmops-application-security} \r\n### Application Security\r\n\r\nSecuring modern cloud-native applications is challenging. Adding an LLM to the mix opens\r\nup a new chapter in application security. And it\u0027s a relatively new chapter, so there\u0027s\r\nnot much information on securing an LLM-based application. I will cover some LLM\r\nsecurity strategies around security in greater detail in the following chapters, but I\r\nwant to make sure you have a good overview of the topic first.\r\n\r\nApplying a deep defense strategy is the best way to handle application security. You\u0027ll\r\nwant to use multiple layers of defenses to ensure that if a hacker gets through one\r\nlayer, it will have to work hard to get through the next. A layered defense also helps\r\nkeep an attack\u0027s blast radius to a minimum.\r\n\r\nOne of the best moves you can make in terms of application security is not giving your\r\napplication access to resources it shouldn\u0027t have access to. I see many people using\r\nCopilot 365 and oversharing information because of bad defaults and the admin not being",
        "Embedding": []
      },
      {
        "Id": 16,
        "OriginalFileName": "Content\\02-essential-llmops-knowledge.md",
        "Content": "When I can, I\u0027ll examine monitoring data during early testing to determine how many\r\ntokens per minute we need and set the quota accordingly. You need to strike a balance\r\nbetween costs and user experience here because rate limits help you control costs, too.\r\nThe lower the quota, the lower the amount of tokens that can be spent per minute, and\r\nthe lower the bill. However, too many retries can lead to a bad user experience.\r\n\r\nIn [#s](#the-art-and-nonsense-of-prompt-engineering), we\u0027ll cover configuring parameters\r\nfor your prompt to maximize your quota.\r\n\r\nRate limiting can have a significant impact on the performance of your application. However, there are sources of performance issues that we need to cover.\r\n\r\n## LLM performance and the user experience of your application\r\n\r\nLLMs are very slow, and you need to think about how to work around that\r\nlimitation because you can\u0027t speed them up yourself. You can increase rate limits to\r\nspeed things up a little, but that\u0027s about it.\r\n\r\nLet me shift your perspective about performance in software applications. Under normal\r\ncircumstances, you need to look for performance bottlenecks in all areas of your\r\napplication. And it\u0027s likely you can solve many of those bottlenecks yourself. When\r\nusing an LLM in your application, it is important to know that the LLM will be your\r\nbiggest bottleneck, and you can\u0027t speed it up by much.\r\n\r\nThere are two things that you need to think about when optimizing LLM-based application\r\nperformance:\r\n\r\n- First, you must think about ways to improve the user experience. For example, you can\r\nlet the LLM stream the response to a prompt. This way, the user receives an answer in\r\nchunks but quickly.\r\n- Second, you need to think less about the performance of the code around the call to an\r\nLLM. It won\u0027t matter as much what you do there because the LLM is the slowest piece in\r\nthe puzzle.\r\n\r\nDo keep in mind that you\u0027ll want to optimize the rest of the code that doesn\u0027t call the\r\nLLM to be fast; otherwise, you\u0027re just adding to the problem. I\u0027ve found that if the\r\nrest of the application is nice and fast, users are more forgiving about the LLM being\r\nslow.\r\n\r\n{#llomops-failover-strategies}\r\n## Failover strategies\r\n\r\nAs if performance wasn\u0027t hard enough, we got another nice present from the LLM\r\nproviders. LLM providers are sometimes unavailable because you\u0027ve hit a hard limit or\r\nbecause they\u0027re overloaded. Anthropic had this problem frequently when I wrote this\r\nchapter, where you receive overloaded errors and can only call the LLM again after a few\r\nhours. This can be problematic when you run a production application that needs some\r\nform of availability.\r\n\r\nIn many cases, I\u0027ve found that letting the user know that the LLM is unavailable is\r\nenough if you have no availability requirements. Many of my clients don\u0027t have any\r\navailability requirements because they\u0027re just starting out and know that they can\u0027t\r\nguarantee 100% uptime.\r\n\r\nFrom experience, I learned that despite the lack of availability requirements, it\u0027s\r\nbeneficial to have a failover strategy in place. For example, you could have GPT-4o as a\r\nfallback for Claude Sonnet in your application. This way, you can switch to GPT-4o when\r\nClaude Sonnet is unavailable.\r\n\r\nRegardless of what your client thinks, it\u0027s good to spend some time planning for\r\nfailover scenarios, as this significantly impacts the user experience and the\r\ndevelopment effort you need to spend.\r\n\r\n{#llmops-security} \r\n## Security and privacy in an LLM-based application\r\n\r\nWe\u0027ve covered quite a few LLMOps essentials. You may have noticed that not many of these\r\naspects are well understood by users of LLM-based applications. This is especially true\r\nfor security and privacy concerns.\r\n\r\nI\u0027ve not had many security discussions with clients about LLMs. This is because we\u0027re\r\njust getting started using LLMs in production. Not many people realize how dangerous",
        "Embedding": []
      },
      {
        "Id": 5,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "Once I saw what LLMs could do, I went overboard with ChatGPT. I used it for everything\r\ntext-related. I even messed up two blog posts on my website with negative feedback to\r\nshow for it. LLMs produce flavorless and pretty mediocre content. They\u0027re trained to\r\nrepresent the average of what language has to offer. And that\u0027s pretty average and\r\nflavorless.\r\n\r\nI used LLMs for coding, too, as I am a developer. I wrote a complete application using\r\nonly AI. And it\u0027s used in production today. But it was hard to get there. The LLM\r\nfrequently steered into the wall with weird layouts and useless unit tests. I haven\u0027t\r\nbothered measuring how quickly I built the application. I was quicker but less satisfied\r\nwith the result because writing great code is a skill I\u0027m proud of.\r\n\r\nAfter learning about open-source LLMs, I decided to try them, too. It was very slow,\r\neven on a beefy Intel Core i9 machine with a massive graphics card. I quickly learned\r\nthat you need a lot of power to run an LLM on your machine and in the cloud. And with a\r\nprice tag of 3500 euros for a decent machine, it\u0027s not something you want to do for a\r\nhobby project.\r\n\r\nThere are plenty more experiences where I found the boundaries of what LLMs can do, but\r\nlet me finish with one final example. I tried using an LLM to upgrade program code from\r\na low-code solution to Typescript without human supervision. We quickly had to add human\r\nsupervision because it wouldn\u0027t help us without that.\r\n\r\n### Key lessons learned\r\n\r\nYou might wonder, why am I telling you my experiences? There are three key lessons that\r\nI want you to keep in mind while reading this book:\r\n\r\n1. Be specific in what you ask from the LLM. Don\u0027t just ask for an article about LLMs; provide specific instructions.\r\n2. Always review and understand the output of the LLM. Don\u0027t let your users use the output of the LLM unseen. The output will be wrong in all the weird ways you\u0027ve never thought of.\r\n3. Break big problems down into smaller problems. Instead of asking the LLM to perform 10 steps, ask it for just one step. It will be easier for the LLM to perform and easier for you to debug.\r\n4. Keep track of the context and provide it in focused chunks. LLMs have limited input and output space, so they can\u0027t keep track of a complete book or even a blog post.\r\n\r\n### Evolution of my understanding\r\n\r\nAfter the initial rollercoaster ride with LLMs, my understanding of them evolved. I\r\nstopped seeing them as a silver bullet that could solve all my language-related problems\r\nand started seeing them as a powerful pattern-matching engine capable of transforming\r\ntext.\r\n\r\nOne crucial moment was realizing that LLMs excel at clearly defined tasks that involve\r\nmatching a pattern in the source text and transforming it into other text. If you can\r\nfind a clear pattern in the input and clearly define the target structure, an LLM is\r\nlikely a good solution to your problem. The less clear the problem statement is, the\r\nmore issues you\u0027ll experience.\r\n\r\nYou\u0027ll want to balance humans and machines well to build a practical LLM application.\r\nHuman oversight is essential when using an LLM. Throughout the rest of the book, you\r\nwill find that I\u0027m using interaction patterns that promote human oversight because it\u0027s\r\nnecessary and improves the experience significantly.\r\n\r\n### How I integrated LLMs into real projects\r\n\r\nClearly defined problems and human oversight are essential when you view an LLM-based\r\napplication from a functional perspective. From a technical standpoint, you must\r\nconsider applying LLMs as a software engineering problem with an AI aspect rather than a\r\npure AI project.\r\n\r\nHere are three reasons why you should use a software engineering approach:\r\n\r\n- LLMs will behave better when you follow a structured approach. The more structure, the better.\r\n- LLM behavior changes when providers push new versions of the models; automated testing is your friend.",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "How can a filter be applied to metric configurations when analyzing prompt templates?",
    "Answer": "To apply a filter to metric configurations when analyzing prompt templates, you can use the *Add filter* button in the metric configuration settings. Specifically, you can filter on the \u0060semantic_kernel.function.name\u0060 property. This property corresponds to the name of the prompt template, which is especially useful if you have stored your prompt templates in a YAML file. In that case, you can filter based on the value of the \u0060name\u0060 property in the YAML file. This enables you to display separate charts for each prompt template, which helps in analyzing how individual prompts behave.",
    "Context": [
      {
        "Id": 50,
        "OriginalFileName": "Content\\05-testing-and-monitoring-prompts.md",
        "Content": "![Fully configured metric for input tokens](configured-input-token-metric.png)\r\n\r\nYou can repeat the same steps for the output tokens metric. The metric you must select is \u0060semantic_kernel.invocation.function.token_usage.completion\u0060.\r\n\r\nAfter configuring the metrics, you can add another tile to the dashboard to show the duration of each prompt in your application. The metric you need for this is \u0060semantic_kernel.invocation.function.duration\u0060.\r\n\r\nI\u0027ve found that displaying separate charts for each prompt template in the application is useful for learning how each individual prompt behaves. You can add a filter to the metric configuration using the *Add filter* button. You can filter on the \u0060semantic_kernel.function.name\u0060 property. The name will match the name of the prompt template. If you\u0027ve used a YAML file to store your prompt template, you can filter on the value of the name property in the YAML file.\r\n\r\nNote that as your application grows, managing the dashboard becomes harder. You probably have a few prompt templates that you\u0027re using. It\u0027s a good idea to put only those metrics that require attention in the dashboard because you\u0027ve recently changed them or added them to the application. Dashboards are a living piece of your monitoring setup and should be updated regularly. If you don\u0027t, you\u0027ll likely not look at it or do anything with the information on it.\r\n\r\nLet\u0027s add the final piece of the puzzle. The tokens and duration metrics are useful for finding unusual things. But it\u0027s the costs that matter for most people. Here\u0027s how to add them to the dashboard.\r\n\r\nNavigate to the OpenAI Resource in the Azure Portal, and select the *Resource Management* \u003E *Cost analysis* option from the left sidebar. This will show you a chart displaying the costs for the resource. To add the cost analysis to your dashboard, click the pushpin next to the title *Cost analysis* and choose the dashboard to which you want to add it.\r\n\r\nNow that we have monitoring let\u0027s return to collecting feedback information. If you\u0027ve enabled the collection of prompts and responses in your application, you can export that data and use it to improve your tests.\r\n\r\n### Collecting data to improve tests\r\n\r\nTelemetry data in Application Insights is stored in a structured format in a Log Analytics Workspace. You can query the data using the [Kusto Query Language (KQL)][KUSTO_INTRODUCTION]. But you can also export it to a storage account for later use.\r\n\r\nWe\u0027ll use the data export feature to retrieve the prompts and responses for specific prompt templates. After we\u0027ve downloaded the data, we\u0027ll build an application to extract the prompts and responses from the raw log data and use them to improve our tests.\r\n\r\nBefore exporting data, ensure you have a storage account with hierarchical namespaces enabled. You can create one using [this guide][CREATE_STORAGE_ACCOUNT].\r\n\r\nLet\u0027s create an export rule to copy data from the Log Analytics Workspace to the storage account. You can get access to the Log Analytics Workspace for your Application Insights resource by going to the Application Insights in the Azure portal and clicking on the link next to the Workspace property in the overview page of the resource. This will take you to the Log Analytics Workspace.\r\n\r\nIn the Log Analytics Workspace, you can find the data export feature in the sidebar of the workspace resource under *Settings* \u003E *Data export*. [#s](#export-rules-overview) shows the data export overview page.\r\n\r\n{#export-rules-overview} \r\n![Export rules overview page](azure-law-export-rules-overview.png)\r\n\r\nCreate a new rule by clicking the *New export rule* button. This will take you to the rule creation page. On this page, you can set a name for the new rule, select the tables you want to export, and select the destination for the data. [#s](#export-rule-step-1) shows what this page looks like.\r\n\r\n{#export-rule-step-1} \r\n![Export rule creation page](azure-law-export-rules-step-1.png)",
        "Embedding": []
      },
      {
        "Id": 39,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "Prompt engineering is an art, but it can also be dangerous if you\u0027re not careful. Let\u0027s examine how to protect your prompts from abuse.\r\n\r\n## Security considerations when using prompts\r\n\r\nIn [#s](#llmops-application-security), we discussed the importance of a layered defense. The layered defense starts with the prompt execution process. We\u0027ll need to filter the prompt\u0027s input and output for unwanted content and reduce the risk of prompt injection.\r\n\r\n### Filtering executable code from prompts and the model output\r\n\r\nOne attack vector impacting prompt engineering is prompt injection. Malicious actors use this class of attack to inject content into the prompt, producing output that the application executes in unwanted ways.\r\n\r\nFor example, you can construct prompts that produce harmful javascript that your browser will execute if you don\u0027t filter out the executable code from the response.\r\n\r\nBy default, the template engines used in Semantic Kernel escape javascript, CSS, and script tags in the prompt input and output. This way, you\u0027re somewhat protected against abuse if you render content in the browser. However, if you\u0027re going to use the output of a prompt as code to be executed in the context of your application, you should be aware that it can\u0027t be trusted. I wouldn\u0027t do it at all, but if you have to, limit what that code has access to.\r\n\r\nKeep in mind that filtering the LLM\u0027s input and output is an important step, but it will not be enough to protect your application. We\u0027ll cover more steps in [#s](#enhancing-llms-with-tools) as we look at using functions with the Semantic Kernel.\r\n\r\nFully understanding just how dangerous prompt injection is requires a deep dive into security and is beyond the book\u0027s scope. However, I recommend reading the [MITRE Atlas][MITRE] and the [OWASP Top 10 for LLMs][OWASP_LLM] to get a better understanding of how hackers are abusing prompts these days.\r\n\r\n### Filtering PII from the prompt and the model output\r\n\r\nMany organizations want to ensure they\u0027re not exposing personal information to the LLM because they\u0027re worried that the LLM provider may store that information. This is a valid concern, although many LLM providers have provisions that handle this concern quite well in their contract.\r\n\r\nIf you have to filter PII from the input of the prompt, you can use a filter to do so. Let me show you a quick example of how to build a filter for this purpose:\r\n\r\n\u0060\u0060\u0060csharp\r\npublic class PIIFilter: IPromptRenderFilter\r\n{\r\n\u00A0 \u00A0 public async Task OnPromptRenderAsync(\r\n        PromptRenderContext context, \r\n  \u00A0 \u00A0 \u00A0 Func\u003CPromptRenderContext, Task\u003E next)\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 // This function is called when the prompt is rendered. This is where\r\n\u00A0 \u00A0 \u00A0 \u00A0 // we can filter the contents of the prompt before it\u0027s submitted.\r\n\u00A0 \u00A0 \u00A0 \u00A0 \r\n\u00A0 \u00A0 \u00A0 \u00A0 var renderedPrompt = context.RenderedPrompt;\r\n\u00A0 \u00A0 \u00A0 \u00A0 \r\n\u00A0 \u00A0 \u00A0 \u00A0 //TODO: Filter the prompt contents\r\n\u00A0 \u00A0 \u00A0 \u00A0 \r\n\u00A0 \u00A0 \u00A0 \u00A0 // Replace the original prompt with the filtered prompt.\r\n        context.RenderedPrompt = renderedPrompt; \r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 await next(context);\r\n\u00A0 \u00A0 }\r\n}\r\n\u0060\u0060\u0060\r\n\r\nIn this code, we perform the following steps:\r\n\r\n1. First, we create a class that implements the \u0060IPromptRenderFilter\u0060 interface.\r\n2. Next, we implement the \u0060OnPromptRenderAsync\u0060 to filter the rendered prompt.\r\n\r\nTo use the filter, we can configure it in the kernel using the following code:\r\n\r\n\u0060\u0060\u0060csharp\r\nkernel.PromptRenderFilters.Add(new PIIFilter());\r\n\u0060\u0060\u0060\r\n\r\nNote that I haven\u0027t included any actual PII filtering code in the filter. If you\u0027re interested in learning how to use one of the popular PII filtering tools, please find the documentation for each of them here:\r\n\r\n1. [Azure PII Detection][AZ_PII_DETECTION]\r\n2. [Google de-identification][GA_PII_DETECTION]\r\n3. [AWS PII detection][AWS_PII_DETECTION]\r\n\r\nI recommend getting rid of PII as soon as it enters the application. The sooner you remove it, the less likely it is to leak anywhere.\r\n\r\n## Summary",
        "Embedding": []
      },
      {
        "Id": 33,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "The frequency penalty is the last hyperparameter we need to discuss. This hyperparameter looks similar to the presence penalty hyperparameter but uses a different approach. Instead of applying a flat penalty to tokens that occurred before in the output, a frequency penalty applies a penalty to the probability of tokens that frequently appear in the output. The more often a token appears in the output, the higher the penalty and the less likely the token is to be selected during the Top-P sampling process.\r\n\r\n#### What to choose for each of the hyperparameters\r\n\r\nI know this is a lot to take in, so let me give you some direction on what to choose for each parameter.\r\n\r\nCoding requires a more repetitive and boring output to be effective, so you want to choose a lower value for Top-P and a lower value for temperature. The presence penalty and frequency penalty should be set to 0 because we don\u0027t want to apply any penalties. Coding is repetitive by nature.\r\n\r\nI recommend setting the Top-P value higher and applying a higher temperature for general-purpose tasks like generating marketing content. The presence penalty and frequency penalty can help make the text more varied. Don\u0027t go overboard, though; a low value is often enough. Setting the presence and frequency penalty to a high value leads to fascinating but often nonsensical output.\r\n\r\nYou must test your prompts with various inputs and settings to establish what works for most cases. Because the probability distribution values ultimately depend on your prompt\u0027s content, you likely need to adjust the hyperparameters a little bit based on that.\r\n\r\nOnce you\u0027ve written a good-quality prompt, you may want to keep it around for longer. For this, it\u0027s nice to have some sort of templating system in place. Let\u0027s take a look at what Semantic Kernel has to offer.\r\n\r\n{#prompt-templates} \r\n## Writing prompt templates for reusability\r\n\r\nWriting your prompts in line with other C# codes is never a good plan. It\u0027s hard to read and maintain, and you can\u0027t reuse it. That\u0027s why Semantic Kernel offers a way to write prompt templates using a variety of templating languages:\r\n\r\n- Semantic Kernel Templates: The internal format developed by Microsoft.\r\n- Handlebars: A popular templating language that is available for many\r\nprogramming languages.\r\n- Liquid: An alternative to handlebars that is also available for several programming languages.\r\n\r\nThe prompt templating feature in Semantic Kernel is quite powerful. Let\u0027s first examine the internal templating engine.\r\n\r\n### Creating a prompt template in Semantic Kernel\r\n\r\nThe Semantic Kernel templating language is a text-based language. You can write a basic prompt template like this:\r\n\r\n\u0060\u0060\u0060text\r\nHelp me cook something nice, give me a recipe for {{ $dish }}\r\n\u0060\u0060\u0060\r\n\r\nIn this template, we ask for a basic recipe for a dish. Dish is a variable identified by \u0060{{ \\$dish }}\u0060. We can fill this variable later when we invoke the prompt.\r\n\r\nYou can invoke the prompt template using the following code:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar promptTemplate = File.ReadAllText(\r\n    Path.Join(Directory.GetCurrentDirectory(), \u0022prompt.txt\u0022)\r\n);\r\n\r\nvar result = await kernel.InvokePromptAsync(promptTemplate,\r\n    arguments: new KernelArguments\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 [\u0022dish\u0022] = \u0022pizza\u0022\r\n\u00A0 \u00A0 },\r\n    templateFormat: \u0022semantic-kernel\u0022);\r\n\r\nConsole.WriteLine(result);\r\n\u0060\u0060\u0060\r\n\r\nLet\u0027s go over this code to understand what\u0027s happening:\r\n\r\n1. First, we load a prompt file from disk using the standard .NET I/O functions.\r\n2. Then, we call the \u0060InvokePromptAsync\u0060 method on the kernel instance to execute the\r\nprompt template providing \u0060arguments\u0060, and the \u0060templateFormat\u0060.\r\n3. Finally, we print the result to the console.\r\n\r\nThe code sample uses a kernel instance as described in [#s](#setting-up-semantic-kernel). The full source code for this sample is in the [GitHub repository][SK_TEMPLATE_SAMPLE].",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "Does the synthetic dataset have no use in validating the RAG pattern implementation?",
    "Answer": "No, the synthetic dataset is useful in validating the RAG pattern implementation. As mentioned in the context, the dataset is used to generate test samples and measure the faithfulness of the RAG pipeline. By storing the context, question, and answer in the dataset, it allows you to verify the quality and accuracy of the generated responses. Additionally, tools like Ragas leverage this dataset to measure the faithfulness of the pipeline, providing valuable insights into the performance of the RAG implementation.",
    "Context": [
      {
        "Id": 73,
        "OriginalFileName": "Content\\07-retrieval-augmented-generation.md",
        "Content": "The code that runs this prompt is part of a bigger program that performs a couple of steps:\r\n\r\n1. First, the program chunks the content as we would normally when indexing content in the vector database.\r\n2. Next, the program iterates over each generated chunk and run the prompt with the content of each chunk.\r\n\r\nI recommend storing the used context, the question, and the answer in the dataset so you can verify the quality of the generated dataset later on.\r\n\r\nThe full source code for the validation dataset is stored in the [GitHub repository][GEN_VAL_DATA_SAMPLE], feel free to take a look at that and use it for your own projects. In the sample code you\u0027ll find several variations on the question generation prompt that. I\u0027ve included one to generate true/false statements, and one to generate short answers.\r\n\r\nNow that we have a validation dataset, let\u0027s take a look at generating responses that we can use to measure faithfulness of the RAG system.\r\n\r\n### Generating test samples\r\n\r\nFor this step in the evaluation workflow we\u0027ll take the code we wrote in [#s](#using-the-vector-store-with-a-prompt) use it with some modifications to generate test samples.\r\n\r\nThe main program code looks like this:\r\n\r\n\u0060\u0060\u0060csharp\r\n\r\n\u0060\u0060\u0060\r\n\r\nIn this code we perform the following steps:\r\n\r\n1. First, we load the validation dataset we created earlier.\r\n2. Next, we use the \u0060QuestionAnsweringTool\u0060 to generate a response for each question in the dataset.\r\n3. Finally, we store the question, the answer, and the used context in a CSV file.\r\n\r\nThe code for this step of the validation workflow can also be found on [GitHub][GH_TEST_SAMPLES]. It includes a few more details related to setting up the kernel instance, and code to handle indexing of the content similar to how we did it in [#s](#using-the-vector-store-with-a-prompt).\r\n\r\nWith the test samples generated, we can move on to writing some Python code to measure the faithfulness of the RAG pipelines using Ragas.\r\n\r\n### Measuring faithfulness with Ragas\r\n\r\nThe following code shows how to use Ragas to measure the faithfulness of the RAG pipeline:\r\n\r\n\u0060\u0060\u0060python\r\n\r\n\u0060\u0060\u0060\r\n\r\nIn this code we perform the following steps:\r\n\r\n1. First, we load the test samples we generated earlier.\r\n2. Next, we use the \u0060Ragas\u0060 library to measure the faithfulness of the RAG pipeline.\r\n3. Finally, we print out the results in the terminal.\r\n\r\nThe code for this step of the validation workflow can also be found on [GitHub][GH_RAGAS_SAMPLE]. It includes a few more details related to setting up the kernel instance, and code to handle indexing of the content similar to how we did it in [#s](#using-the-vector-store-with-a-prompt).\r\n\r\n\r\n\r\n## Summary\r\n\r\nIn this chapter we covered a lot around the RAG pattern, one of the first design patterns we\u0027ll discuss in the book. We talked about the components involved in the RAG pattern and how to implement an end-to-end RAG pattern. We also covered how to test various parts of the RAG pattern with a mix of unit-tests, manual tests, and model-based tests. Finally, we talked about about variations on the regular RAG pattern with including how you can use tools and a regular database to quickly find relevant information to base a response on.\r\n\r\nThe next chapter we\u0027ll shift our focus towards using LLMs to generate structured output. It will be less involved than implementing a RAG pattern, I promise, but it will still be fun, and essential if you want to build more complicated workflows as we continue working towards chapter 9 and 10 of the book.\r\n\r\n[SAMPLE_SOURCE_1]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-07/Chapter7.RetrievalAugmentedGeneration/\r\n[SO_BLOG]: https://stackoverflow.blog/2024/12/27/breaking-up-is-hard-to-do-chunking-in-rag-applications/\r\n[VALIDATION_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-07/Chapter7.PipelineValidation/ [GH_TEST_SAMPLES]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-07/Chapter7.TestSampleGeneration/\r\n[GH_RAGAS_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-07/Chapter7.RagasValidation/",
        "Embedding": []
      },
      {
        "Id": 71,
        "OriginalFileName": "Content\\07-retrieval-augmented-generation.md",
        "Content": "new TextUnitStringMapper(),\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 new TextUnitTextSearchResultMapper());\r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 var searchFunction = textSearch.CreateGetTextSearchResults();\r\n\r\n        kernel.Plugins.AddFromFunctions(\u0022SearchPlugin\u0022, [searchFunction]);\r\n\u00A0 \r\n\u00A0 \u00A0 \u00A0 \u00A0 var citationsFilter = new CitationCapturingFilter();\r\n            kernel.FunctionInvocationFilters.Add(citationsFilter);\r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 // ... Rest of the code\r\n\u00A0 \u00A0 }\r\n}\r\n\u0060\u0060\u0060\r\n\r\nLet\u0027s go over the code to understand the modifications compared to the original\r\nversion of this code: The start of the method is the same as before, but we\u0027ve added a new filter right after configuring the text search plugin.\r\n\r\nIn previous code fragments, we were already using the \u0060TextUnitTextSearchResultMapper\u0060. Let\u0027s discuss what this mapper does because it\u0027s necessary here to make the function filter work. The class looks like this:\r\n\r\n\u0060\u0060\u0060csharp\r\npublic class TextUnitTextSearchResultMapper : ITextSearchResultMapper\r\n{\r\n\u00A0 \u00A0 public TextSearchResult MapFromResultToTextSearchResult(object result)\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 if (result is TextUnit textUnit)\r\n\u00A0 \u00A0 \u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 return new TextSearchResult(value: textUnit.Content)\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 {\r\n                Link = textUnit.OriginalFileName,\r\n                Name = textUnit.Id.ToString()\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 };\r\n\u00A0 \u00A0 \u00A0 \u00A0 }\r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 throw new ArgumentException(\u0022Invalid result object\u0022);\r\n\u00A0 \u00A0 }\r\n}\r\n\u0060\u0060\u0060\r\n\r\nIn this class, we perform the following steps:\r\n\r\n1. First, we implement the \u0060ISearchResultMapper\u0060 interface, creating the \u0060MapFromResultToTextSearchResult\u0060 method.\r\n2. Then, in the method, we check if the result is a \u0060TextUnit\u0060 instance.\r\n3. Next, we create a new \u0060TextSearchResult\u0060 instance with the Content of the text unit, including the search result\u0027s identifier, link, and value.\r\n\r\nWith the additional filter, you can be sure that you\u0027re capturing the search results found by the vector search instead of relying on the LLM to choose whether something was used in the response. However, you may still get some false positives because the LLM may not actually use a result that you retrieved.\r\n\r\nImplementing a RAG pattern takes effort to get right, and it will not always be 100% perfect. You have to make a choice here: Do you want to have a response where the LLM makes up sources? Or do you want to have a response with a separate set of citations that may not be included in the actual response?\r\n\r\nWhatever you choose, I recommend spending some time to establish a good test strategy for the various parts of your RAG implementation to ensure the highest possible quality. In the next section, we will discuss how to approach testing the RAG pattern.\r\n\r\n## A practical approach to validating the RAG pipeline\r\n\r\nTesting the RAG pattern can be quite complicated. [#s](#rag-evaluation-controls) shows the various aspects of the RAG pattern we discussed and the quality controls that are available to validate them. It\u0027s a lot to get through.\r\n\r\n{#rag-evaluation-controls}\r\n![RAG pattern evaluation controls](rag-evaluation-controls.png)\r\n\r\nWe could go through this step-by-step explaining all the small details of how to validate each component in the RAG pipeline on its own. But I\u0027ve found this to be very unpractical. Every piece in the pipeline interacts with other components and you\u0027ll likely influence one component by changing another.\r\n\r\nLet me help you make sense of RAG pipeline validation by talking you through a practical approach. We\u0027ll use the RAG pipeline from the previous sections based on the content of this book as input for the validation.\r\n\r\nLet\u0027s get started with an overview of the validation process.\r\n\r\n### Overview of the validation process\r\n\r\nWhen testing a RAG pipeline I like working with one or two basic metrics to measure performance. For a RAG pipeline it\u0027s important that the final answer given by the LLM makes sense using as content from the source documents as faithfully as possible.",
        "Embedding": []
      },
      {
        "Id": 62,
        "OriginalFileName": "Content\\07-retrieval-augmented-generation.md",
        "Content": "{#retrieval-augmented-generation}\r\n# Retrieval augmented generation\r\n\r\nIn previous chapters, we focused on learning the basic building blocks of an LLM-based application. We covered the basics of LLMs, prompting, testing, and using tools. Now, it\u0027s time to move on to more advanced topics. In this chapter and the ones after it, we\u0027ll focus on common patterns you can use to build more elaborate LLM-based applications. We\u0027ll use the knowledge learned in previous chapters to help implement the patterns.\r\n\r\nIn this chapter, you\u0027ll learn how to use Retrieval Augmented Generation to answer questions based on internal knowledge that has not previously been trained in the LLM. You\u0027ll learn how to index documents for retrieval and use indexed Content in the LLMs response.\r\n\r\nWe\u0027ll cover the following topics:\r\n\r\n- What is Retrieval Augmented Generation (RAG)\r\n- Building an end-to-end RAG pipeline with Semantic Kernel\r\n- A practical approach to validating the RAG pipeline\r\n- Optimizing retrieval for RAG\r\n- Variations on the RAG pattern\r\n\r\nLet\u0027s start by discussing Retrieval Augmented Generation (RAG) and learning what components are involved in this pattern.\r\n\r\n## What is Retrieval Augmented Generation (RAG)\r\n\r\nRetrieval Augmented Generation (RAG) is a pattern in which you use information from an external source as extra context information when generating a response to a prompt. \r\n\r\nThe RAG pattern is a form of in-context learning we discussed in [#s](#few-shot-learning). You can use RAG for many scenarios. But you\u0027re most likely using RAG to answer users\u0027 questions. [#s](#rag-pattern-architecture) shows the structure of the RAG pattern.\r\n\r\n{#rag-pattern-architecture}\r\n![RAG pattern architecture](rag-pattern-architecture.png)\r\n\r\nThe RAG pattern has two main components:\r\n\r\n1. the retrieval portion is responsible for finding relevant information based on the entered prompt.\r\n2. Next, there\u0027s the generation portion, responsible for generating the final response to the prompt.\r\n\r\nThe basics of the RAG pattern aren\u0027t overly involved, but you can find many variants on the Internet. That\u0027s the beauty of the pattern: You can adapt and extend it to your needs.\r\n\r\nLet\u0027s examine the retrieval component of the RAG pattern more closely to understand how to retrieve information to answer questions.\r\n\r\n{#retrieval-component-architecture}\r\n### Retrieval component architecture\r\n\r\nThe retrieval component of the RAG pattern is usually made out of two subcomponents. You need a method to process information into a format that\u0027s easy for an LLM to answer questions with, and you need a method for the application to retrieve the preprocessed data. [#s](#retrieval-architecture) shows the details of the retrieval portion of the RAG pattern.\r\n\r\n{#retrieval-architecture}\r\n![Retrieval component of the RAG pattern](retrieval-architecture.png)\r\n\r\nLet\u0027s explore the retrieval component of the RAG pattern with a practical use case. Imagine you must explain how to disassemble a washing machine to a mechanic. You could give the whole manual to the mechanic and let them read it. But that\u0027s not efficient because it takes quite a long time. It\u0027s nicer when the mechanic can ask, \u0022How do I remove the electrical board from the front of the Miele W1 washing machine?\u0022 and the LLM answers with specific instructions on how to do that, but nothing more.\r\n\r\nWe could give the LLM the whole washing machine manual and let it determine the answer. But that doesn\u0027t work very well; it might be even worse than the mechanic reading the manual. First, the manual might not fit in the LLM\u0027s context window. Next, the LLM might be unable to grab the correct information because of the limitations in what it can focus on within its context window, as we discussed in [#s](#context-window-limits).",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "Why might vague instructions be insufficient for certain applications?",
    "Answer": "Vague instructions might be insufficient for certain applications because they can lead to inconsistent and unpredictable outputs from the LLM. This is due to the LLM\u0027s reliance on pattern matching and its inability to \u0022understand\u0022 or \u0022reason\u0022 beyond reproducing patterns it has seen before. Without clear and detailed instructions, the LLM might generate responses with too much variation or fail to produce the desired output format or style, especially in business applications where high-quality, consistent results are critical. Providing precise instructions helps the LLM follow a specific style or structure, ensuring the generated output meets the application\u0027s requirements.",
    "Context": [
      {
        "Id": 30,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "Any LLM can reproduce patterns learned from the large body of text on which it was trained. But that body of text hopefully doesn\u0027t contain internal documents of the client or company you work for. So, if you need to answer questions about internal information, you must tell the LLM about the information.\r\n\r\nWe\u0027ll cover Retrieval Augmented Generation in greater depth in chapter 7. But for now, it\u0027s essential to understand that the LLM can\u0027t remember facts. It\u0027s a talking parrot that reproduces patterns it has seen before. This looks like a limitation, but you can turn this into a powerful trait.\r\n\r\nFor example, I want to answer a question about the company\u0027s policy on remote work. In that case, I can first look up documents related to the question by performing a similarity search using a search engine. The documents I can put into the prompt as additional context and then instruct the LLM to answer the question. A prompt for answering questions typically will look like this:\r\n\r\n\u0060\u0060\u0060text\r\nYou\u0027re a digital assistant for our company\u0027s HR department. \r\nPlease answer the employee\u0027s question based on the content \r\nprovided as context.\r\n\r\n## Context\r\n\r\n\u003CThe found fragments about the policy on remote work\u003E\r\n\r\n## Question\r\n\r\n\u003CThe employee question\u003E\r\n\u0060\u0060\u0060\r\n\r\nYou can consider the context heading in the prompt as samples that the model can use to generate a helpful response. Because of the talking parrot effect, the LLM will likely reproduce the content you provided. The model is \u0022learning\u0022 from information in the context. This is often called [in-context learning][INCONTEXT_LEARNING].\r\n\r\nIf the information in the context is incorrect, vague, or non-existent, then the LLM can\u0027t match the pattern, and you\u0027ll get an answer that doesn\u0027t make much sense or is plain misleading. So it\u0027s essential to ensure you only inject safe and sensible information.\r\n\r\nAdding samples to a prompt helps establish a good pattern for the LLM. You\u0027ll see many people add examples of their desired results to a prompt. Adding examples to your prompt is called one-shot learning or few-shot learning if you add more than one example of the expected output. Let\u0027s look at an example to understand what that looks like:\r\n\r\n~~~text\r\nYou\u0027re a world-class chef, and you\u0027re preparing a meal. Provide a recipe for apple pie please. Output the result in JSON; use the following example as a guide.\r\n\r\n## Example\r\n\u0060\u0060\u0060\r\n{\r\n   \u0022title\u0022: \u0022The recipe title\u0022,\r\n   \u0022ingredients\u0022: [\r\n       { \u0022unit\u0022: \u0022tbsp\u0022, \u0022amount\u0022: 1, \u0022name\u0022: \u0022the name of the ingredient\u0022 },\r\n       { \u0022unit\u0022: \u0022grams\u0022, \u0022amount\u0022: 12, \u0022name\u0022: \u0022another ingredient\u0022 },\r\n   ],\r\n   \u0022instructions\u0022: \u0022The instructions to prepare the recipe\u0022\r\n}\r\n\u0060\u0060\u0060\r\n\r\n~~~\r\n\r\nWe\u0027ll get a completely different response from the previous sample when we run the prompt. The LLM will now generate a JSON object for your recipe.\r\n\r\n~~~text\r\nCertainly! Here\u0027s a recipe for a classic apple pie in JSON format:\r\n\u0060\u0060\u0060json\r\n{\r\n \\\u0022title\\\u0022: \\\u0022Classic Apple Pie\\\u0022,\r\n \\\u0022ingredients\\\u0022: [\r\n { \\\u0022unit\\\u0022: \\\u0022cups\\\u0022, \\\u0022amount\\\u0022: 2.5, \\\u0022name\\\u0022: \\\u0022all-purpose flour\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022granulated sugar\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022salt\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022cups\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022unsalted butter, cold and diced\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 6, \\\u0022name\\\u0022: \\\u0022ice water\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022lbs\\\u0022, \\\u0022amount\\\u0022: 2, \\\u0022name\\\u0022: \\\u0022apples\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022cups\\\u0022, \\\u0022amount\\\u0022: 0.75, \\\u0022name\\\u0022: \\\u0022granulated sugar\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 2, \\\u0022name\\\u0022: \\\u0022all-purpose flour\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022ground cinnamon\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022pinch\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022ground nutmeg\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022lemon juice\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022vanilla extract\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 2, \\\u0022name\\\u0022: \\\u0022unsalted butter (for filling)\\\u0022 },",
        "Embedding": []
      },
      {
        "Id": 29,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "Vague instructions aren\u0027t going to work. You may get lucky, but you\u0027re more likely to get too much variation in response as more users use your application. And that\u0027s okay for chat applications like ChatGPT, but it\u0027s not for business applications where users expect high-quality results.\r\n\r\nTwo things help generate a pattern that the LLM can match. First, it\u0027s beneficial to provide the LLM with a clear context. For example, you can give it a persona to provide the context or tell it what project/task you\u0027re working on. Second, it\u0027s essential to provide the LLM with detailed instructions about the response\u0027s content, style, and structure.\r\n\r\nThe context helps set the embeddings and attention mechanism of the LLM, so it starts searching for output tokens in the correct word space. For example, the persona of a chef will likely lead the LLM in the direction of food, recipes, cooking techniques, and so on. A copywriter persona primes the embeddings and attention mechanism in the direction of marketing, text writing, and publishing. In the case of the copywriter, you\u0027ll also need to provide the medium in which the text is intended. Copywriting is different for a blog post than it is for a book.\r\n\r\nThe detailed instructions help the LLM follow a specific style pattern. For example, if you ask for short responses, the LLM is likelier to produce short outputs.\r\n\r\n### Specify the output format for the prompt\r\n\r\nNext, we have to consider the prompt\u0027s output format. In chat applications, it\u0027s often enough not to specify the output format at all. LLMs are trained as chatbots, and they will produce conversational output. But if you want to use the LLM for other tasks, it\u0027s essential to tell the LLM precisely what output format you want.\r\n\r\nFor example, you can tell the LLM to output a list of items or a table or to put a specific portion of the output in a fenced Markdown code block. This outputs the results of the prompt between \\\u0060\\\u0060\\\u0060 and \\\u0060\\\u0060\\\u0060 so it\u0027s easily parsable by your application.\r\n\r\nLet me demonstrate by asking the LLM to give me the apple pie recipe we used at the start of the chapter but put it in a code block so I can easily extract it later.\r\n\r\nHere\u0027s the prompt:\r\n\r\n\u0060\u0060\u0060text\r\nYou\u0027re a world-class chef, and you\u0027re preparing a meal. \r\nProvide a recipe for apple pie please.\r\nPut the recipe in a fenced markdown block.\r\n\u0060\u0060\u0060\r\n\r\nGPT-4o will respond with something along these lines:\r\n\r\n~~~text\r\nCertainly! Here\u0027s a classic apple pie recipe in a fenced markdown block:\r\n\r\n\u0060\u0060\u0060markdown\r\n# Classic Apple Pie Recipe\r\n\r\n## Ingredients\r\n### For the Pie Crust:\r\n- 2 \u00BD cups (310g) all-purpose flour\r\n- 1 tsp salt\r\n- 1 tsp granulated sugar\r\n- 1 cup (230g) unsalted butter, cold and cubed\r\n- 6\u20138 tbsp ice water\r\n\r\n### For the Apple Filling:\r\n- 6 cups (about five large) apples, peeled, cored, and thinly sliced\r\n- \u00BE cup (150g) granulated sugar\r\n- \u00BC cup (55g) brown sugar\r\n- 2 tbsp all-purpose flour\r\n- 1 tbsp lemon juice\r\n- 1 tsp ground cinnamon\r\n- \u00BC tsp ground nutmeg\r\n- \u00BC tsp ground allspice\r\n- Pinch of salt\r\n\r\n### For Assembly:\r\n- 1 egg, beaten (for egg wash)\r\n- 1 tbsp granulated sugar (for sprinkling, optional)\r\n\r\n... remainder of the response\r\n\u0060\u0060\u0060\r\n~~~\r\n\r\nNote that the response contains the recipe between Markdown fenced code block delimiters. We can now extract the recipe from the response using a regular expression.\r\n\r\nWhile this looks like an excellent way of working with LLMs in use cases outside of chat, there are better ways to handle structured outputs. You can instruct the LLM to generate JSON results or even call a function to output the generated code. Both of these options provide much more control over the output format. We\u0027ll cover structured output in greater detail in chapter 8.\r\n\r\nNext, let\u0027s look at how you can help the model with samples.\r\n\r\n{#few-shot-learning}\r\n### Add samples to the prompt",
        "Embedding": []
      },
      {
        "Id": 28,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "While you can reuse prompts over time, you\u0027ll have to account for the fact that LLMs get updated and retrained periodically. A prompt that worked before will likely break because the LLM was updated. Prompts that may have worked with Claude will produce different results with GPT-4o because they used a different training set.\r\n\r\n{#context-window-limits}\r\n#### Complicated prompts yield better results\r\n\r\nSome people will tell you to write complicated prompts to get better results. But you will often get better results by splitting complex tasks into smaller tasks and running them separately. This is because of how the context window of the LLM works. From [this paper][CONTEXT_WINDOW_PAPER], we can learn that LLMs have an attention span that isn\u0027t exactly what you\u0027d expect from a machine. Although the models have been updated since the paper\u0027s release, I still see this behavior in our production applications. [#s](#context-window-curve) shows the attention span of a typical LLM. Input at the start and end of a prompt is likely to get picked up better by the LLM than content in the middle of the prompt.\r\n\r\n{#context-window-curve} \r\n![Context window attention curve](context-window-curve.png)\r\n\r\nLonger and more complicated prompts can work, but if the LLM doesn\u0027t give you the desired response, it\u0027s essential to understand the context window curve and adjust your application accordingly.\r\n\r\n#### You can rely on the general knowledge captured in the LLM\r\n\r\nLLMs are trained on a lot of data. As a result, you can get a recipe for apple pie that looks very reasonable. But you can\u0027t rely on the general knowledge captured in the LLM. There\u0027s a high statistical chance of getting a reasonable response, but the LLM may lie to you convincingly. After all, we\u0027re just matching a pattern. The LLM doesn\u0027t remember facts; it just predicts the next token based on the pattern it saw.\r\n\r\nThere\u0027s a lot that can go sideways with prompts. However, understanding prompt engineering can be very helpful in building effective LLM-based applications.\r\n\r\n{#prompt-principles} \r\n## The five basics of a good prompt\r\n\r\nWriting a good prompt is hard because you have to think like the LLM you\u0027re working with. If you want to be close to 100% effective, you\u0027ll need to know:\r\n\r\n- What data the LLM was trained with so you know which language (Dutch, English, Spanish, Chinese, etc.)\r\nit understands best.\r\n- What tasks was the LLM trained on to write the prompt using the same structure and style so the pattern is as straightforward as possible?\r\n- How was the LLM [aligned][LLM_ALIGNMENT], so you know the style of text the LLM can produce?\r\n\r\nSadly, you can\u0027t know these three facts as hard facts. There are benchmarks, but I\u0027ve found that they only provide a biased guideline, as many LLM providers are likely to game them. You\u0027ll have to guess what works best because the datasets used to train LLMs aren\u0027t open-source. They\u0027re a well-guarded secret by the companies that build them.\r\n\r\nThat leaves us with five basic principles that I know from experience work well for prompts:\r\n\r\n- Provide clear direction\r\n- Specify the output format for the prompt\r\n- Add context and samples to the prompt\r\n- Keep the prompt focused on one task\r\n- Tune your prompt with hyperparameters\r\n\r\nLet\u0027s go through them one by one.\r\n\r\n### Provide clear direction\r\n\r\nTake a step back and think about how the LLM works. Remember that the LLM is a pattern-matching machine that uses embeddings (modeling patterns of relationships between words in a sentence) and an attention mechanism (modeling patterns from the context of the prompt and the response) to produce results. The LLM is only as good as the pattern it can match.",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "What is the difference between one-shot and few-shot learning?",
    "Answer": "The difference between **one-shot learning** and **few-shot learning** lies in the number of examples provided to the LLM in the prompt:\n\n- **One-shot Learning**: You provide a **single example** in the prompt to guide the model on how to perform the task.\n  - Example:\n    \u0060\u0060\u0060plaintext\n    Input: \u0022The pizza was cold\u0022\n    Output: Negative sentiment\n\n    Now classify: \u0022The service was excellent.\u0022\n    \u0060\u0060\u0060\n\n- **Few-shot Learning**: You provide **multiple examples** in the prompt to guide the model, which can be helpful for more complex tasks.\n  - Example:\n    \u0060\u0060\u0060plaintext\n    Input: \u0022The pizza was cold\u0022\n    Output: Negative sentiment\n\n    Input: \u0022The atmosphere was lovely\u0022\n    Output: Positive sentiment\n\n    Now classify: \u0022The service was excellent.\u0022\n    \u0060\u0060\u0060\n\nIn summary, the primary difference is the **number of examples** provided\u2014one example for one-shot learning, and multiple examples for few-shot learning.",
    "Context": [
      {
        "Id": 10,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "Sometimes the best way to get what you want from an LLM is to show it examples. We call\r\nthe use of examples in a prompt few-shot learning, and it comes in two flavors:\r\n\r\n**One-shot Learning** You provide a single example:\r\n\r\n\u0060\u0060\u0060plaintext\r\nInput: \u0022The pizza was cold\u0022\r\nOutput: Negative sentiment\r\n\r\nNow classify: \u0022The service was excellent.\u0022\r\n\u0060\u0060\u0060\r\n\r\n**Few-shot Learning** You provide multiple examples:\r\n\r\n\u0060\u0060\u0060plaintext\r\nInput: \u0022The pizza was cold\u0022\r\nOutput: Negative sentiment\r\n\r\nInput: \u0022The atmosphere was lovely\u0022\r\nOutput: Positive sentiment\r\n\r\nNow classify: \u0022The service was excellent.\u0022\r\n\u0060\u0060\u0060\r\n\r\nOne good example is enough, but complex tasks might need more.\r\n\r\n#### Zero-shot capabilities\r\n\r\nModern LLMs are so well-trained that they often perform tasks without examples. This is\r\ncalled the zero-shot capability of a model. You describe the structure of what you want:\r\n\r\n\u0060\u0060\u0060plaintext\r\nClassify the sentiment of this review: \u0022The service was excellent.\u0022\r\nGive me just the score.\r\n\u0060\u0060\u0060\r\n\r\nWhile I often start with zero-shot for simplicity, I\u0027m not afraid to add examples if the\r\nresults aren\u0027t quite what I need. The key is being flexible and pragmatic about which\r\napproach you use.\r\n\r\nIn the following section we\u0027ll examine core concepts for working with these models.\r\n\r\n### Core concepts\r\n\r\n#### Prompt engineering\r\n\r\nThe input you give to an LLM is called a prompt. Think of it as instructions that tell\r\nthe model what you want it to do. A prompt contains the task description and any context\r\nthe model needs to generate the correct output.\r\n\r\nHere\u0027s a simple example:\r\n\r\n**Bad prompt:**\r\n\r\n\u0060\u0060\u0060plaintext\r\n\u0022Write a function that validates email addresses.\u0022\r\n\u0060\u0060\u0060\r\n\r\n**Good prompt:**\r\n\r\n\u0060\u0060\u0060plaintext\r\n\u0022Write a C# function that validates email addresses. The function should:\r\n\r\n- Use regular expressions for validation\r\n- Return a boolean indicating if the email is valid\r\n- Handle common edge cases like missing @ symbols\u0022\r\n\u0060\u0060\u0060\r\n\r\nYou\u0027ll find many websites promoting \u0022proven\u0022 prompt patterns, often with claims like:\r\n\r\n- \u0022Always start with \u0027You are an expert...\u0027\u0022\r\n- \u0022Use this exact format for best results...\u0022\r\n- \u0022Include these specific phrases...\u0022\r\n\r\nI\u0027ve learned the hard way that while these patterns might work initially, they often\r\nbreak in unexpected ways. What works better is:\r\n\r\n- Testing your prompts thoroughly\r\n- Adjusting temperature and other parameters\r\n- Building robust error handling around the LLM\r\n- Iterating based on actual usage\r\n\r\nWe\u0027ll dive deep into practical prompt engineering in Chapter 3.\r\n\r\n#### Fine-tuning vs. prompt engineering\r\n\r\nYou might hear people talk about fine-tuning models for specific domains. Let me share\r\nmy perspective on this.\r\n\r\nFine-tuning means taking a base model (like GPT-4) and training it on your specific data\r\nto improve it at particular tasks. While this sounds appealing, there are several\r\nreasons I rarely recommend it:\r\n\r\n**Why People Consider Fine-tuning:**\r\n\r\n- Make the model more specialized for specific tasks and domains\r\n- Improve performance for domain-specific language\r\n- Ensure consistent outputs\r\n\r\n**Why I Usually Avoid It:**\r\n\r\n1. Cost\r\n\u00A0 \u00A0- Significant computing resources required\r\n\u00A0 \u00A0- High monetary investment\r\n\u00A0 \u00A0- Ongoing maintenance costs\r\n\r\n2. Complexity\r\n\u00A0 \u00A0- Requires extensive training data\r\n\u00A0 \u00A0- Technical expertise needed\r\n\u00A0 \u00A0- Time-consuming process\r\n\r\n3. Trade-offs\r\n\u00A0 \u00A0- Lose general capabilities\r\n\u00A0 \u00A0- Limited flexibility\r\n\u00A0 \u00A0- Can be overkill for most use cases\r\n\r\nInstead of fine-tuning, I recommend looking at alternatives like Retrieval Augmented\r\nGeneration (RAG). Here\u0027s a practical example:\r\n\r\nYou\u0027re building a chatbot to answer questions about your company\u0027s products. Instead of\r\nfine-tuning a model on your product documentation, you could:\r\n\r\n1. Store your documentation in a vector database\r\n2. Search for relevant information when a question comes in\r\n3. Include that information in your prompt as context\r\n4. Let the LLM generate an answer based on the provided context",
        "Embedding": []
      },
      {
        "Id": 9,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "make better decisions during the development process of your LLM-based applications.\r\n\r\n## Key concepts and terminology\r\n\r\nBefore we dive into building applications with LLMs, let\u0027s cover some essential concepts\r\nyou\u0027ll need to understand. Don\u0027t worry if some of these seem abstract at first \u2013 we\u0027ll\r\nput them into practice throughout the rest of the book.\r\n\r\n### Essential terminology\r\n\r\n#### Tokens and tokenization\r\n\r\nBefore an LLM can process your text, it needs to break it down into tokens. Think of\r\ntokens as the building blocks the model uses to understand text. A token can be:\r\n\r\n- A complete word\r\n- Part of a word\r\n- A number\r\n- A special character\r\n- A delimiter\r\n\r\nThe process works like this: your text gets split into tokens, then converted into\r\nnumbers using the model\u0027s vocabulary. This conversion is necessary because the\r\ntransformer-based neural network that powers the LLM can only process numbers, not raw\r\ntext.\r\n\r\nFor example, the word \u0022tokenization\u0022 might be split like this:\r\n\r\n\u0060\u0060\u0060plaintext\r\n\u0022tokenization\u0022 -\u003E [\u0022token\u0022, \u0022ization\u0022]\r\n\u0060\u0060\u0060\r\n\r\nWhen the model generates a response, the process happens in reverse \u2013 tokens are\r\nconverted back into text. This is why sometimes you might see slightly odd word splits\r\nin responses, especially with technical terms or rare words.\r\n\r\n#### Embeddings\r\n\r\nAt the input side of almost all LLMs is something called an embedding layer. This\r\ncomponent turns tokens into dense vectors that capture the semantic meaning of the text.\r\nThese are called embeddings or embedding vectors. Embedding vectors are interesting\r\nbecause they can represent the relationships between words in a mathematical space.\r\n\r\nThe embedding layer isn\u0027t just a random part of the model \u2013 it\u0027s trained on vast amounts\r\nof text to understand how words relate to each other based on how they are used. Think\r\nof it as a map where similar words or concepts are located close to each other.\r\n\r\nThe embedding concept can be challenging without seeing it in motion. [This\r\nwebsite][UNDERSTANDING_TRANSFORMERS] does a great job visualizing the concept.\r\n\r\nYou\u0027ll work directly with embeddings later when we implement the Retrieval Augmented\r\nGeneration (RAG) pattern in Chapter 5. For now, just know they\u0027re essential for LLMs\u0027\r\nunderstanding text.\r\n\r\n#### Context window\r\n\r\nEvery LLM has a limit to how much text it can consider at once \u2013 this is called the\r\ncontext window. It\u0027s essentially the model\u0027s short-term memory, including your input and\r\noutput.\r\n\r\nContext windows have grown significantly: Most modern commercial models handle 100K-250K\r\ninput tokens and around 4K output tokens. This translates to roughly 100K words on\r\naverage, which is the size of a full book. \r\n\r\nOpen-source models typically have smaller windows due to the more limited number of\r\nparameters in the neural network used in these models.\r\n\r\nThe exact size is found in the model\u0027s documentation (often called a model card). As\r\nwe\u0027ll see in later chapters, effectively managing this context window becomes crucial\r\nwhen building applications.\r\n\r\n{#llm-output-sampling} \r\n#### Output sampling and temperature\r\n\r\nLLMs aim to produce human-like text; one way they do this is through output sampling.\r\nWhen generating each token, the model doesn\u0027t just pick the most likely option; it\r\nsamples from a distribution of possibilities.\r\n\r\nTemperature is your main control over this sampling process:\r\n\r\n- Low temperature (0.1-0.3): More focused, deterministic responses\r\n- High temperature (0.7-0.9): More creative, varied output\r\n\r\nHere\u0027s how I typically set the temperature:\r\n\r\n- Code generation: 0.2 (we want precision)\r\n- Content creation: 0.7 (we want creativity)\r\n- Factual responses: 0.1 (we want consistency)\r\n\r\nWhile temperature is the most common setting you\u0027ll adjust, other sampling parameters\r\nare available. For a deeper dive into all the options, I recommend checking out [this\r\narticle][PARAMETER_EXPLANATION].\r\n\r\n#### Few-shot learning",
        "Embedding": []
      },
      {
        "Id": 32,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "#### Top-P\r\n\r\nLet\u0027s start with the Top-P setting, which refers to how LLMs select the output token that should come next in the response. The neural network of the LLM doesn\u0027t predict words; it predicts numbers that are then translated to tokens (words and parts of words) using a dictionary lookup. The LLM translates the numbers in the output layer to a probability distribution to select the next output token in the response. Each possible token is assigned a value representing the likelihood that it will be the next token. The higher the value, the more likely the token is to be the next token in the output.\r\n\r\nWe could select the token with the highest probability as the output from this probability distribution. However, this leads to repetitive and boring output. We can fix this by using a smarter sampling technique.\r\n\r\nAll modern LLMs use Top-P sampling. Here\u0027s how it works: First, we establish a threshold for the cumulative probability. We then sort the tokens from highest to lowest probability and start collecting tokens, adding their probabilities to a cumulative probability. We collect tokens until the cumulative probability value reaches the threshold. The process is demonstrated in [#s](#top-p-sampling).\r\n\r\n{#top-p-sampling} \r\n![Top-P sampling](probability-visualization.png)\r\n\r\nFor the samples in [#s](#top-p-sampling), we\u0027ve established a threshold of 0.9. The tokens with the highest probability are selected until the cumulative probability reaches 0.9. The tokens with the lowest probability are removed from the distribution.\r\n\r\nOnce we\u0027ve selected the candidate tokens in the distribution, we randomly select one of them as the output, resulting in a more varied output.\r\n\r\nThe key thing to remember here is that a higher Top-P results in more tokens being selected as candidates. A lower value will result in a more focused, repetitive output. This is ideal for generating code but less so for marketing content where you want greater variety.\r\n\r\n#### Temperature\r\n\r\nLLMs don\u0027t just use Top-P sampling. The inventors of these types of models found that Top-P sampling isn\u0027t enough to get natural text. So, they decided to add temperature to the mix. Temperature controls the shape of the probability distribution used to perform Top-P sampling.\r\n\r\nHere\u0027s how temperature works. Before performing the Top-P sampling, the raw values for the candidate tokens are divided by the temperature value. After that, the probability distribution is determined as usual. In [#s](#temperature-effects), you can see this in action with a before and after.\r\n\r\n{#temperature-effects} \r\n![The effect of temperature on the probability distribution](scaled-probability-comparison.png)\r\n\r\nThe higher the temperature, the softer the curve is. It takes longer for the cumulative probability function to reach the threshold, so more candidate tokens make it into the selection. Conversely, the lower the temperature, the harder the curve is. It takes less time to reach the threshold.\r\n\r\nAt this point, I understand that you\u0027re probably thinking that this is a lot to get through. And it is. I\u0027ve included an interactive notebook that demonstrates Top-P sampling and temperature scaling in the [GitHub repository][TOP_P_SAMPLE]. You can run the notebook inside Visual Studio Code to explore the effects of temperature and Top-P sampling and better understand these concepts.\r\n\r\n#### Presence Penalty\r\n\r\nAs you can tell by now, the sampling techniques used in LLMs are complex. And it gets even more complicated. Let me add one more variable to the mix: presence penalty. A positive presence penalty reduces the probability value of a token seen before in the output. This forces candidate tokens lower in the selection ranking for the Top-P sampling process. It is less likely that you see the same token in the output if you provide a positive value for the presence penalty.\r\n\r\n#### Frequency Penalty",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "What will be covered in the next chapter?",
    "Answer": "The next chapter will focus on expanding the basic LLM functionality with custom tools. This includes finalizing the fundamentals of building an LLM-based application before diving into design patterns.",
    "Context": [
      {
        "Id": 53,
        "OriginalFileName": "Content\\05-testing-and-monitoring-prompts.md",
        "Content": "In this chapter, we\u0027ve looked at how you can test and monitor your LLM-based application. We started by looking at how to test prompts in a controlled environment. We then covered how to add monitoring, build a dashboard, and collect data from the monitoring environment to improve the tests.\r\n\r\nIn the next chapter, we\u0027ll expand the basic LLM functionality with custom tools. Learning about using tools finalizes the fundamentals of building an LLM-based application before looking at design patterns.\r\n\r\n## Further reading\r\n\r\n- [GPTScore: Evaluate as you desire](https://arxiv.org/abs/2302.04166)\r\n- [G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment](https://arxiv.org/abs/2303.16634)\r\n\r\n## Running the samples in this chapter\r\n\r\nCheck out the samples for this chapter in the [Github repository][GH_SAMPLE_DIR]. Each sample has a README file containing system requirements and instructions on how to run the sample.\r\n\r\n[XUNIT_DATA_DRIVEN_TESTS]: https://hamidmosalla.com/2017/02/25/xunit-theory-working-with-inlinedata-memberdata-classdata/\r\n[G_EVAL]: https://arxiv.org/abs/2303.16634\r\n[GPTSCORE]: https://arxiv.org/pdf/2302.04166\r\n[OPEN_TELEMETRY]: https://opentelemetry.io/\r\n[GENAI_STANDARD]: https://opentelemetry.io/docs/specs/semconv/gen-ai/\r\n[AI_DOCS]: https://learn.microsoft.com/en-us/azure/azure-monitor/app/create-workspace-resource?tabs=bicep#create-a-workspace-based-resource\r\n[KUSTO_INTRODUCTION]: https://learn.microsoft.com/en-us/azure/azure-monitor/logs/log-analytics-overview\r\n[CREATE_STORAGE_ACCOUNT]: https://learn.microsoft.com/en-us/azure/storage/blobs/create-data-lake-storage-account\r\n[STORAGE_EXPLORER]: https://learn.microsoft.com/en-us/azure/storage/storage-explorer/vs-azure-tools-storage-manage-with-storage-explorer?tabs=windows\r\n[NUGET_BLOB_STORAGE]: https://www.nuget.org/packages/Azure.Storage.Blobs\r\n[MONITORING_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-05/csharp/Chapter5.ApplicationInsightsExporter\r\n[EXTRACTION_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-05/csharp/Chapter5.ExtractPromptTestData\r\n[PROMPT_TEST_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-05/csharp/Chapter5.PromptTestingBasics\r\n[MODEL_BASED_TEST_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-05/csharp/Chapter5.ModelBasedTesting\r\n[OTEL_METRICS]: https://opentelemetry.io/docs/concepts/signals/metrics/\r\n[OTEL_ASPNET]: https://medium.com/@jepozdemir/configuring-opentelemetry-tracing-for-asp-net-core-114c2c9cf557\r\n[GH_SAMPLE_DIR]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-05/",
        "Embedding": []
      },
      {
        "Id": 61,
        "OriginalFileName": "Content\\06-enhancing-llms-with-tools.md",
        "Content": "before it goes back into the LLM. Capturing data retrieved by a function is especially\r\nuseful when you want to implement the RAG pattern (see Chapter 7) and need to store\r\nreferences to the original documents found.\r\n\r\nLet\u0027s look at how you can build a function filter so you understand how to use them in\r\npatterns like the RAG design pattern. The following code demonstrates a basic filter\r\nthat logs the input and output of a function:\r\n\r\n\u0060\u0060\u0060csharp\r\nusing Microsoft.SemanticKernel;\r\n\r\npublic class LoggingFunctionFilter : IFunctionInvocationFilter\r\n{\r\n\u00A0 \u00A0 public async Task OnFunctionInvocationAsync(\r\n FunctionInvocationContext context, \r\n \u00A0 \u00A0 \u00A0 Func\u003CFunctionInvocationContext, Task\u003E next)\r\n \u00A0 {\r\n Console.WriteLine($\u0022Invoking {context.Function.Name}\u0022);\r\n\r\n\u00A0 \u00A0 \u00A0await next(context);\r\n\u00A0 \u00A0 \u00A0 \u00A0 \r\n Console.WriteLine($\u0022Done invoking {context.Function.Name}\u0022);\r\n\u00A0 }\r\n}\r\n\u0060\u0060\u0060\r\n\r\nThe code performs the following steps:\r\n\r\n1. First, we create a new class that implements \u0060IFunctionInvocationFilter\u0060.\r\n2. Next, we add a method \u0060OnFunctionInvocationAsync\u0060 that accepts the invocation context\r\n and the next filter in the chain.\r\n3. Then, in the method \u0060OnFunctionInvocationAsync\u0060, we log the name of the function\r\n that\u0027s being called before and after calling the \u0060next\u0060 filter.\r\n\r\nThere\u0027s something special happening when you look at the output in\r\n[#s](#filter-sample-output), since we\u0027re using \u0060InvokePromptAsync\u0060, we\u0027re calling a\r\nfunction wrapped around the prompt we provide. The output in shows two calls happening,\r\none for the prompt function and one for the time function.\r\n\r\n{#filter-sample-output}\r\n![Function filter sample output](filter-sample-output.png)\r\n\r\nThe example is quite simple, and I don\u0027t recommend adding a logging filter, because\r\nlogging is already built into Semantic Kernel. However, this sample does demonstrate how\r\nyou can modify the input and output of a function before and after it\u0027s called.\r\n\r\nFor example, you can access the \u0060Arguments\u0060 property on the \u0060FunctionInvocationContext\u0060\r\nto modify the function\u0027s input. You can also modify the \u0060Result\u0060 property to change the\r\nfunction\u0027s output.\r\n\r\nFinally, you can decide not to let the LLM call the function. If you don\u0027t call the\r\n\u0060next\u0060 function, it\u0027s required to provide a value for \u0060Result\u0060. Otherwise, the\r\napplication will throw an exception.\r\n\r\nFilters work best in the following cases:\r\n\r\n1. When you can\u0027t change the logic in the function itself.\r\n2. When you need to apply a transformation on all functions in the application.\r\n\r\nFunctions and filters are what make Semantic Kernel powerful. With these two constructs\r\non top of the kernel, you can build all the patterns from the upcoming chapters.\r\n\r\n## Summary\r\n\r\nIn this chapter we covered how to extend the capabilities of an LLM with functions. We\r\nlearned how to use functions to access external tools like relational databases and\r\nsearch engines. We also covered how to use functions to manipulate state outside the LLM\r\nby, for example, sending notifications.\r\n\r\nWe then covered how to work with functions in larger applications by moving them into\r\nseparate services and connect them by using the Semantic Kernel OpenAPI plugins\r\nextension.\r\n\r\nFinally, we covered how to work with filters to validate input for functions and change\r\nfunction output. We learned that filters can be useful to implement security checks into\r\nyour application.\r\n\r\nIn the next chapter, we\u0027ll use the skills obtained in this chapter to learn how to apply\r\nRetrieval Augmented Generation (RAG) to build intelligent chat applications that can use\r\nyour organization\u0027s internal information to generate useful responses to questions of\r\nusers.\r\n\r\n## Running the samples in this chapter\r\n\r\nCheck out the samples for this chapter in the [Github repository][GH_SAMPLE_DIR]. Each sample has a README file containing system requirements and instructions on how to run the sample. [CODE_BASED_FUNCTIONS_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-06/csharp/Chapter6.CodeBasedFunctions\r\n[OPENAPI_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-06/csharp/Chapter6.ExternalApiPlugins\r\n[LOGIC_APPS]: https://learn.microsoft.com/en-us/semantic-kernel/concepts/plugins/adding-logic-apps-as-plugins\r\n[GH_SAMPLE_DIR]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-06/",
        "Embedding": []
      },
      {
        "Id": 73,
        "OriginalFileName": "Content\\07-retrieval-augmented-generation.md",
        "Content": "The code that runs this prompt is part of a bigger program that performs a couple of steps:\r\n\r\n1. First, the program chunks the content as we would normally when indexing content in the vector database.\r\n2. Next, the program iterates over each generated chunk and run the prompt with the content of each chunk.\r\n\r\nI recommend storing the used context, the question, and the answer in the dataset so you can verify the quality of the generated dataset later on.\r\n\r\nThe full source code for the validation dataset is stored in the [GitHub repository][GEN_VAL_DATA_SAMPLE], feel free to take a look at that and use it for your own projects. In the sample code you\u0027ll find several variations on the question generation prompt that. I\u0027ve included one to generate true/false statements, and one to generate short answers.\r\n\r\nNow that we have a validation dataset, let\u0027s take a look at generating responses that we can use to measure faithfulness of the RAG system.\r\n\r\n### Generating test samples\r\n\r\nFor this step in the evaluation workflow we\u0027ll take the code we wrote in [#s](#using-the-vector-store-with-a-prompt) use it with some modifications to generate test samples.\r\n\r\nThe main program code looks like this:\r\n\r\n\u0060\u0060\u0060csharp\r\n\r\n\u0060\u0060\u0060\r\n\r\nIn this code we perform the following steps:\r\n\r\n1. First, we load the validation dataset we created earlier.\r\n2. Next, we use the \u0060QuestionAnsweringTool\u0060 to generate a response for each question in the dataset.\r\n3. Finally, we store the question, the answer, and the used context in a CSV file.\r\n\r\nThe code for this step of the validation workflow can also be found on [GitHub][GH_TEST_SAMPLES]. It includes a few more details related to setting up the kernel instance, and code to handle indexing of the content similar to how we did it in [#s](#using-the-vector-store-with-a-prompt).\r\n\r\nWith the test samples generated, we can move on to writing some Python code to measure the faithfulness of the RAG pipelines using Ragas.\r\n\r\n### Measuring faithfulness with Ragas\r\n\r\nThe following code shows how to use Ragas to measure the faithfulness of the RAG pipeline:\r\n\r\n\u0060\u0060\u0060python\r\n\r\n\u0060\u0060\u0060\r\n\r\nIn this code we perform the following steps:\r\n\r\n1. First, we load the test samples we generated earlier.\r\n2. Next, we use the \u0060Ragas\u0060 library to measure the faithfulness of the RAG pipeline.\r\n3. Finally, we print out the results in the terminal.\r\n\r\nThe code for this step of the validation workflow can also be found on [GitHub][GH_RAGAS_SAMPLE]. It includes a few more details related to setting up the kernel instance, and code to handle indexing of the content similar to how we did it in [#s](#using-the-vector-store-with-a-prompt).\r\n\r\n\r\n\r\n## Summary\r\n\r\nIn this chapter we covered a lot around the RAG pattern, one of the first design patterns we\u0027ll discuss in the book. We talked about the components involved in the RAG pattern and how to implement an end-to-end RAG pattern. We also covered how to test various parts of the RAG pattern with a mix of unit-tests, manual tests, and model-based tests. Finally, we talked about about variations on the regular RAG pattern with including how you can use tools and a regular database to quickly find relevant information to base a response on.\r\n\r\nThe next chapter we\u0027ll shift our focus towards using LLMs to generate structured output. It will be less involved than implementing a RAG pattern, I promise, but it will still be fun, and essential if you want to build more complicated workflows as we continue working towards chapter 9 and 10 of the book.\r\n\r\n[SAMPLE_SOURCE_1]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-07/Chapter7.RetrievalAugmentedGeneration/\r\n[SO_BLOG]: https://stackoverflow.blog/2024/12/27/breaking-up-is-hard-to-do-chunking-in-rag-applications/\r\n[VALIDATION_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-07/Chapter7.PipelineValidation/ [GH_TEST_SAMPLES]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-07/Chapter7.TestSampleGeneration/\r\n[GH_RAGAS_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-07/Chapter7.RagasValidation/",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "Should performance improvements focus more on code optimizations around the LLM rather than on user experience?",
    "Answer": "No, performance improvements should not focus more on code optimizations around the LLM than on user experience. The book emphasizes that the LLM itself will be the biggest bottleneck in your application, and you cannot speed it up significantly. Instead, the focus should be on improving the user experience, such as implementing strategies like streaming responses to provide quick feedback to users. Additionally, while the rest of the code should still be optimized to avoid adding unnecessary delays, the impact of those optimizations will be relatively minor compared to addressing the inherent slowness of the LLM.",
    "Context": [
      {
        "Id": 16,
        "OriginalFileName": "Content\\02-essential-llmops-knowledge.md",
        "Content": "When I can, I\u0027ll examine monitoring data during early testing to determine how many\r\ntokens per minute we need and set the quota accordingly. You need to strike a balance\r\nbetween costs and user experience here because rate limits help you control costs, too.\r\nThe lower the quota, the lower the amount of tokens that can be spent per minute, and\r\nthe lower the bill. However, too many retries can lead to a bad user experience.\r\n\r\nIn [#s](#the-art-and-nonsense-of-prompt-engineering), we\u0027ll cover configuring parameters\r\nfor your prompt to maximize your quota.\r\n\r\nRate limiting can have a significant impact on the performance of your application. However, there are sources of performance issues that we need to cover.\r\n\r\n## LLM performance and the user experience of your application\r\n\r\nLLMs are very slow, and you need to think about how to work around that\r\nlimitation because you can\u0027t speed them up yourself. You can increase rate limits to\r\nspeed things up a little, but that\u0027s about it.\r\n\r\nLet me shift your perspective about performance in software applications. Under normal\r\ncircumstances, you need to look for performance bottlenecks in all areas of your\r\napplication. And it\u0027s likely you can solve many of those bottlenecks yourself. When\r\nusing an LLM in your application, it is important to know that the LLM will be your\r\nbiggest bottleneck, and you can\u0027t speed it up by much.\r\n\r\nThere are two things that you need to think about when optimizing LLM-based application\r\nperformance:\r\n\r\n- First, you must think about ways to improve the user experience. For example, you can\r\nlet the LLM stream the response to a prompt. This way, the user receives an answer in\r\nchunks but quickly.\r\n- Second, you need to think less about the performance of the code around the call to an\r\nLLM. It won\u0027t matter as much what you do there because the LLM is the slowest piece in\r\nthe puzzle.\r\n\r\nDo keep in mind that you\u0027ll want to optimize the rest of the code that doesn\u0027t call the\r\nLLM to be fast; otherwise, you\u0027re just adding to the problem. I\u0027ve found that if the\r\nrest of the application is nice and fast, users are more forgiving about the LLM being\r\nslow.\r\n\r\n{#llomops-failover-strategies}\r\n## Failover strategies\r\n\r\nAs if performance wasn\u0027t hard enough, we got another nice present from the LLM\r\nproviders. LLM providers are sometimes unavailable because you\u0027ve hit a hard limit or\r\nbecause they\u0027re overloaded. Anthropic had this problem frequently when I wrote this\r\nchapter, where you receive overloaded errors and can only call the LLM again after a few\r\nhours. This can be problematic when you run a production application that needs some\r\nform of availability.\r\n\r\nIn many cases, I\u0027ve found that letting the user know that the LLM is unavailable is\r\nenough if you have no availability requirements. Many of my clients don\u0027t have any\r\navailability requirements because they\u0027re just starting out and know that they can\u0027t\r\nguarantee 100% uptime.\r\n\r\nFrom experience, I learned that despite the lack of availability requirements, it\u0027s\r\nbeneficial to have a failover strategy in place. For example, you could have GPT-4o as a\r\nfallback for Claude Sonnet in your application. This way, you can switch to GPT-4o when\r\nClaude Sonnet is unavailable.\r\n\r\nRegardless of what your client thinks, it\u0027s good to spend some time planning for\r\nfailover scenarios, as this significantly impacts the user experience and the\r\ndevelopment effort you need to spend.\r\n\r\n{#llmops-security} \r\n## Security and privacy in an LLM-based application\r\n\r\nWe\u0027ve covered quite a few LLMOps essentials. You may have noticed that not many of these\r\naspects are well understood by users of LLM-based applications. This is especially true\r\nfor security and privacy concerns.\r\n\r\nI\u0027ve not had many security discussions with clients about LLMs. This is because we\u0027re\r\njust getting started using LLMs in production. Not many people realize how dangerous",
        "Embedding": []
      },
      {
        "Id": 14,
        "OriginalFileName": "Content\\02-essential-llmops-knowledge.md",
        "Content": "Let\u0027s dive into each of these aspects in more detail to understand what they mean for your LLM-based application starting with testing LLM-based applications.\r\n\r\n## Testing LLM-based applications\r\n\r\nTesting LLM-based applications is different from testing regular software applications.\r\nIn regular software applications, you can use tests that assert a true or false\r\nstatement. Typically, you\u0027ll write a combination of unit tests, integration tests, and\r\npossibly even acceptance tests to validate that your application is working as expected,\r\nusing a boolean approach to verify that some rules hold for your system.\r\n\r\nYou need to approach testing LLM-based applications differently from regular software.\r\nIt isn\u0027t as black and white. LLMs give you a different response every time because of\r\nwhat we discussed in [#s](#understanding-llms); they use output sampling techniques. You\r\nneed a different testing approach to validate that your LLM-based application is\r\nworking.\r\n\r\nWhere unit and integration tests should already be standard practice, you must apply one\r\nextra layer of tests for LLM-based applications. LLM-based applications need\r\ntests that verify the structure of the LLMs output.\r\n\r\nVerifying the structure of the LLM output is challenging because the output is non-deterministic. You won\u0027t get the same answer every time you run the test. So, instead of testing for hard results, you want the correct answer most of the time. We\u0027re trying to lower our solution\u0027s risk as much as possible. We must accept that, just like regular software, the system sometimes fails for unknown reasons. But we have a good idea of where the problem is coming from for an LLM-based application.\r\n\r\nOne testing strategy that works great in an LLM-based is running multiple samples\r\nthrough the application and then having the test cast a majority vote to determine if\r\nthe call to the LLM produces the correct output.\r\n\r\nAnother great strategy is to use metrics with a value range to determine that the output\r\nis within acceptable limits for your chosen metric.\r\n\r\nI can imagine that knowing just these rules for testing LLM-based applications generates\r\nmore questions than answers. For now, it\u0027s essential to understand that you need to\r\nchange your perspective on testing. In [#s](#prompt-testing-and-monitoring) we\u0027ll cover\r\nhow to test prompts in more detail.\r\n\r\nYou can consider testing as a way to ensure you have a safety net to catch problems\r\nbefore they happen in production. Monitoring is the second safety net you need to catch\r\nproblems while running in production.\r\n\r\n{#llmops-monitoring}\r\n## Monitoring and evaluation of your application\r\n\r\nThere are two essential aspects to monitoring LLM solutions. Unsurprisingly, you must\r\nmonitor for any infrastructure and technical problems with your code. This is more of a\r\nDevOps practice than an LLMOps practice.\r\n\r\nThe LLMOps approach to monitoring focuses on the interaction between your application\r\nand the LLM. Machine-learning models like LLMs tend to fail silently in production.\r\nPeople change their interaction patterns as the model is used, and the model starts to\r\ngenerate less accurate responses. This shift is called model drift or concept drift.\r\n\r\nWhen applying patterns like Retrieval Augmented Generation (RAG), tracking interactions\r\nbetween the LLM and external data sources is essential. As information changes in your\r\ndata sources, the LLM may generate less accurate responses, which you could call data\r\ndrift.\r\n\r\nWe\u0027ll cover monitoring in greater detail in [#s](#prompt-testing-and-monitoring).\r\n\r\nMonitoring interactions is essential to keeping your LLM-based application running smoothly in production. When setting up monitoring, I recommend including monitoring costs as well.\r\n\r\n## Cost management and optimization\r\n\r\nCost management is a big part of LLMOps. When you host an LLM-based application in the\r\ncloud, you can quickly run into high costs if you build just the right kind of logic so",
        "Embedding": []
      },
      {
        "Id": 5,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "Once I saw what LLMs could do, I went overboard with ChatGPT. I used it for everything\r\ntext-related. I even messed up two blog posts on my website with negative feedback to\r\nshow for it. LLMs produce flavorless and pretty mediocre content. They\u0027re trained to\r\nrepresent the average of what language has to offer. And that\u0027s pretty average and\r\nflavorless.\r\n\r\nI used LLMs for coding, too, as I am a developer. I wrote a complete application using\r\nonly AI. And it\u0027s used in production today. But it was hard to get there. The LLM\r\nfrequently steered into the wall with weird layouts and useless unit tests. I haven\u0027t\r\nbothered measuring how quickly I built the application. I was quicker but less satisfied\r\nwith the result because writing great code is a skill I\u0027m proud of.\r\n\r\nAfter learning about open-source LLMs, I decided to try them, too. It was very slow,\r\neven on a beefy Intel Core i9 machine with a massive graphics card. I quickly learned\r\nthat you need a lot of power to run an LLM on your machine and in the cloud. And with a\r\nprice tag of 3500 euros for a decent machine, it\u0027s not something you want to do for a\r\nhobby project.\r\n\r\nThere are plenty more experiences where I found the boundaries of what LLMs can do, but\r\nlet me finish with one final example. I tried using an LLM to upgrade program code from\r\na low-code solution to Typescript without human supervision. We quickly had to add human\r\nsupervision because it wouldn\u0027t help us without that.\r\n\r\n### Key lessons learned\r\n\r\nYou might wonder, why am I telling you my experiences? There are three key lessons that\r\nI want you to keep in mind while reading this book:\r\n\r\n1. Be specific in what you ask from the LLM. Don\u0027t just ask for an article about LLMs; provide specific instructions.\r\n2. Always review and understand the output of the LLM. Don\u0027t let your users use the output of the LLM unseen. The output will be wrong in all the weird ways you\u0027ve never thought of.\r\n3. Break big problems down into smaller problems. Instead of asking the LLM to perform 10 steps, ask it for just one step. It will be easier for the LLM to perform and easier for you to debug.\r\n4. Keep track of the context and provide it in focused chunks. LLMs have limited input and output space, so they can\u0027t keep track of a complete book or even a blog post.\r\n\r\n### Evolution of my understanding\r\n\r\nAfter the initial rollercoaster ride with LLMs, my understanding of them evolved. I\r\nstopped seeing them as a silver bullet that could solve all my language-related problems\r\nand started seeing them as a powerful pattern-matching engine capable of transforming\r\ntext.\r\n\r\nOne crucial moment was realizing that LLMs excel at clearly defined tasks that involve\r\nmatching a pattern in the source text and transforming it into other text. If you can\r\nfind a clear pattern in the input and clearly define the target structure, an LLM is\r\nlikely a good solution to your problem. The less clear the problem statement is, the\r\nmore issues you\u0027ll experience.\r\n\r\nYou\u0027ll want to balance humans and machines well to build a practical LLM application.\r\nHuman oversight is essential when using an LLM. Throughout the rest of the book, you\r\nwill find that I\u0027m using interaction patterns that promote human oversight because it\u0027s\r\nnecessary and improves the experience significantly.\r\n\r\n### How I integrated LLMs into real projects\r\n\r\nClearly defined problems and human oversight are essential when you view an LLM-based\r\napplication from a functional perspective. From a technical standpoint, you must\r\nconsider applying LLMs as a software engineering problem with an AI aspect rather than a\r\npure AI project.\r\n\r\nHere are three reasons why you should use a software engineering approach:\r\n\r\n- LLMs will behave better when you follow a structured approach. The more structure, the better.\r\n- LLM behavior changes when providers push new versions of the models; automated testing is your friend.",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "Does the manual always fit within the LLM\u0027s context window?",
    "Answer": "No, the manual does not always fit within the LLM\u0027s context window. As explained in the provided context, every LLM has a limit to how much text it can consider at once, called the **context window**. If the manual exceeds the size of the context window, it will not fit, requiring techniques such as breaking the input into focused chunks or using mechanisms like Retrieval Augmented Generation (RAG) to manage the context efficiently.",
    "Context": [
      {
        "Id": 9,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "make better decisions during the development process of your LLM-based applications.\r\n\r\n## Key concepts and terminology\r\n\r\nBefore we dive into building applications with LLMs, let\u0027s cover some essential concepts\r\nyou\u0027ll need to understand. Don\u0027t worry if some of these seem abstract at first \u2013 we\u0027ll\r\nput them into practice throughout the rest of the book.\r\n\r\n### Essential terminology\r\n\r\n#### Tokens and tokenization\r\n\r\nBefore an LLM can process your text, it needs to break it down into tokens. Think of\r\ntokens as the building blocks the model uses to understand text. A token can be:\r\n\r\n- A complete word\r\n- Part of a word\r\n- A number\r\n- A special character\r\n- A delimiter\r\n\r\nThe process works like this: your text gets split into tokens, then converted into\r\nnumbers using the model\u0027s vocabulary. This conversion is necessary because the\r\ntransformer-based neural network that powers the LLM can only process numbers, not raw\r\ntext.\r\n\r\nFor example, the word \u0022tokenization\u0022 might be split like this:\r\n\r\n\u0060\u0060\u0060plaintext\r\n\u0022tokenization\u0022 -\u003E [\u0022token\u0022, \u0022ization\u0022]\r\n\u0060\u0060\u0060\r\n\r\nWhen the model generates a response, the process happens in reverse \u2013 tokens are\r\nconverted back into text. This is why sometimes you might see slightly odd word splits\r\nin responses, especially with technical terms or rare words.\r\n\r\n#### Embeddings\r\n\r\nAt the input side of almost all LLMs is something called an embedding layer. This\r\ncomponent turns tokens into dense vectors that capture the semantic meaning of the text.\r\nThese are called embeddings or embedding vectors. Embedding vectors are interesting\r\nbecause they can represent the relationships between words in a mathematical space.\r\n\r\nThe embedding layer isn\u0027t just a random part of the model \u2013 it\u0027s trained on vast amounts\r\nof text to understand how words relate to each other based on how they are used. Think\r\nof it as a map where similar words or concepts are located close to each other.\r\n\r\nThe embedding concept can be challenging without seeing it in motion. [This\r\nwebsite][UNDERSTANDING_TRANSFORMERS] does a great job visualizing the concept.\r\n\r\nYou\u0027ll work directly with embeddings later when we implement the Retrieval Augmented\r\nGeneration (RAG) pattern in Chapter 5. For now, just know they\u0027re essential for LLMs\u0027\r\nunderstanding text.\r\n\r\n#### Context window\r\n\r\nEvery LLM has a limit to how much text it can consider at once \u2013 this is called the\r\ncontext window. It\u0027s essentially the model\u0027s short-term memory, including your input and\r\noutput.\r\n\r\nContext windows have grown significantly: Most modern commercial models handle 100K-250K\r\ninput tokens and around 4K output tokens. This translates to roughly 100K words on\r\naverage, which is the size of a full book. \r\n\r\nOpen-source models typically have smaller windows due to the more limited number of\r\nparameters in the neural network used in these models.\r\n\r\nThe exact size is found in the model\u0027s documentation (often called a model card). As\r\nwe\u0027ll see in later chapters, effectively managing this context window becomes crucial\r\nwhen building applications.\r\n\r\n{#llm-output-sampling} \r\n#### Output sampling and temperature\r\n\r\nLLMs aim to produce human-like text; one way they do this is through output sampling.\r\nWhen generating each token, the model doesn\u0027t just pick the most likely option; it\r\nsamples from a distribution of possibilities.\r\n\r\nTemperature is your main control over this sampling process:\r\n\r\n- Low temperature (0.1-0.3): More focused, deterministic responses\r\n- High temperature (0.7-0.9): More creative, varied output\r\n\r\nHere\u0027s how I typically set the temperature:\r\n\r\n- Code generation: 0.2 (we want precision)\r\n- Content creation: 0.7 (we want creativity)\r\n- Factual responses: 0.1 (we want consistency)\r\n\r\nWhile temperature is the most common setting you\u0027ll adjust, other sampling parameters\r\nare available. For a deeper dive into all the options, I recommend checking out [this\r\narticle][PARAMETER_EXPLANATION].\r\n\r\n#### Few-shot learning",
        "Embedding": []
      },
      {
        "Id": 5,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "Once I saw what LLMs could do, I went overboard with ChatGPT. I used it for everything\r\ntext-related. I even messed up two blog posts on my website with negative feedback to\r\nshow for it. LLMs produce flavorless and pretty mediocre content. They\u0027re trained to\r\nrepresent the average of what language has to offer. And that\u0027s pretty average and\r\nflavorless.\r\n\r\nI used LLMs for coding, too, as I am a developer. I wrote a complete application using\r\nonly AI. And it\u0027s used in production today. But it was hard to get there. The LLM\r\nfrequently steered into the wall with weird layouts and useless unit tests. I haven\u0027t\r\nbothered measuring how quickly I built the application. I was quicker but less satisfied\r\nwith the result because writing great code is a skill I\u0027m proud of.\r\n\r\nAfter learning about open-source LLMs, I decided to try them, too. It was very slow,\r\neven on a beefy Intel Core i9 machine with a massive graphics card. I quickly learned\r\nthat you need a lot of power to run an LLM on your machine and in the cloud. And with a\r\nprice tag of 3500 euros for a decent machine, it\u0027s not something you want to do for a\r\nhobby project.\r\n\r\nThere are plenty more experiences where I found the boundaries of what LLMs can do, but\r\nlet me finish with one final example. I tried using an LLM to upgrade program code from\r\na low-code solution to Typescript without human supervision. We quickly had to add human\r\nsupervision because it wouldn\u0027t help us without that.\r\n\r\n### Key lessons learned\r\n\r\nYou might wonder, why am I telling you my experiences? There are three key lessons that\r\nI want you to keep in mind while reading this book:\r\n\r\n1. Be specific in what you ask from the LLM. Don\u0027t just ask for an article about LLMs; provide specific instructions.\r\n2. Always review and understand the output of the LLM. Don\u0027t let your users use the output of the LLM unseen. The output will be wrong in all the weird ways you\u0027ve never thought of.\r\n3. Break big problems down into smaller problems. Instead of asking the LLM to perform 10 steps, ask it for just one step. It will be easier for the LLM to perform and easier for you to debug.\r\n4. Keep track of the context and provide it in focused chunks. LLMs have limited input and output space, so they can\u0027t keep track of a complete book or even a blog post.\r\n\r\n### Evolution of my understanding\r\n\r\nAfter the initial rollercoaster ride with LLMs, my understanding of them evolved. I\r\nstopped seeing them as a silver bullet that could solve all my language-related problems\r\nand started seeing them as a powerful pattern-matching engine capable of transforming\r\ntext.\r\n\r\nOne crucial moment was realizing that LLMs excel at clearly defined tasks that involve\r\nmatching a pattern in the source text and transforming it into other text. If you can\r\nfind a clear pattern in the input and clearly define the target structure, an LLM is\r\nlikely a good solution to your problem. The less clear the problem statement is, the\r\nmore issues you\u0027ll experience.\r\n\r\nYou\u0027ll want to balance humans and machines well to build a practical LLM application.\r\nHuman oversight is essential when using an LLM. Throughout the rest of the book, you\r\nwill find that I\u0027m using interaction patterns that promote human oversight because it\u0027s\r\nnecessary and improves the experience significantly.\r\n\r\n### How I integrated LLMs into real projects\r\n\r\nClearly defined problems and human oversight are essential when you view an LLM-based\r\napplication from a functional perspective. From a technical standpoint, you must\r\nconsider applying LLMs as a software engineering problem with an AI aspect rather than a\r\npure AI project.\r\n\r\nHere are three reasons why you should use a software engineering approach:\r\n\r\n- LLMs will behave better when you follow a structured approach. The more structure, the better.\r\n- LLM behavior changes when providers push new versions of the models; automated testing is your friend.",
        "Embedding": []
      },
      {
        "Id": 40,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "In this chapter, we\u0027ve examined the art and nonsense of prompt engineering. We discussed the important role prompts play in LLM-based applications and how to write effective and safe prompts.\r\n\r\nIn the next chapter, we\u0027ll look at testing and monitoring prompts in your LLM-based application.\r\n\r\n## Running the samples in this chapter\r\n\r\nCheck out the samples for this chapter in the [Github repository][GH_SAMPLE_DIR]. Each sample has a README file containing system requirements and instructions on how to run the sample.\r\n\r\n[CONTEXT_WINDOW_PAPER]: https://arxiv.org/abs/2307.03172\r\n[LLM_ALIGNMENT]: https://medium.com/@madalina.lupu.d/align-llms-with-reinforcement-learning-from-human-feedback-595d61f160d5\r\n[TOP_P_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/notebooks\r\n[SK_TEMPLATE_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-04/csharp/Chapter4.SemanticKernelTemplates\r\n[HB_TEMPLATE_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-04/csharp/Chapter4.HandleBarsTemplates\r\n[KF_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-04/csharp/Chapter4.KernelFunctionPrompts\r\n[HB_MANUAL]: https://handlebarsjs.com/guide/\r\n[STREAMING_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-04/csharp/Chapter4.StreamingChatCompletions\r\n[HISTORY_REDUCERS]: https://learn.microsoft.com/en-us/semantic-kernel/concepts/ai-services/chat-completion/chat-history?pivots=programming-language-csharp#chat-history-reduction\r\n[AZ_PII_DETECTION]: https://learn.microsoft.com/en-us/azure/ai-services/language-service/personally-identifiable-information/how-to-call\r\n[GA_PII_DETECTION]: https://cloud.google.com/sensitive-data-protection/docs/deidentify-sensitive-data\r\n[AWS_PII_DETECTION]: https://docs.aws.amazon.com/comprehend/latest/dg/how-pii.html\r\n[OWASP_LLM]: https://owasp.org/www-project-top-10-for-large-language-model-applications/\r\n[MITRE]: https://atlas.mitre.org/matrices/ATLAS/\r\n[GH_SAMPLE_DIR]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-04/\r\n[INCONTEXT_LEARNING]: https://ai.stanford.edu/blog/understanding-incontext/",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "How did the author\u0027s perception of LLMs evolve over time?",
    "Answer": "The author\u0027s perception of LLMs evolved significantly over time, moving from initial skepticism and over-reliance to a more nuanced understanding of their capabilities and limitations.\n\n1. **Initial Enthusiasm and Overuse**: The author initially went \u0022overboard\u0022 with LLMs, using them for a wide range of tasks like writing blog posts and coding entire applications. However, this led to mixed results. Blog posts generated by LLMs were criticized for being \u0022flavorless and pretty mediocre,\u0022 and while coding applications using LLMs was faster, the process produced unsatisfactory results, with issues like \u0022weird layouts and useless unit tests.\u0022\n\n2. **Recognition of Limitations**: Through these experiences, the author realized that LLMs have inherent limitations. They are best at tasks involving well-defined patterns and clear instructions. When problems were vague or complex, the LLMs struggled, requiring human oversight to achieve acceptable results.\n\n3. **Shift in Perspective**: Over time, the author stopped viewing LLMs as a \u0022silver bullet\u0022 and began to see them as powerful tools for pattern matching and text transformation. The realization that LLMs excel at clearly defined tasks marked a crucial turning point in their understanding.\n\n4. **Balanced Approach**: The author now emphasizes the importance of balancing human oversight with LLM usage. They advocate for treating LLM-based applications as software engineering problems with an AI aspect, rather than purely AI projects. This approach involves breaking down tasks, reviewing outputs, and maintaining structured, context-aware workflows.\n\nIn summary, the author\u0027s perception evolved from overestimating LLMs\u0027 abilities to appreciating their strengths as pattern-matching engines, while also acknowledging their limitations and the necessity of human intervention and structured engineering practices.",
    "Context": [
      {
        "Id": 5,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "Once I saw what LLMs could do, I went overboard with ChatGPT. I used it for everything\r\ntext-related. I even messed up two blog posts on my website with negative feedback to\r\nshow for it. LLMs produce flavorless and pretty mediocre content. They\u0027re trained to\r\nrepresent the average of what language has to offer. And that\u0027s pretty average and\r\nflavorless.\r\n\r\nI used LLMs for coding, too, as I am a developer. I wrote a complete application using\r\nonly AI. And it\u0027s used in production today. But it was hard to get there. The LLM\r\nfrequently steered into the wall with weird layouts and useless unit tests. I haven\u0027t\r\nbothered measuring how quickly I built the application. I was quicker but less satisfied\r\nwith the result because writing great code is a skill I\u0027m proud of.\r\n\r\nAfter learning about open-source LLMs, I decided to try them, too. It was very slow,\r\neven on a beefy Intel Core i9 machine with a massive graphics card. I quickly learned\r\nthat you need a lot of power to run an LLM on your machine and in the cloud. And with a\r\nprice tag of 3500 euros for a decent machine, it\u0027s not something you want to do for a\r\nhobby project.\r\n\r\nThere are plenty more experiences where I found the boundaries of what LLMs can do, but\r\nlet me finish with one final example. I tried using an LLM to upgrade program code from\r\na low-code solution to Typescript without human supervision. We quickly had to add human\r\nsupervision because it wouldn\u0027t help us without that.\r\n\r\n### Key lessons learned\r\n\r\nYou might wonder, why am I telling you my experiences? There are three key lessons that\r\nI want you to keep in mind while reading this book:\r\n\r\n1. Be specific in what you ask from the LLM. Don\u0027t just ask for an article about LLMs; provide specific instructions.\r\n2. Always review and understand the output of the LLM. Don\u0027t let your users use the output of the LLM unseen. The output will be wrong in all the weird ways you\u0027ve never thought of.\r\n3. Break big problems down into smaller problems. Instead of asking the LLM to perform 10 steps, ask it for just one step. It will be easier for the LLM to perform and easier for you to debug.\r\n4. Keep track of the context and provide it in focused chunks. LLMs have limited input and output space, so they can\u0027t keep track of a complete book or even a blog post.\r\n\r\n### Evolution of my understanding\r\n\r\nAfter the initial rollercoaster ride with LLMs, my understanding of them evolved. I\r\nstopped seeing them as a silver bullet that could solve all my language-related problems\r\nand started seeing them as a powerful pattern-matching engine capable of transforming\r\ntext.\r\n\r\nOne crucial moment was realizing that LLMs excel at clearly defined tasks that involve\r\nmatching a pattern in the source text and transforming it into other text. If you can\r\nfind a clear pattern in the input and clearly define the target structure, an LLM is\r\nlikely a good solution to your problem. The less clear the problem statement is, the\r\nmore issues you\u0027ll experience.\r\n\r\nYou\u0027ll want to balance humans and machines well to build a practical LLM application.\r\nHuman oversight is essential when using an LLM. Throughout the rest of the book, you\r\nwill find that I\u0027m using interaction patterns that promote human oversight because it\u0027s\r\nnecessary and improves the experience significantly.\r\n\r\n### How I integrated LLMs into real projects\r\n\r\nClearly defined problems and human oversight are essential when you view an LLM-based\r\napplication from a functional perspective. From a technical standpoint, you must\r\nconsider applying LLMs as a software engineering problem with an AI aspect rather than a\r\npure AI project.\r\n\r\nHere are three reasons why you should use a software engineering approach:\r\n\r\n- LLMs will behave better when you follow a structured approach. The more structure, the better.\r\n- LLM behavior changes when providers push new versions of the models; automated testing is your friend.",
        "Embedding": []
      },
      {
        "Id": 17,
        "OriginalFileName": "Content\\02-essential-llmops-knowledge.md",
        "Content": "LLMs are. I could write an entire book about this topic, but I\u0027ll have to keep this\r\nshorter because we need to talk about other things. But let me give you the basics so\r\nyou know what you\u0027re getting yourself into.\r\n\r\nThere are three main topics when we talk about security in LLMs:\r\n\r\n- **Data privacy:** Customers and users will talk about data privacy because they\u0027re\r\nworried about the provider stealing their data to train the LLM.\r\n- **Application security:** This is about protecting your application from being hacked.\r\nThis is a big topic because LLMs are very powerful and can be used to generate malicious\r\ncontent.\r\n- **User safety:** This is about protecting your users from being exposed to harmful\r\ncontent generated by the LLM. LLMs can generate harmful content or misleading content\r\nthat can be used to manipulate people.\r\n\r\nLet me go over each of these topics to give you a direction on how to think about them.\r\n\r\n{#llmops-data-privacy} \r\n### Data privacy\r\n\r\nBefore you start using an LLM, you need to think about the data you\u0027re sending to it.\r\n\r\nThe [Samsung incident][SAMSUNG_INCIDENT] teaches us an important lesson: If your data is\r\ngoing to be used for training, it is likely to end up on someone else\u0027s desk at some\r\npoint in the future. Hackers are actively probing with prompts to find out what data is\r\nbeing used to train the LLM.\r\n\r\nSometimes, it\u0027s better not to use an LLM if you\u0027re worried about company secrets being\r\nleaked. For most cases, however, it\u0027s good enough to agree with the LLM provider and ask\r\nit not to use your data for training. All LLM Providers offer this option today and have\r\na section about it in their legal agreements.\r\n\r\nIt is essential to understand that security is a trade-off. You can\u0027t have 100% security\r\nbecause then you wouldn\u0027t use LLMs in the first place and miss out on their value.\r\nHowever, you can\u0027t wholly live without security, either. Have an active discussion with\r\nyour client about the data you\u0027re sending to the LLM and the risks involved, and then\r\ndecide based on that.\r\n\r\nThere\u0027s another closely related topic to data privacy that I want to address here. In\r\nincreasingly increasing countries, some laws require you to be careful with personal\r\nidentifiable information (PII). For example, in Europe, you need to comply with GDPR. In\r\nCalifornia, you need to comply with CCPA. These laws require you to be careful with the\r\ndata you\u0027re processing.\r\n\r\nManaging personal data is a topic you should address alongside the other security\r\nrequirements when planning an LLM-based application.\r\n\r\nMany cloud providers offer standard tools to filter out PII from text. For example,\r\nAzure has a tool called [Text Analytics][PII_FILTER] that can filter out PII from text.\r\nThis way, you can avoid sending PII to the LLM or storing it in your application.\r\nConsider it an extra safety measure on top of ensuring people know they\u0027re not supposed\r\nto send in PII.\r\n\r\n{#llmops-application-security} \r\n### Application Security\r\n\r\nSecuring modern cloud-native applications is challenging. Adding an LLM to the mix opens\r\nup a new chapter in application security. And it\u0027s a relatively new chapter, so there\u0027s\r\nnot much information on securing an LLM-based application. I will cover some LLM\r\nsecurity strategies around security in greater detail in the following chapters, but I\r\nwant to make sure you have a good overview of the topic first.\r\n\r\nApplying a deep defense strategy is the best way to handle application security. You\u0027ll\r\nwant to use multiple layers of defenses to ensure that if a hacker gets through one\r\nlayer, it will have to work hard to get through the next. A layered defense also helps\r\nkeep an attack\u0027s blast radius to a minimum.\r\n\r\nOne of the best moves you can make in terms of application security is not giving your\r\napplication access to resources it shouldn\u0027t have access to. I see many people using\r\nCopilot 365 and oversharing information because of bad defaults and the admin not being",
        "Embedding": []
      },
      {
        "Id": 4,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "Think about how you understand this sentence: \u0022The developer copy-pasted the code into\r\nChatGPT to generate unit tests for it.\u0022. You automatically know \u0022it\u0022 refers to the code,\r\nnot the tool ChatGPT. Transformers can make these connections, too, and they can do it\r\nat scale.\r\n\r\n### How LLMs Work\r\n\r\nAt their core, LLMs predict what comes next based on what they\u0027ve seen before. When you\r\ngive an LLM input text, it\u0027s not searching through a database for answers. Instead, it\u0027s\r\nusing its understanding of patterns to generate responses word by word.\r\n\r\nHere\u0027s a simplified view of what happens:\r\n\r\n1. Your input gets broken down into tokens (pieces of words or characters)\r\n2. The model looks at these tokens and their relationships using attention mechanisms\r\n3. It predicts the most likely next token based on its training\r\n4. This process repeats until it generates a complete response\r\n\r\nThe magic happens in how these models handle context. The attention mechanism helps\r\npredict the combined context of input and (the to-be-generated) output. When you feed\r\ntokens into the model, the attention mechanism state is updated with the current\r\nunderstanding of the context that we\u0027re working on. Based on this information, the model\r\ncan now more reliably predict the next likely token.\r\n\r\n### Why this matters for software development\r\n\r\nYou might think, \u0022Okay, cool technology, but why should I care as a developer?\u0022 Here\u0027s\r\nwhy:\r\n\r\nFirst, LLMs are changing how we write code. They\u0027re not just glorified autocomplete \u2013\r\nthey can understand intent. When working on a new feature, I can describe what I want in\r\nplain English, and an LLM can help me scaffold the code, suggest test cases, or point\r\nout potential issues.\r\n\r\nSo, even if you\u0027re not building AI applications, it will change how your tools work and\r\nhow quickly and effectively you can write code.\r\n\r\nSecond, and this is what this book is about, they\u0027re enabling new types of applications.\r\nThink about all the tasks that were too complex or expensive to automate before because\r\nthey required understanding natural language. Now, we can build applications that can:\r\n\r\n- Generate human-like responses to customer inquiries\r\n- Upgrade code bases from deprecated frameworks or old languages to more modern equivalents \r\n- Translate raw input from field reports into a coherent and actionable management summary\r\n- Help review a document that you wrote by providing valuable suggestions to improve it\r\n\r\nAnd we\u0027re just getting started!\r\n\r\n### The current reality\r\n\r\nLet\u0027s be clear, though \u2013 LLMs aren\u0027t magic. They have fundamental limitations. They can\r\nprovide you with some pretty surprising responses; the response can look right but\r\ncontain the wrong information, and you need to apply careful engineering to use LLMs in\r\nproduction systems.\r\n\r\nThat\u0027s precisely why understanding how they work is so crucial. When you know what\u0027s\r\nhappening under the hood, you can:\r\n\r\n- Design better prompts that get more reliable results\r\n- Build safeguards against standard failure modes\r\n- Create hybrid systems that combine LLM capabilities with traditional programming\r\n- Make informed decisions about when (and when not) to use LLMs\r\n\r\nIn the next sections, we\u0027ll explore these practical aspects more deeply. But first, let\r\nme share some real examples from my own journey with LLMs\u2014including the mistakes I\r\nmade\u2014so you don\u0027t have to repeat them.\r\n\r\n## My journey with LLMs\r\n\r\nMy journey with LLMs started with a healthy dose of \u0022this isn\u0027t going to work.\u0022 Like\r\nmany of you, I didn\u0027t understand the impact of this new technology.\r\n\r\nThe unit testing experiment changed everything for me. Not because it solved all my\r\nunit-testing needs, but because it showed me that LLMs could understand context and\r\ngenerate meaningful, helpful output. This wasn\u0027t just pattern matching or template\r\nfilling \u2013 it was something completely different.\r\n\r\n### Early experiments",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "What argument does the \u0060AnswerAsync\u0060 method take?",
    "Answer": "The \u0060AnswerAsync\u0060 method takes a single argument: a string called \u0060question\u0060.",
    "Context": [
      {
        "Id": 68,
        "OriginalFileName": "Content\\07-retrieval-augmented-generation.md",
        "Content": "[\u0022question\u0022] = question,\r\n\u00A0 \u00A0 [\u0022fragments\u0022] = fragments\r\n});\r\n\r\nreturn response.GetValue\u003Cstring\u003E()!;\r\n\u0060\u0060\u0060\r\n\r\nIn the method, we\u0027re performing the following steps:\r\n\r\n1. First, we\u0027ll lookup the collection containing the preprocessed text units in the vector store. We\u0027ll use this collection to search for relevant text units.\r\n2. Next, we generate an embedding vector for the question using the same embedding model we used to create embedding vectors for the text units.\r\n3. Then, we perform a cosine similarity search to find the most relevant text units for the question. We\u0027ll ask for three text units to be returned.\r\n4. Next, we process the results into a list of text units to be inserted into the prompt.\r\n5. Then, we load up the \u0060answer-question.yaml\u0060 prompt from the file system.\r\n6. Finally, we execute the prompt with the retrieved text and the user question and return the output.\r\n\r\nYou may be wondering what the prompt looks like. The prompt is stored in a YAML file following the structure discussed earlier in [#s](#prompt-templates). The Content of the \u0060answer-question.yaml\u0060 file looks like this:\r\n\r\n\u0060\u0060\u0060yaml\r\nname: answer_question\r\ntemplate: |\r\n  You\u0027re a helpful assistant, supporting me by answering questions \r\n  about the book building effective llm-based applications with\r\n  semantic kernel. Answer the question using the provided context.\r\n  If you don\u0027t know the answer, say so, don\u0027t make up \r\n  answers.\r\n\r\n  ## Context\r\n  {{#each fragments}}\r\n  {{ .Content }}\r\n  {{/each}}\r\n\r\n  ## Question\r\n\r\n  {{question}}\r\ntemplate_format: handlebars\r\ninput_variables:\r\n\u00A0 - name: fragments\r\n\u00A0 \u00A0 description: The topic you want to discuss in the blog post.\r\n\u00A0 \u00A0 is_required: true\r\n\u00A0 - name: question\r\n\u00A0 \u00A0 description: The question you want to ask about the topic.\r\n\u00A0 \u00A0 is_required: true \u00A0\r\nexecution_settings:\r\n\u00A0 default:\r\n\u00A0 \u00A0 top_p: 0.98\r\n\u00A0 \u00A0 temperature: 0.7\r\n\u00A0 \u00A0 presence_penalty: 0.0\r\n\u00A0 \u00A0 frequency_penalty: 0.0\r\n\u00A0 \u00A0 max_tokens: 1200\r\n\u0060\u0060\u0060\r\n\r\nIn this prompt, we\u0027re telling the LLM we\u0027re answering questions about this book. We then provide the fragments using a \u0060foreach\u0060 loop rendering the \u0060Content\u0060 property of the \u0060TextUnit\u0060 class. Finally, we give the question that we need an answer to.\r\n\r\nThe rest of the file lists the input variables for the prompt and the execution settings for the LLM.\r\n\r\nTo use the question-answering tool, we can hook up the required components in the \u0060Program.cs\u0060 file of the project:\r\n\r\n\u0060\u0060\u0060csharp\r\n// Setup logic for semantic kernel\r\n\r\nbuilder.Services.AddTransient\u003CContentIndexer\u003E();\r\nbuilder.Services.AddTransient\u003CQuestionAnsweringTool\u003E();\r\n\r\nvar app = builder.Build();\r\n\r\napp.MapGet(\u0022/answer\u0022, async (\r\n\u00A0 \u00A0 [FromServices] QuestionAnsweringTool tool, [FromQuery] string question) =\u003E\r\n{\r\n    return await tool.AnswerAsync(question);\r\n});\r\n\r\nvar scope = app.Services.CreateScope();\r\nvar indexer = scope.ServiceProvider.GetRequiredService\u003CContentIndexer\u003E();\r\n\r\nawait indexer.ProcessContentAsync();\r\n\r\n// The rest of Program.cs\r\n\u0060\u0060\u0060\r\n\r\nThe code in this fragment must be added after configuring the kernel in the service collection. The code performs the following steps:\r\n\r\n1. First, we register the content indexer and answering tool so we can use both.\r\n2. Next, we build the web application and map a new endpoint \u0060/answer\u0060 that takes a query parameter \u0060question\u0060. The endpoint uses the \u0060QuestionAnsweringTool\u0060 to generate a response to the question.\r\n3. Then, we create a new scope to get the Content indexer from the service provider.\r\n4. Finally, we process the Content to store it in the vector store.\r\n\r\nThe rest of the \u0060Program.cs\u0060 file remains the same.\r\n\r\nYou can now run the application and ask a question from the browser by navigating to\r\n\u0060http://localhost:\u003Cport\u003E/answer?question=What\u002Bis\u002Bthe\u002BRAG\u002Bpattern\u0060. Ensure the port matches the one shown in the terminal when you start the application.\r\n\r\nThe application should return a response to the question you asked based on the Content of the book.",
        "Embedding": []
      },
      {
        "Id": 67,
        "OriginalFileName": "Content\\07-retrieval-augmented-generation.md",
        "Content": "ITextEmbeddingGenerationService embeddingGenerator,\r\n{\r\n\u00A0 \u00A0 public async Task ProcessContentAsync()\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 ulong currentIdentifier = 1L;\r\n\u00A0 \u00A0 \u00A0 \u00A0 \r\n\u00A0 \u00A0 \u00A0 \u00A0 var files = Directory.GetFiles(\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u0022Content\u0022, \u0022*.md\u0022, SearchOption.AllDirectories);\r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 var collection = vectorStore.GetCollection\u003Culong, TextUnit\u003E(\u0022content\u0022);\r\n\u00A0 \u00A0 \u00A0 \u00A0 await collection.CreateCollectionIfNotExistsAsync();\r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 foreach (var file in files)\r\n\u00A0 \u00A0 \u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 var lines = await File.ReadAllLinesAsync(file);\r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 var chunks = TextChunker.SplitMarkdownParagraphs(\r\n                lines, maxTokensPerParagraph: 1000);\r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 foreach (var chunk in chunks)\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 var embedding = \r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 await embeddingGenerator.GenerateEmbeddingAsync(chunk);\r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 var textUnit = new TextUnit\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 {\r\n                    Content = chunk,\r\n                    Embedding = embedding,\r\n                    OriginalFileName = file,\r\n                    Id = currentIdentifier\u002B\u002B\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 };\r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 await collection.UpsertAsync(textUnit);\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 }\r\n\u00A0 \u00A0 \u00A0 \u00A0 }\r\n\u00A0 \u00A0 }\r\n}\r\n\u0060\u0060\u0060\r\n\r\nA lot is going on in the \u0060ContentIndexer\u0060 class, so let\u0027s break it down:\r\n\r\n1. First, we create a new class, \u0060ContentIndexer,\u0060 which depends on the vector store and the embedding generator.\r\n2. Next, we create a new method to process content called \u0060ProcessContentAsync\u0060\r\n3. Then, we list all markdown files in the \u0060Content\u0060 directory.\r\n4. After that, we ensure we have a collection called \u0060content\u0060 to store the processed \u0060TextUnit\u0060 instances in\r\n5. Next, we loop over all markdown files and create chunks of 1000 tokens each.\r\n6. Then, we generate an embedding vector for the chunks.\r\n7. After, we create a new \u0060TextUnit\u0060 instance and store it in the vector store.\r\n\r\nThe \u0060ContentIndexer\u0060 class is a basic implementation of a content processing tool. I recommend applying a retry mechanism for embedding generation. You\u0027ll also want to make sure that you can reprocess a file if you encounter a transient error. There is nothing worse than having to start over the whole indexing process because of a single failure.\r\n\r\nNow that we have the content indexer let\u0027s finish up the RAG pipeline by building the question-answering tool.\r\n\r\n{#using-the-vector-store-with-a-prompt}\r\n### Using the vector store with a prompt\r\n\r\nThe generation component of the RAG pattern implementation we\u0027re working on is formed by the \u0060QuestionAnsweringTool\u0060. This class is a C# class that looks like this:\r\n\r\n\u0060\u0060\u0060csharp\r\npublic class QuestionAnsweringTool(\r\n    Kernel kernel, IVectorStore vectorStore,\r\n    ITextEmbeddingGenerationService embeddingGenerator)\r\n{\r\n    public async Task\u003Cstring\u003E AnswerAsync(string question)\r\n    {\r\n\u00A0 \u00A0 \u00A0 \u00A0 //Content for the method\r\n    }\r\n}\r\n\u0060\u0060\u0060\r\n\r\nLet\u0027s go over this code step by step:\r\n\r\n1. First, we\u0027re creating a class that depends on the vector store we used earlier, the embedding generator, and the kernel we need to use for generating output.\r\n2. Next, we create a new method, \u0060AnswerAsync\u0060, that takes a question as input and produces an answer as output.\r\n\r\nThe code for the \u0060AnswerAsync\u0060 method looks like this:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar collection = vectorStore.GetCollection\u003Culong, TextUnit\u003E(\u0022Content\u0022);\r\n\r\nvar questionEmbedding = await embeddingGenerator.GenerateEmbeddingAsync(\r\n question);\r\n\r\nvar searchOptions = new VectorSearchOptions\r\n{\r\n Top = 3,\r\n};\r\n\r\nvar searchResponse = await collection.VectorizedSearchAsync(\r\n    questionEmbedding, searchOptions);\r\n\r\nvar fragments = new List\u003CTextUnit\u003E();\r\n\r\nawait foreach (var fragment in searchResponse.Results)\r\n{\r\n    fragments.Add(fragment.Record);\r\n}\r\n\r\nvar promptTemplateContent = File.ReadAllText(\u0022Prompts/answer-question.yaml\u0022);\r\n\r\nvar promptTemplate = kernel.CreateFunctionFromPromptYaml(\r\n    promptTemplateContent, new HandlebarsPromptTemplateFactory());\r\n\r\nvar response = await promptTemplate.InvokeAsync(kernel, new KernelArguments\r\n{",
        "Embedding": []
      },
      {
        "Id": 37,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "var serviceSelection = new PromptExecutionSettings()\r\n{\r\n    ServiceId = \u0022azure_openai\u0022\r\n};\r\n\r\nvar result = await kernel.InvokeAsync(prompt,\r\n    arguments: new KernelArguments(serviceSelection)\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 [\u0022dish\u0022] = \u0022pizza\u0022,\r\n\u00A0 \u00A0 \u00A0 \u00A0 [\u0022ingredients\u0022] = new List\u003Cstring\u003E\r\n\u00A0 \u00A0 \u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u0022pepperoni\u0022,\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u0022mozzarella\u0022,\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u0022spinach\u0022 \r\n\u00A0 \u00A0 \u00A0 \u00A0 }\r\n\u00A0 \u00A0 });\r\n\u0060\u0060\u0060\r\n\r\nIn this code, we stack an additional set of execution settings on top of the ones specified in the YAML file. You only need to select the name of the LLM provider you want to use in the \u0060ServiceId\u0060 property of the \u0060PromptExecutionSettings\u0060 object and pass the execution settings into the arguments of the \u0060InvokeAsync\u0060 method.\r\n\r\nSwitching between LLM providers is now as simple as setting a different value for the \u0060ServiceId\u0060 property in the execution settings.\r\n\r\nThe YAML format does have some limitations. I\u0027ve found that it is sensitive to mismatches in spaces and tabs. Editing the template text in the YAML file can be a bit cumbersome. But it\u0027s nice to have the option to store prompts in a single file with multiple sets of execution settings.\r\n\r\nWorking with multiple LLM providers is experimental at the time of writing. You must add \u0060\u003CNoWarn\u003ESKEXP0001\u003C/NoWarn\u003E\u0060 to your project file\u0027s \u0060PropertyGroup\u0060 section to suppress the build error telling you that the feature is experimental. The following code shows how to turn off the warning:\r\n\r\n\u0060\u0060\u0060xml\r\n\u003CPropertyGroup\u003E\r\n    \u003CNoWarn\u003ESKEXP0001\u003C/NoWarn\u003E\r\n\u003C/PropertyGroup\u003E\r\n\u0060\u0060\u0060\r\n\r\n{#working-with-chat-completion}\r\n## Using the chat history to your advantage\r\n\r\nIn [#s](#prompt-templates), we discussed using single prompts with Semantic Kernel. While this is useful for non-chat-based scenarios, I think it\u0027s important to also discuss chat-based scenarios. When talking about chat-based scenarios, you have to keep in mind that you don\u0027t necessarily need a chat user interface. We\u0027re strictly talking about a list of prompts and responses that form a conversation.\r\n\r\nYou\u0027ll need a different approach when working with chat history. We\u0027re no longer dealing with a scenario where we must generate a single response. Instead, we\u0027ll build a conversation that can have multiple turns and use that as the central unit of content to work with.\r\n\r\nLet\u0027s look at the chat history object first, as this will be the central unit we\u0027ll work with. You can build a conversation using the following piece of code:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar chatHistory = new ChatHistory();\r\n\r\nchatHistory.AddSystemMessage(\r\n\u00A0 \u00A0 \u0022You\u0027re a digital chef; help me cook. Your name is Flora.\u0022\r\n);\r\n\r\nchatHistory.AddUserMessage(\r\n\u00A0 \u00A0 \u0022Hi, I\u0027d like a nice recipe for a French-style apple pie.\u0022\r\n);\r\n\u0060\u0060\u0060\r\n\r\nThis code performs the following steps:\r\n\r\n1. First, we\u0027ll create a new chat history object.\r\n2. Then, we add a new system message containing instructions for the assistant.\r\n3. Finally, we add the first user message asking for an apple pie recipe.\r\n\r\nThe system instructions ensure that the LLM generates responses that are in line with the application\u0027s goal. They typically also provide instructions regarding the style of communication and things that we don\u0027t like to see in the response.\r\n\r\nYou can have three different types of messages in the chat history:\r\n\r\n1. System - Reserved for providing system-level instructions that should not be visible to a user.\r\n2. User - Content submitted by the user\r\n3. Assistant - Content generated by the LLM\r\n\r\nYou can add as many messages as you like to the chat history. Once we have a chat history object, we can use it to generate a response using the \u0060IChatCompletionService\u0060. The following code fragment demonstrates how this works:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar chatCompletionService = \r\n    kernel.Services.GetService\u003CIChatCompletionService\u003E();\r\n\r\nvar response = await chatCompletionService!\r\n    .GetChatMessageContentsAsync(chatHistory);\r\n\r\nConsole.WriteLine(response[0].Content);\r\n\u0060\u0060\u0060\r\n\r\nThe code fragment performs the following steps:",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "Does the example suggest that failover strategies cannot be supported using multiple execution settings in a YAML file?",
    "Answer": "No, the example suggests that failover strategies **can** be supported using multiple execution settings in a YAML file. It explicitly explains how you can configure different execution settings for various LLM providers in a single YAML file. For instance, the YAML file includes both a \u0060default\u0060 execution setting and an \u0060azure_openai\u0060 execution setting. This allows you to switch between providers by specifying the \u0060ServiceId\u0060 when invoking the kernel function, enabling failover scenarios.",
    "Context": [
      {
        "Id": 36,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "There\u0027s a lot to unpack here. Let\u0027s go over the essential properties:\r\n\r\n1. \u0060name\u0060 Determines the name of the kernel function that we\u0027ll create from the YAML\r\nfile.\r\n2. \u0060template\u0060 Contains the prompt template for the prompt. This is the same as the prompt template we used in the previous sections.\r\n3. \u0060template_format\u0060 Specifies the template format we use for the YAML prompt.\r\n4. \u0060input_variables\u0060 Describe the expected input data for the prompt.\r\n5. \u0060execution_settings\u0060 Describe the hyperparameters for the prompt.\r\n\r\nDepending on your use case, you\u0027ll want to configure more settings. For a complete description of the YAML format, you can refer to the [Semantic Kernel documentation](https://learn.microsoft.com/en-us/semantic-kernel/concepts/prompts/yaml-schema).\r\n\r\nYou\u0027ll need to add the \u0060Microsoft.SemanticKernel.Yaml\u0060 package to your project to use the YAML-based prompt files. After you\u0027ve added it to your project, you can load the YAML-based prompt with the following code:\r\n\r\n\u0060\u0060\u0060csharp\r\n\r\nvar promptTemplate = File.ReadAllText(\r\n    Path.Join(Directory.GetCurrentDirectory(), \u0022prompt.yaml\u0022)\r\n);\r\n\r\nvar prompt = kernel.CreateFunctionFromPromptYaml(\r\n    promptTemplate, \r\n\u00A0 \u00A0 new HandlebarsPromptTemplateFactory());\r\n\u0060\u0060\u0060\r\n\r\nIn this code fragment, we load the prompt YAML file from the disk and then use the \u0060CreateFunctionFromPromptYaml\u0060 method on the kernel to create a kernel function from the YAML file. As the Handlebars format we\u0027ve used for the prompt template isn\u0027t readily available, we have to explicitly tell Semantic Kernel that we want to use it for the prompt template.\r\n\r\nThe \u0060prompt\u0060 variable now contains a kernel function you can use in your application.\r\n\r\nThe YAML format is interesting because it allows you to store prompts with their execution settings in a single file. I find the \u0060execution_settings\u0060 option the best option for this format because you can configure different execution settings depending on the LLM provider you\u0027re using.\r\n\r\nLet me explain why having multiple execution settings is helpful with a bit more detail. Remember from [#s](#llomops-failover-strategies) that having a failover option in your application can be beneficial. Using the YAML file, you can specify multiple execution settings for different LLM providers.\r\n\r\nTo support the failover scenario, we need to modify the YAML file to include extra execution settings:\r\n\r\n\u0060\u0060\u0060yaml\r\nname: GenerateRecipe\r\ndescription: Generates a recipe based on ingredients in your fridge\r\ntemplate: |\r\n   Help me cook something nice, give me a recipe for {{ dish }}.\r\n   Use the ingredients I have in the fridge: \r\n\r\n\u00A0  {{#each ingredients}}\r\n\u00A0 \u00A0- {{ . }}\r\n \u00A0 {{/each}}\r\ntemplate_format: handlebars\r\ninput_variables:\r\n\u00A0 - name: dish\r\n\u00A0 \u00A0 description: The name of the dish you want to make\r\n\u00A0 \u00A0 is_required: true\r\n\u00A0 - name: ingredients\r\n\u00A0 \u00A0 description: A list of ingredient names you have in the fridge\r\n\u00A0 \u00A0 is_required: true\r\nexecution_settings:\r\n\u00A0 default:\r\n\u00A0 \u00A0 top_p: 0.98\r\n\u00A0 \u00A0 temperature: 0.7\r\n\u00A0 \u00A0 presence_penalty: 0.0\r\n\u00A0 \u00A0 frequency_penalty: 0.0\r\n\u00A0 \u00A0 max_tokens: 1200\r\n\u00A0 azure_openai:\r\n\u00A0 \u00A0 top_p: 0.9\r\n\u00A0 \u00A0 temperature: 0.7\r\n\u00A0 \u00A0 presence_penalty: 0.0\r\n\u00A0 \u00A0 frequency_penalty: 0.0\r\n\u00A0 \u00A0 max_tokens: 1200\r\n\u0060\u0060\u0060\r\n\r\nThe first set of execution settings specifies defaults for all LLM providers. The second set of execution settings is only valid for an LLM provider we registered with a service ID of \u0060azure_openai\u0060.\r\n\r\nWhen creating a kernel, we can add a chat completion service with a Service ID. Here\u0027s the code to do so:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar kernel = Kernel.CreateBuilder()\r\n\u00A0 \u00A0 .AddAzureOpenAIChatCompletion(\r\n        configuration[\u0022LanguageModel:DeploymentName\u0022]!,\r\n        endpoint: configuration[\u0022LanguageModel:Endpoint\u0022]!,\r\n        apiKey: configuration[\u0022LanguageModel:ApiKey\u0022]!,\r\n        serviceId: \u0022azure_openai\u0022\r\n\u00A0 \u00A0 )\r\n\u00A0 \u00A0 .Build();\r\n\u0060\u0060\u0060\r\n\r\nNow, when we want to execute a YAML-based prompt with the Azure OpenAI LLM provider, we can use the following code:\r\n\r\n\u0060\u0060\u0060csharp",
        "Embedding": []
      },
      {
        "Id": 35,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "The full source for this sample is in the [GitHub repository][HB_TEMPLATE_SAMPLE], which includes instructions on how to run it yourself.\r\n\r\n{#reusable-prompts}\r\n### Maximizing the reuse of prompts in your application\r\n\r\nWhen you\u0027ve tried the samples and debugged them in VSCode or your favorite IDE, you\u0027ll have noticed that the output of \u0060InvokePromptAsync\u0060 is a \u0060FunctionResult\u0060. Prompts in Semantic Kernel are turned into callable C# functions called Kernel Functions.\r\n\r\nWhy would Semantic Kernel do this? Compiling prompts down to program functions helps make the prompts reusable as program logic. You can store compiled prompts in your program logic, reducing the amount of code you need to run a prompt.\r\n\r\nIf you use the same prompts repeatedly in your program, it\u0027s helpful to upgrade the prompts from the coding pattern we used in the previous sections to a reusable kernel function. Let me show you how to create a kernel function from a prompt with the following code:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar promptTemplate = File.ReadAllText(\r\n    Path.Join(Directory.GetCurrentDirectory(), \u0022prompt.txt\u0022)\r\n);\r\n\r\nvar executionSettings = new AzureOpenAIPromptExecutionSettings\r\n{\r\n    MaxTokens = 1200,\r\n    Temperature = 0.5,\r\n    TopP = 1.0,\r\n    FrequencyPenalty = 0.0,\r\n    PresencePenalty = 0.0\r\n};\r\n\r\nvar prompt = kernel.CreateFunctionFromPrompt(\r\n    promptTemplate, templateFormat: \u0022handlebars\u0022,\r\n    promptTemplateFactory: new HandlebarsPromptTemplateFactory(),\r\n    executionSettings: executionSettings);\r\n\r\nvar result = await kernel.InvokeAsync(prompt, new KernelArguments\r\n{\r\n\u00A0 \u00A0 [\u0022dish\u0022] = \u0022pizza\u0022,\r\n\u00A0 \u00A0 [\u0022ingredients\u0022] = new List\u003Cstring\u003E\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u0022pepperoni\u0022,\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u0022mozzarella\u0022,\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u0022spinach\u0022\r\n\u00A0 \u00A0 }\r\n});\r\n\u0060\u0060\u0060\r\n\r\nLet\u0027s go through this code to understand the differences from earlier code samples:\r\n\r\n1. First, we load the prompt template from the disk.\r\n2. Next, we specify the execution settings we want to use for the prompt.\r\n3. Then, we call \u0060CreateFunctionFromPrompt\u0060 instead of invoking the prompt directly. The\r\ninput for the function contains the prompt template, the prompt template factory we want to use, and the execution settings.\r\n4. Finally, we can invoke the new function using \u0060InvokeAsync\u0060 on the \u0060kernel\u0060 object\r\nand pass the arguments to the function.\r\n\r\nIn the sample, we store the function in a \u0060prompt\u0060 variable. In production code, you can store the prompt as a private variable of a class that wraps around the Semantic Kernel code.\r\n\r\nIn [#s](#enhancing-llms-with-tools), we\u0027ll explore other patterns for efficiently making reusable prompts available to your application.\r\n\r\nI\u0027ve made sure that the code for building a kernel function is available in the [GitHub repository][KF_SAMPLE] so you can explore it in greater depth.\r\n\r\nKernel functions are a nice step towards fully reusable prompts. But if you want to make your business logic more readable, there\u0027s one more step.\r\n\r\n{#yaml-based-prompts} \r\n### Using YAML-based prompt configuration\r\n\r\nAs prompts come with additional settings, consider storing the prompt configuration with the prompt in a dedicated file. In Semantic Kernel, you can use YAML files. Let me demonstrate what the YAML format for a prompt looks like:\r\n\r\n\u0060\u0060\u0060yaml\r\nname: GenerateRecipe\r\ndescription: Generates a recipe based on ingredients in your fridge\r\ntemplate: |\r\n    Help me cook something nice, give me a recipe for {{ dish }}. \r\n    Use the ingredients I have in the fridge: \r\n\r\n  \u00A0 {{#each ingredients}}\r\n\u00A0 \u00A0 - {{ . }}\r\n  \u00A0 {{/each}}\r\ntemplate_format: handlebars\r\ninput_variables:\r\n\u00A0 - name: dish\r\n\u00A0 \u00A0 description: The name of the dish you want to make\r\n\u00A0 \u00A0 is_required: true\r\n\u00A0 - name: ingredients\r\n\u00A0 \u00A0 description: A list of ingredient names you have in the fridge\r\n\u00A0 \u00A0 is_required: true\r\nexecution_settings:\r\n\u00A0 default:\r\n\u00A0 \u00A0 top_p: 0.98\r\n\u00A0 \u00A0 temperature: 0.7\r\n\u00A0 \u00A0 presence_penalty: 0.0\r\n\u00A0 \u00A0 frequency_penalty: 0.0\r\n\u00A0 \u00A0 max_tokens: 1200\r\n\u0060\u0060\u0060",
        "Embedding": []
      },
      {
        "Id": 37,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "var serviceSelection = new PromptExecutionSettings()\r\n{\r\n    ServiceId = \u0022azure_openai\u0022\r\n};\r\n\r\nvar result = await kernel.InvokeAsync(prompt,\r\n    arguments: new KernelArguments(serviceSelection)\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 [\u0022dish\u0022] = \u0022pizza\u0022,\r\n\u00A0 \u00A0 \u00A0 \u00A0 [\u0022ingredients\u0022] = new List\u003Cstring\u003E\r\n\u00A0 \u00A0 \u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u0022pepperoni\u0022,\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u0022mozzarella\u0022,\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u0022spinach\u0022 \r\n\u00A0 \u00A0 \u00A0 \u00A0 }\r\n\u00A0 \u00A0 });\r\n\u0060\u0060\u0060\r\n\r\nIn this code, we stack an additional set of execution settings on top of the ones specified in the YAML file. You only need to select the name of the LLM provider you want to use in the \u0060ServiceId\u0060 property of the \u0060PromptExecutionSettings\u0060 object and pass the execution settings into the arguments of the \u0060InvokeAsync\u0060 method.\r\n\r\nSwitching between LLM providers is now as simple as setting a different value for the \u0060ServiceId\u0060 property in the execution settings.\r\n\r\nThe YAML format does have some limitations. I\u0027ve found that it is sensitive to mismatches in spaces and tabs. Editing the template text in the YAML file can be a bit cumbersome. But it\u0027s nice to have the option to store prompts in a single file with multiple sets of execution settings.\r\n\r\nWorking with multiple LLM providers is experimental at the time of writing. You must add \u0060\u003CNoWarn\u003ESKEXP0001\u003C/NoWarn\u003E\u0060 to your project file\u0027s \u0060PropertyGroup\u0060 section to suppress the build error telling you that the feature is experimental. The following code shows how to turn off the warning:\r\n\r\n\u0060\u0060\u0060xml\r\n\u003CPropertyGroup\u003E\r\n    \u003CNoWarn\u003ESKEXP0001\u003C/NoWarn\u003E\r\n\u003C/PropertyGroup\u003E\r\n\u0060\u0060\u0060\r\n\r\n{#working-with-chat-completion}\r\n## Using the chat history to your advantage\r\n\r\nIn [#s](#prompt-templates), we discussed using single prompts with Semantic Kernel. While this is useful for non-chat-based scenarios, I think it\u0027s important to also discuss chat-based scenarios. When talking about chat-based scenarios, you have to keep in mind that you don\u0027t necessarily need a chat user interface. We\u0027re strictly talking about a list of prompts and responses that form a conversation.\r\n\r\nYou\u0027ll need a different approach when working with chat history. We\u0027re no longer dealing with a scenario where we must generate a single response. Instead, we\u0027ll build a conversation that can have multiple turns and use that as the central unit of content to work with.\r\n\r\nLet\u0027s look at the chat history object first, as this will be the central unit we\u0027ll work with. You can build a conversation using the following piece of code:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar chatHistory = new ChatHistory();\r\n\r\nchatHistory.AddSystemMessage(\r\n\u00A0 \u00A0 \u0022You\u0027re a digital chef; help me cook. Your name is Flora.\u0022\r\n);\r\n\r\nchatHistory.AddUserMessage(\r\n\u00A0 \u00A0 \u0022Hi, I\u0027d like a nice recipe for a French-style apple pie.\u0022\r\n);\r\n\u0060\u0060\u0060\r\n\r\nThis code performs the following steps:\r\n\r\n1. First, we\u0027ll create a new chat history object.\r\n2. Then, we add a new system message containing instructions for the assistant.\r\n3. Finally, we add the first user message asking for an apple pie recipe.\r\n\r\nThe system instructions ensure that the LLM generates responses that are in line with the application\u0027s goal. They typically also provide instructions regarding the style of communication and things that we don\u0027t like to see in the response.\r\n\r\nYou can have three different types of messages in the chat history:\r\n\r\n1. System - Reserved for providing system-level instructions that should not be visible to a user.\r\n2. User - Content submitted by the user\r\n3. Assistant - Content generated by the LLM\r\n\r\nYou can add as many messages as you like to the chat history. Once we have a chat history object, we can use it to generate a response using the \u0060IChatCompletionService\u0060. The following code fragment demonstrates how this works:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar chatCompletionService = \r\n    kernel.Services.GetService\u003CIChatCompletionService\u003E();\r\n\r\nvar response = await chatCompletionService!\r\n    .GetChatMessageContentsAsync(chatHistory);\r\n\r\nConsole.WriteLine(response[0].Content);\r\n\u0060\u0060\u0060\r\n\r\nThe code fragment performs the following steps:",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "What did the example recipe JSON format in the text include?",
    "Answer": "The example recipe JSON format in the text included the following structure:\n\n- **Title**: The recipe\u0027s name.\n- **Ingredients**: A list of ingredients, where each ingredient is represented as an object with:\n  - \u0060unit\u0060: The unit of measurement for the ingredient (e.g., tbsp, grams).\n  - \u0060amount\u0060: The quantity of the ingredient.\n  - \u0060name\u0060: The name of the ingredient.\n- **Instructions**: A string containing the steps to prepare the recipe.\n\nHere is the example JSON format provided in the text:\n\n\u0060\u0060\u0060json\n{\n   \u0022title\u0022: \u0022The recipe title\u0022,\n   \u0022ingredients\u0022: [\n       { \u0022unit\u0022: \u0022tbsp\u0022, \u0022amount\u0022: 1, \u0022name\u0022: \u0022the name of the ingredient\u0022 },\n       { \u0022unit\u0022: \u0022grams\u0022, \u0022amount\u0022: 12, \u0022name\u0022: \u0022another ingredient\u0022 }\n   ],\n   \u0022instructions\u0022: \u0022The instructions to prepare the recipe\u0022\n}\n\u0060\u0060\u0060",
    "Context": [
      {
        "Id": 29,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "Vague instructions aren\u0027t going to work. You may get lucky, but you\u0027re more likely to get too much variation in response as more users use your application. And that\u0027s okay for chat applications like ChatGPT, but it\u0027s not for business applications where users expect high-quality results.\r\n\r\nTwo things help generate a pattern that the LLM can match. First, it\u0027s beneficial to provide the LLM with a clear context. For example, you can give it a persona to provide the context or tell it what project/task you\u0027re working on. Second, it\u0027s essential to provide the LLM with detailed instructions about the response\u0027s content, style, and structure.\r\n\r\nThe context helps set the embeddings and attention mechanism of the LLM, so it starts searching for output tokens in the correct word space. For example, the persona of a chef will likely lead the LLM in the direction of food, recipes, cooking techniques, and so on. A copywriter persona primes the embeddings and attention mechanism in the direction of marketing, text writing, and publishing. In the case of the copywriter, you\u0027ll also need to provide the medium in which the text is intended. Copywriting is different for a blog post than it is for a book.\r\n\r\nThe detailed instructions help the LLM follow a specific style pattern. For example, if you ask for short responses, the LLM is likelier to produce short outputs.\r\n\r\n### Specify the output format for the prompt\r\n\r\nNext, we have to consider the prompt\u0027s output format. In chat applications, it\u0027s often enough not to specify the output format at all. LLMs are trained as chatbots, and they will produce conversational output. But if you want to use the LLM for other tasks, it\u0027s essential to tell the LLM precisely what output format you want.\r\n\r\nFor example, you can tell the LLM to output a list of items or a table or to put a specific portion of the output in a fenced Markdown code block. This outputs the results of the prompt between \\\u0060\\\u0060\\\u0060 and \\\u0060\\\u0060\\\u0060 so it\u0027s easily parsable by your application.\r\n\r\nLet me demonstrate by asking the LLM to give me the apple pie recipe we used at the start of the chapter but put it in a code block so I can easily extract it later.\r\n\r\nHere\u0027s the prompt:\r\n\r\n\u0060\u0060\u0060text\r\nYou\u0027re a world-class chef, and you\u0027re preparing a meal. \r\nProvide a recipe for apple pie please.\r\nPut the recipe in a fenced markdown block.\r\n\u0060\u0060\u0060\r\n\r\nGPT-4o will respond with something along these lines:\r\n\r\n~~~text\r\nCertainly! Here\u0027s a classic apple pie recipe in a fenced markdown block:\r\n\r\n\u0060\u0060\u0060markdown\r\n# Classic Apple Pie Recipe\r\n\r\n## Ingredients\r\n### For the Pie Crust:\r\n- 2 \u00BD cups (310g) all-purpose flour\r\n- 1 tsp salt\r\n- 1 tsp granulated sugar\r\n- 1 cup (230g) unsalted butter, cold and cubed\r\n- 6\u20138 tbsp ice water\r\n\r\n### For the Apple Filling:\r\n- 6 cups (about five large) apples, peeled, cored, and thinly sliced\r\n- \u00BE cup (150g) granulated sugar\r\n- \u00BC cup (55g) brown sugar\r\n- 2 tbsp all-purpose flour\r\n- 1 tbsp lemon juice\r\n- 1 tsp ground cinnamon\r\n- \u00BC tsp ground nutmeg\r\n- \u00BC tsp ground allspice\r\n- Pinch of salt\r\n\r\n### For Assembly:\r\n- 1 egg, beaten (for egg wash)\r\n- 1 tbsp granulated sugar (for sprinkling, optional)\r\n\r\n... remainder of the response\r\n\u0060\u0060\u0060\r\n~~~\r\n\r\nNote that the response contains the recipe between Markdown fenced code block delimiters. We can now extract the recipe from the response using a regular expression.\r\n\r\nWhile this looks like an excellent way of working with LLMs in use cases outside of chat, there are better ways to handle structured outputs. You can instruct the LLM to generate JSON results or even call a function to output the generated code. Both of these options provide much more control over the output format. We\u0027ll cover structured output in greater detail in chapter 8.\r\n\r\nNext, let\u0027s look at how you can help the model with samples.\r\n\r\n{#few-shot-learning}\r\n### Add samples to the prompt",
        "Embedding": []
      },
      {
        "Id": 30,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "Any LLM can reproduce patterns learned from the large body of text on which it was trained. But that body of text hopefully doesn\u0027t contain internal documents of the client or company you work for. So, if you need to answer questions about internal information, you must tell the LLM about the information.\r\n\r\nWe\u0027ll cover Retrieval Augmented Generation in greater depth in chapter 7. But for now, it\u0027s essential to understand that the LLM can\u0027t remember facts. It\u0027s a talking parrot that reproduces patterns it has seen before. This looks like a limitation, but you can turn this into a powerful trait.\r\n\r\nFor example, I want to answer a question about the company\u0027s policy on remote work. In that case, I can first look up documents related to the question by performing a similarity search using a search engine. The documents I can put into the prompt as additional context and then instruct the LLM to answer the question. A prompt for answering questions typically will look like this:\r\n\r\n\u0060\u0060\u0060text\r\nYou\u0027re a digital assistant for our company\u0027s HR department. \r\nPlease answer the employee\u0027s question based on the content \r\nprovided as context.\r\n\r\n## Context\r\n\r\n\u003CThe found fragments about the policy on remote work\u003E\r\n\r\n## Question\r\n\r\n\u003CThe employee question\u003E\r\n\u0060\u0060\u0060\r\n\r\nYou can consider the context heading in the prompt as samples that the model can use to generate a helpful response. Because of the talking parrot effect, the LLM will likely reproduce the content you provided. The model is \u0022learning\u0022 from information in the context. This is often called [in-context learning][INCONTEXT_LEARNING].\r\n\r\nIf the information in the context is incorrect, vague, or non-existent, then the LLM can\u0027t match the pattern, and you\u0027ll get an answer that doesn\u0027t make much sense or is plain misleading. So it\u0027s essential to ensure you only inject safe and sensible information.\r\n\r\nAdding samples to a prompt helps establish a good pattern for the LLM. You\u0027ll see many people add examples of their desired results to a prompt. Adding examples to your prompt is called one-shot learning or few-shot learning if you add more than one example of the expected output. Let\u0027s look at an example to understand what that looks like:\r\n\r\n~~~text\r\nYou\u0027re a world-class chef, and you\u0027re preparing a meal. Provide a recipe for apple pie please. Output the result in JSON; use the following example as a guide.\r\n\r\n## Example\r\n\u0060\u0060\u0060\r\n{\r\n   \u0022title\u0022: \u0022The recipe title\u0022,\r\n   \u0022ingredients\u0022: [\r\n       { \u0022unit\u0022: \u0022tbsp\u0022, \u0022amount\u0022: 1, \u0022name\u0022: \u0022the name of the ingredient\u0022 },\r\n       { \u0022unit\u0022: \u0022grams\u0022, \u0022amount\u0022: 12, \u0022name\u0022: \u0022another ingredient\u0022 },\r\n   ],\r\n   \u0022instructions\u0022: \u0022The instructions to prepare the recipe\u0022\r\n}\r\n\u0060\u0060\u0060\r\n\r\n~~~\r\n\r\nWe\u0027ll get a completely different response from the previous sample when we run the prompt. The LLM will now generate a JSON object for your recipe.\r\n\r\n~~~text\r\nCertainly! Here\u0027s a recipe for a classic apple pie in JSON format:\r\n\u0060\u0060\u0060json\r\n{\r\n \\\u0022title\\\u0022: \\\u0022Classic Apple Pie\\\u0022,\r\n \\\u0022ingredients\\\u0022: [\r\n { \\\u0022unit\\\u0022: \\\u0022cups\\\u0022, \\\u0022amount\\\u0022: 2.5, \\\u0022name\\\u0022: \\\u0022all-purpose flour\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022granulated sugar\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022salt\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022cups\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022unsalted butter, cold and diced\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 6, \\\u0022name\\\u0022: \\\u0022ice water\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022lbs\\\u0022, \\\u0022amount\\\u0022: 2, \\\u0022name\\\u0022: \\\u0022apples\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022cups\\\u0022, \\\u0022amount\\\u0022: 0.75, \\\u0022name\\\u0022: \\\u0022granulated sugar\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 2, \\\u0022name\\\u0022: \\\u0022all-purpose flour\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022ground cinnamon\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022pinch\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022ground nutmeg\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022lemon juice\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022vanilla extract\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 2, \\\u0022name\\\u0022: \\\u0022unsalted butter (for filling)\\\u0022 },",
        "Embedding": []
      },
      {
        "Id": 42,
        "OriginalFileName": "Content\\05-testing-and-monitoring-prompts.md",
        "Content": "public RecipeGenerationTests()\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 var configuration = new ConfigurationBuilder()\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 .AddUserSecrets\u003CRecipeGenerationTests\u003E()\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 .Build();\r\n\r\n        _kernel = Kernel.CreateBuilder()\r\n            .AddAzureOpenAIChatCompletion(\r\n                deploymentName: configuration[\u0022LanguageModel:DeploymentName\u0022]!,\r\n                endpoint: configuration[\u0022LanguageModel:Endpoint\u0022]!,\r\n                apiKey: configuration[\u0022LanguageModel:ApiKey\u0022]!\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 ).Build();\r\n\r\n        _prompt = _kernel.CreateFunctionFromPromptYaml(\r\n            EmbeddedResource.Read(\u0022prompt.yaml\u0022), \r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 new HandlebarsPromptTemplateFactory()\r\n\u00A0 \u00A0 \u00A0 \u00A0 );\r\n\u00A0 \u00A0 }\r\n\r\n\u00A0 \u00A0 [Theory]\r\n\u00A0 \u00A0 public async Task TestGenerateRecipe_ContainsInstructions(\r\n\u00A0 \u00A0 \u00A0 \u00A0 string dish, string[] ingredients)\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 // The content of your test.\r\n\u00A0 \u00A0 }\r\n}\r\n\u0060\u0060\u0060\r\n\r\nThis code performs the following steps:\r\n\r\n- First, we define a test class with two variables, \u0060_kernel\u0060 to store the kernel\r\ninstance used in the test, and \u0060_prompt\u0060 to store the prompt function that we\u0027re testing.\r\n- Next, we create a new constructor for the test class that initializes the kernel and the prompt function. We\u0027re using \u0060Microsoft.Extensions.Configuration\u0060 to obtain settings from the user-secrets store. This allows us to keep the API key and other sensitive information from the codebase.\r\n- Finally, we define a test method that inputs a dish name and a list of ingredients. We marked the test method as \u0060[Theory]\u0060 to enable parameterized testing. The test method will be called for each set of input data.\r\n\r\nWith the skeleton for the test in place, we can write the test logic to validate the prompt. The following code shows how to write the test logic:\r\n\r\n\u0060\u0060\u0060csharp\r\n[Theory]\r\n[InlineData(\u0022pizza\u0022, new string[] { \u0022spinach\u0022, \u0022mozzarella\u0022 })]\r\npublic async Task TestGenerateRecipe_ContainsInstructions(\r\n\u00A0 \u00A0 string dish, string[] ingredients)\r\n{\r\n\u00A0 \u00A0 var result = await _prompt.InvokeAsync(_kernel, new KernelArguments\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 [\u0022dish\u0022] = dish,\r\n\u00A0 \u00A0 \u00A0 \u00A0 [\u0022ingredients\u0022] = ingredients\r\n\u00A0 \u00A0 });\r\n\r\n    Assert.Contains(\u0022ingredients\u0022, result.ToString().ToLower());\r\n    Assert.Contains(\u0022instructions\u0022, result.ToString().ToLower());\r\n}\r\n\u0060\u0060\u0060\r\n\r\nThe test performs the following steps:\r\n\r\n1. First, we marked the test method with \u0060[InlineData(...)]\u0060 to specify a sample for which we want to run the test. You can use inline data or load data from a test file in your code base. You can learn more about the various methods to load test samples for your data-driven test in [this blog post][XUNIT_DATA_DRIVEN_TESTS].\r\n2. Next, we call the prompt function with the dish name and the list of ingredients as\r\ninput. The result is stored in the \u0060result\u0060 variable.\r\n3. Finally, we use the \u0060Assert.Contains\u0060 method to check if the result contains the\r\nwords *ingredients* and *instructions*. If the result includes these words, the test passes.\r\n\r\nYou can extend this test with more samples as you see fit. The test will run for each provided sample, and separate test results will be reported. You can find the complete source code for the test on [Github][PROMPT_TEST_SAMPLE].\r\n\r\nIt may come as no surprise that running more samples will slow the test, which isn\u0027t fast, to begin with. I highly recommend you mark the test with a separate category and only run the LLM-based tests when you\u0027re about to finish up a new user story in your application or when you change the prompt, the LLM version, or the model configuration.\r\n\r\nYou can mark the test with a category by adding the following attribute to the test method:\r\n\r\n\u0060\u0060\u0060csharp\r\n[Trait(\u0022Category\u0022, \u0022LLM\u0022)]\r\n\u0060\u0060\u0060\r\n\r\nWith this attribute in place, you can filter the tests in the test runner by category. For example, if you use \u0060dotnet test\u0060 to run your tests, you can filter the tests by using the following command:\r\n\r\n\u0060\u0060\u0060bash\r\ndotnet test --filter Category!=LLM\r\n\u0060\u0060\u0060",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "Why is the Azure.Identity package recommended for use with Azure services?",
    "Answer": "The \u0060Azure.Identity\u0060 package is recommended for use with Azure services because it provides the \u0060DefaultAzureCredential\u0060 class, which enables secure and convenient authentication without the need to store secrets or credentials directly within your application. By using managed identities, the \u0060DefaultAzureCredential\u0060 class allows your application to authenticate with Azure services seamlessly and securely. \n\nWhen working locally, you can also authenticate with the Azure environment using the Azure CLI (\u0060az login\u0060), and the application will automatically use those credentials. This approach enhances security and simplifies credential management compared to hardcoding API keys or other sensitive information directly in your application.",
    "Context": [
      {
        "Id": 23,
        "OriginalFileName": "Content\\03-getting-started-with-semantic-kernel.md",
        "Content": "Semantic Kernel. They\u0027re adding more and more with each release. At the time of writing,\r\nthere\u0027s support for the following:\r\n\r\n- OpenAI\r\n- AzureOpenAI\r\n- Amazon Bedrock\r\n- Anthropic\r\n- OLLama\r\n- Google\r\n- Hugging Face\r\n- ONNX\r\n\r\nAnd even if your favorite provider is not listed, it might still work with Semantic\r\nKernel as long as it has an OpenAI-like API interface. [The Semantic Kernel\r\nManual][OTHER_LLM_PROVIDER] explains how to use other LLM providers that support the\r\nOpenAI API interface.\r\n\r\nLet\u0027s start a new project with Semantic Kernel.\r\n\r\n{#setting-up-semantic-kernel}\r\n## Setting up a project with Semantic Kernel\r\n\r\nMost people will build web API around an LLM use case. However, Semantic Kernel can be\r\nused in more places than just web applications. To help you get started, I want to\r\nprovide you with two scenarios:\r\n\r\n- Using Semantic Kernel in a standalone console application\r\n- Using Semantic Kernel in an ASP.NET Core application\r\n\r\nLet\u0027s start with a basic console application to get you started.\r\n\r\n### Using Semantic Kernel in a standalone console application\r\n\r\nTo set up a console application with Semantic Kernel, you must create a new one in\r\nVisual Studio Code, Rider, or Visual Studio. I prefer a terminal with Visual Studio\r\nCode, but you can use the IDE you like. Execute the following command in a terminal to\r\nstart a new console application in C#:\r\n\r\n\u0060\u0060\u0060bash\r\ndotnet new console -n Chapter3.ConsoleApp\r\n\u0060\u0060\u0060\r\n\r\nThis command will create a new console application in a folder called\r\n\u0060Chapter3.ConsoleApp\u0060. You can open the folder in Visual Studio Code by executing the\r\nfollowing command:\r\n\r\n\u0060\u0060\u0060bash\r\ncode Chapter3.ConsoleApp\r\n\u0060\u0060\u0060\r\n\r\nIn Visual Studio Code, you\u0027ll see the project file for the console application,\r\nChapter3.Console.csproj, and a \u0060Program.cs\u0060 file containing the main program code.\r\n\r\nLet\u0027s add the Semantic Kernel package to the project by executing the following commands\r\nin the project directory inside a terminal window:\r\n\r\n\u0060\u0060\u0060bash\r\ndotnet add package Microsoft.SemanticKernel\r\ndotnet add package Azure.Identity\r\n\u0060\u0060\u0060\r\n\r\nThis command will install the Semantic Kernel package that has references to the\r\nfollowing packages:\r\n\r\n- \u0060Microsoft.SemanticKernel.Core\u0060\\\r\n  Contains the core components we discussed in [#s](#core-concepts-and-architecture)\r\n- \u0060Microsoft.SemanticKernel.Abstractions\u0060\\\r\n  Contains the base layer for various types of connectors.\r\n- \u0060Microsoft.SemanticKernel.Connectors.AzureOpenAI\u0060\\\r\n  Contains the connector to Azure OpenAI.\r\n- \u0060Microsoft.SemanticKernel.Connectors.OpenAI\u0060\\\r\n  Contains the connector to OpenAI\r\n\r\nThe Semantic Kernel package provides convenience if you don\u0027t want to worry about\r\nfinding the individual packages. If you care about the size of your application, you can\r\ninstall the individual packages instead and replace the OpenAI connector with the\r\nconnector of your choice.\r\n\r\nI\u0027ve included the \u0060Azure.Identity\u0060 package because it\u0027s a good practice to use managed\r\nidentities when working with Azure services. This package provides the\r\n\u0060DefaultAzureCredential\u0060 class, which you can use to authenticate with Azure services\r\nwithout the need to store secrets with your application. As an alternative, you can also\r\nspecify an API Key for the client.\r\n\r\nIf you\u0027re working from your local machine, you can use [the Azure CLI][AZURE_CLI] to\r\nauthenticate with the Azure environment by executing the command \u0060az login\u0060 from a\r\nterminal window. When you run the sample application, it will use the credentials from\r\nthe Azure CLI to gain access to your Azure OpenAI resource.\r\n\r\nWith the packages installed, you can configure the kernel object we discussed in\r\n[#s](#core-concepts-and-architecture) in the \u0060Program.cs\u0060 file.\r\n\r\n\u0060\u0060\u0060csharp\r\nusing Microsoft.SemanticKernel;\r\nusing Azure.AI.OpenAI;\r\nusing Azure.Identity;\r\nusing OpenAI;\r\n\r\nIKernelBuilder kernelBuilder = Kernel.CreateBuilder();\r\n\r\nvar apiClient = new AzureOpenAIClient(\r\n\u00A0 \u00A0 new Uri(\u0022https://\u003Cyour-endpoint\u003E\u0022),",
        "Embedding": []
      },
      {
        "Id": 24,
        "OriginalFileName": "Content\\03-getting-started-with-semantic-kernel.md",
        "Content": "new DefaultAzureCredential());\r\n\r\nkernelBuilder.AddAzureOpenAIChatCompletion(\r\n deploymentName: \u0022gpt-4o\u0022,\r\n azureOpenAIClient: apiClient);\r\n\r\n// ALTERNATIVELY: Use the OpenAI API.\r\n\r\n// var apiClient = new OpenAIClient(\r\n// \u00A0 \u00A0 Environment.GetEnvironmentVariable(\u0022OPENAI_API_KEY\u0022));\r\n\r\n// kernelBuilder.AddOpenAIChatCompletion(\u0022gpt-4o\u0022, apiClient);\r\n\r\nKernel kernel = kernelBuilder.Build();\r\n\u0060\u0060\u0060\r\n\r\nLet\u0027s go over the code step by step:\r\n\r\n1. First, we import the necessary namespaces for the kernel and the connectors.\r\n2. Then, we create a new \u0060IKernelBuilder\u0060 that we can use to configure the kernel\u0027s functions, filters, and LLM provider.\r\n3. we then call the \u0060Build\u0060 method on the kernel builder to create the kernel object.\r\n\r\nBefore attempting to run the application, make sure to configure the necessary\r\nenvironment variables with the application\u0027s configuration.\r\n\r\nAt this point, you can use the kernel object to execute prompts. We\u0027ll cover how to do\r\nthat in [#s](#executing-your-first-prompt). But before we do that, let\u0027s set up Semantic\r\nKernel in ASP.NET Core, too.\r\n\r\n### Using Semantic Kernel in an ASP.NET Core application\r\n\r\nYou must create a new web API project to set up Semantic Kernel in an ASP.NET Core\r\nproject. You can do this by executing the following command in a terminal:\r\n\r\n\u0060\u0060\u0060bash\r\ndotnet new web -n Chapter3.WebApi\r\n\u0060\u0060\u0060\r\n\r\nThis command will create a new web API project in a folder called \u0060Chapter3.WebApi\u0060. You\r\ncan open the folder in Visual Studio Code by executing the following command:\r\n\r\n\u0060\u0060\u0060bash\r\ncode Chapter3.WebApi\r\n\u0060\u0060\u0060\r\n\r\nIn Visual Studio Code, you\u0027ll see the project file for the web API called\r\n\u0060Chapter3.WebApi.csproj\u0060 and a \u0060Program.cs\u0060 file containing the configuration for the\r\nweb API. You\u0027ll also find a \u0060settings.json\u0060 file and a \u0060settings.Development.json\u0060 file\r\nin the project directory for web applications.\r\n\r\nNext, we need to add the same package as we used in the console application to the web\r\nAPI project. Execute the following commands in the project directory inside a terminal\r\nwindow:\r\n\r\n\u0060\u0060\u0060bash\r\ndotnet add package Microsoft.SemanticKernel\r\ndotnet add package Azure.Identity\r\n\u0060\u0060\u0060\r\n\r\nThese commands will install the necessary packages, including the Azure OpenAI and\r\nOpenAI connector.\r\n\r\nASP.NET Core uses dependency injection to manage application components. Setting up a\r\nSemantic Kernel in an ASP.NET Core application differs from using it in a console\r\napplication. You need to use the dependency injection container to register the kernel\r\nobject in the application. The following code demonstrates this.\r\n\r\n\u0060\u0060\u0060csharp\r\nusing Azure.AI.OpenAI;\r\nusing Azure.Identity;\r\nusing Microsoft.SemanticKernel;\r\nusing Microsoft.SemanticKernel.Connectors.AzureOpenAI;\r\nusing Microsoft.SemanticKernel.Connectors.OpenAI;\r\nusing OpenAI;\r\n\r\nvar builder = WebApplication.CreateBuilder(args);\r\n\r\nbuilder.Services.AddTransient(sp =\u003E new AzureOpenAIClient(\r\n\u00A0 \u00A0 new Uri(builder.Configuration[\u0022AzureOpenAI:Endpoint\u0022]!),\r\n\u00A0 \u00A0 new DefaultAzureCredential()));\r\n\r\nvar kernelBuilder = builder.Services.AddKernel()\r\n\u00A0 \u00A0 .AddAzureOpenAIChatCompletion(deploymentName: \u0022gpt-4o\u0022);\r\n\r\n// ALTERNATIVELY: Use the OpenAI API.\r\n\r\n// builder.Services.AddTransient(sp =\u003E new OpenAIClient(\r\n// \u00A0 \u00A0 builder.Configuration[\u0022OpenAI:ApiKey\u0022]));\r\n\r\n// var kernelBuilder = builder.Services.AddKernel()\r\n// \u00A0 \u00A0 .AddOpenAIChatCompletion(\u0022gpt-4o\u0022);\r\n\r\nvar app = builder.Build();\r\n\r\napp.MapGet(\u0022/\u0022, () =\u003E \u0022Hello World!\u0022);\r\n\r\napp.Run();\r\n\u0060\u0060\u0060\r\n\r\nLet\u0027s go over the code step by step again to understand the differences between the\r\nASP.NET Core code and a regular console application:\r\n\r\n1. The \u0060using\u0060 statements are the same as with a console application. We\u0027ll need to use\r\n   the Semantic Kernel namespace and the connector namespace.\r\n2. Next, we register a new \u0060AzureOpenAIClient\u0060 in the service collection. We use the\r\n   \u0060AzureOpenAI:Endpoint\u0060 configuration value to set the endpoint of the Azure OpenAI",
        "Embedding": []
      },
      {
        "Id": 48,
        "OriginalFileName": "Content\\05-testing-and-monitoring-prompts.md",
        "Content": "We haven\u0027t covered logging yet; let\u0027s look at that before we dive into exporting traces to a monitoring tool like Application Insights.\r\n\r\n### Logging in LLM-based applications\r\n\r\nYou can configure logging as a separate piece of monitoring in Semantic Kernel. When you\u0027re building a console application, you\u0027ll need to add the following code to the startup:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar loggerFactory = LoggerFactory.Create(builder =\u003E\r\n{\r\n    builder.AddOpenTelemetry(options =\u003E\r\n\u00A0 \u00A0 {\r\n        options.SetResourceBuilder(resourceBuilder);\r\n        options.AddConsoleExporter();\r\n        options.IncludeFormattedMessage = true;\r\n        options.IncludeScopes = true;\r\n\u00A0 \u00A0 });\r\n\r\n    builder.SetMinimumLevel(LogLevel.Information);\r\n});\r\n\u0060\u0060\u0060\r\n\r\nThe code performs the following steps:\r\n\r\n1. First, \u00A0we create a new logger factory using the \u0060LoggerFactory\u0060 class.\r\n2. Next, we configure Open Telemetry as a logging provider, using the same OpenTelemetry settings as we did for traces and metrics. We then use the console exporter to write the logs to the terminal.\r\n3. Finally, we set the minimum log level to \u0060Information\u0060.\r\n\r\nYou can write log messages straight to the terminal without using OpenTelemetry, but this will break monitoring when you want to use a tool like Application Insights. Using OpenTelemetry with the logging factory connects the log messages as events to the traces generated by the application. This way, you can view traces and log messages as a single unit in the monitoring tool.\r\n\r\nThe OpenTelemetry console exporter provides a nice method to check that everything is configured as intended, but I don\u0027t recommend using it in production. It will slow down your application, and it\u0027s not durable. Let\u0027s look at how to configure Application Insights as an exporter and how to configure a dashboard for your LLM-based application using this tool.\r\n\r\n### Writing monitoring data to application insights\r\n\r\nConfiguring Application Insights as an exporter for traces, metrics, and logs requires an additional package. Add the \u0060Azure.Monitor.OpenTelemetry.Exporter\u0060 package to your project. Use the following terminal command to add the package to your project:\r\n\r\n\u0060\u0060\u0060bash\r\ndotnet add package Azure.Monitor.OpenTelemetry.Exporter\r\n\u0060\u0060\u0060\r\n\r\nThe following code demonstrates how to configure the exporter in a console application:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar resourceBuilder = ResourceBuilder.CreateDefault()\r\n\u00A0 \u00A0 .AddService(\u0022Chapter5.Telemetry\u0022);\r\n\r\nusing var traceProvider = Sdk.CreateTracerProviderBuilder()\r\n\u00A0 \u00A0 .SetResourceBuilder(resourceBuilder)\r\n\u00A0 \u00A0 .AddSource(\u0022Microsoft.SemanticKernel*\u0022)\r\n\u00A0 \u00A0 .AddAzureMonitorTraceExporter(options =\u003E\r\n\u00A0 \u00A0 {\r\n        options.ConnectionString =\r\n        configuration[\u0022Monitoring:ConnectionString\u0022];\r\n\u00A0 \u00A0 })\r\n\u00A0 \u00A0 .Build();\r\n\r\nusing var meterProvider = Sdk.CreateMeterProviderBuilder()\r\n\u00A0 \u00A0 .SetResourceBuilder(resourceBuilder)\r\n\u00A0 \u00A0 .AddMeter(\u0022Microsoft.SemanticKernel*\u0022)\r\n\u00A0 \u00A0 .AddAzureMonitorMetricExporter(options =\u003E\r\n\u00A0 \u00A0 {\r\n        options.ConnectionString = \r\n        configuration[\u0022Monitoring:ConnectionString\u0022];\r\n\u00A0 \u00A0 })\r\n\u00A0 \u00A0 .Build();\r\n\u0060\u0060\u0060\r\n\r\nThis code performs the following steps:\r\n\r\n1. First, we configure the resource builder for the service metadata to include in the metrics and traces.\r\n2. Then, we configure the trace provider as before, but we\u0027re adding the Azure Monitor exporter this time. We use the connection string from the configuration to connect to Application Insights.\r\n3. Next, we configure the meter provider as before, but this time, we add the Azure Monitor exporter using the same connection string we used for the tracer provider.\r\n\r\nYou can find the complete source code for this sample on [Github][MONITORING_SAMPLE].\r\n\r\nWith this code in place, you can start monitoring your LLM-based application in Application Insights. You need to set up an Application Insights resource in Azure to obtain the connection string for the exporters. A great manual for this is available in [the Application Insights documentation][AI_DOCS].",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "Can you provide a brief history of language models?",
    "Answer": "Certainly! Here\u0027s a brief history of language models based on the provided context:\n\nLanguage models have been evolving for a long time. Early attempts at understanding human language relied on pattern matching using regular expressions. These were rigid and prone to breaking with even slight changes in input. Later, machine learning-based chatbots emerged, which matched patterns to understand intent. However, they were limited to fixed responses and struggled with understanding nuanced input.\n\nThe breakthrough came in 2017 with the introduction of the transformer neural network architecture, as detailed in the paper [\u0022Attention is all you need\u0022](https://arxiv.org/abs/1706.03762). Unlike earlier models that processed text word-by-word and struggled with context, transformers could process entire text sequences at once, capturing relationships between words regardless of their position. This innovation enabled the development of modern Large Language Models (LLMs) like ChatGPT, which exhibit human-like understanding and text generation capabilities.",
    "Context": [
      {
        "Id": 3,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "{#understanding-llms} \r\n{sample: true} \r\n# Understanding Large Language Models\r\n\r\nI remember first realizing how useful large language models could be. Like many\r\ndevelopers, I tried ChatGPT almost as a joke \u2013 I didn\u0027t expect it to work as well as it\r\ndid. I had this piece of code that needed unit tests, and on a whim, I copy-pasted it\r\ninto ChatGPT and asked it to write the tests for me. What happened next surprised me.\r\nWithin seconds, I got back working code that needed only minor tweaks. The overall\r\nstructure was spot-on, saving me 30 minutes of work. While 30 minutes doesn\u0027t sound like\r\na lot of time saved, it does add up, and this is the worst-case scenario at a time when\r\nI didn\u0027t quite understand how an LLM worked.\r\n\r\nThat interaction looked like magic, but it really wasn\u0027t. It resulted from careful\r\nengineering, good quality prompt design, and understanding of what LLMs can (and can\u0027t)\r\ndo well. This chapter is about demystifying these powerful tools so you can put them to\r\nwork in your applications.\r\n\r\nYou\u0027ve probably heard the buzz around LLMs and played with ChatGPT, Claude, or other AI\r\nassistants. Perhaps you\u0027re skeptical about the hype or excited about the possibilities.\r\nWhatever your starting point, this chapter will give you the solid foundation to move\r\nbeyond simple interactions and build real applications with LLMs.\r\n\r\nWe\u0027ll start with the basics \u2013 what LLMs are and why they matter \u2013 but we won\u0027t get\r\nbogged down in the theoretical. Instead, we\u0027ll focus on the practical: how these models\r\nwork in the real world, what they\u0027re good at, and where they fall short. I\u0027ll share\r\nstories from my journey with LLMs, including the failures and breakthroughs that taught\r\nme the most.\r\n\r\nBy the end of this chapter, you\u0027ll understand:\r\n\r\n- What LLMs are and why they\u0027re changing the way we solve problems\r\n- The key concepts and terminology you need to work effectively with LLMs\r\n- The current LLM landscape from a commercial and open-source perspective\r\n- Practical considerations for building production applications\r\n- Real-world use cases and applications that go beyond the obvious\r\n\r\nLet\u0027s explore the world of large language models\u2014not as magical black boxes, but as\r\npractical tools, we can understand, control, and use to solve real problems.\r\n\r\n## What are LLMs, and why do they matter\r\n\r\nLet\u0027s start with the basics. A Large Language Model (LLM) is a neural network trained on\r\nmassive amounts of data to understand and generate human-like text.\r\n\r\n### A brief history\r\n\r\nLanguage models aren\u0027t new at all. We\u0027ve been trying to understand human language for a\r\nvery long time. Early attempts involved pattern matching using regular expressions,\r\nwhich are probably well-known because you only write them once and then never change\r\nthem. They\u0027re hard to understand and relatively inflexible because they break at the\r\nslightest change in the input.\r\n\r\nLater, we introduced clunky chatbots that could match patterns to understand intent\r\nusing machine learning. However, these bots still had fixed responses and were unable to\r\nunderstand intent when the input was even slightly off.\r\n\r\nThe real game-changers started appearing around 2017. Before that, we had specialized\r\nlanguage models that had limited performance. Don\u0027t get me wrong, tools like\r\n[SpaCy](https://spacy.io/) were terrific at the time for what we could do.\r\n\r\n### The breakthrough moment\r\n\r\nEverything changed with the introduction of the transformer neural network architecture.\r\nThe paper [\u0022Attention is all you need\u0022](https://arxiv.org/abs/1706.03762) demonstrated\r\nan entirely new way of processing language. I won\u0027t bore you with the technical details,\r\nbut here is why it matters: Previous models would process text word-by-word and couldn\u0027t\r\nlook at the context as a whole. The transformer architecture instead looks at the entire\r\ncontext at once, understanding the relationship between words regardless of their\r\nposition.",
        "Embedding": []
      },
      {
        "Id": 12,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "were struggling with knowledge sharing. We had a common problem: experts too busy to\r\nwrite articles about their innovations, leading to repeated problem-solving across\r\nprojects.\r\n\r\nInstead of asking people to write articles, we flipped the script. We built a chat-based\r\nsystem where the LLM interviewed people about their work, asking progressively deeper\r\nquestions to fully understand the topic. The system then transformed these conversations\r\ninto structured articles.\r\n\r\nOne interesting discovery was how well users responded to AI interviews. The interaction\r\nfelt natural without falling into the [uncanny\r\nvalley](https://en.wikipedia.org/wiki/Uncanny_valley). However, we learned that letting\r\nthe LLM fully control the interview flow was tricky. We eventually replaced our complex\r\nprompt-based decision-making with a more straightforward function that tracked question\r\ncount to manage interview length.\r\n\r\n### Modernizing legacy code bases\r\n\r\nOne of my favorite projects involved upgrading legacy web forms from XML to TypeScript.\r\nThe organization had so many forms that manual conversion would have taken years.\r\n\r\nWe approached this by building a batch pipeline that combined traditional XML parsing\r\nwith LLM-powered code generation. Rather than asking the LLM to handle everything, we\r\nbroke the problem into manageable chunks, using multiple refinement prompts to improve\r\nthe generated code quality.\r\n\r\nThis project taught us a crucial lesson: LLMs work best when combined with traditional\r\nprogramming logic. While they\u0027re great at understanding and translating code patterns,\r\nthey struggle with handling large amounts of complex code simultaneously. Breaking the\r\nproblem into smaller pieces and using traditional parsing where appropriate gave us much\r\nbetter results.\r\n\r\n### We\u0027re only just starting out\r\n\r\nThese cases represent just the beginning of what\u0027s possible with LLMs. While many people\r\nstart their LLM journey with personal automation through tools like ChatGPT, these\r\nexamples show how to scale up to automate more complex team-based workflows.\r\n\r\nIt\u0027s important to remember that we\u0027re in the early days of this technology. The patterns\r\nI share in this book work well today, but I expect them to evolve significantly over the\r\nnext few years. What excites me most is that we\u0027re just starting to understand what\u0027s\r\npossible.\r\n\r\nAs we progress in this book, we\u0027ll explore various design patterns in detail, showing\r\nyou how to implement them in your projects. But first, let\u0027s make sure you have a solid\r\nfoundation by discussing operating LLMs in production in the next chapter.\r\n\r\n## Summary\r\n\r\nIn this chapter, we\u0027ve learned a brief history of Large Language Models and how they\r\nwork from a conceptual level. We\u0027ve explored the key concepts and terminology to help\r\nyou understand and work with LLMs. Finally, I\u0027ve shared my journey to help you\r\nunderstand the practical considerations and real-world applications of LLMs.\r\n\r\nIn the next chapter, we\u0027ll talk about more practical considerations you need to consider\r\nwhen building LLM-based applications when we discuss the essentials of LLMOps.\r\n\r\n## Further reading\r\n\r\nHere are some resources if you want to learn more about the inner workings of LLMs:\r\n\r\n- [How LLMs think](https://towardsdatascience.com/how-llms-think-d8754a79017d) - An\r\ninteresting article exploring some of the math behind LLMs in an attempt to understand how and why they work.\r\n- [Attention is all you need](https://arxiv.org/abs/1706.03762) - The original paper\r\nthat sparked the transformer revolution.\r\n\r\n[OPENAI_MODELS]: https://platform.openai.com/docs/models#models-overview\r\n[HUGGINGFACE_LLAMA]: https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct\r\n[HUGGINGFACE_MISTRAL]: https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407\r\n[HUGGINGFACE_GEMMA2]: https://huggingface.co/blog/gemma2 [PHI4_ANNOUNCEMENT]: https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090 \r\n[PHI4_BENCHMARKS]: https://www.microsoft.com/en-us/research/uploads/prod/2024/12/P4TechReport.pdf \r\n[PARAMETER_EXPLANATION]: https://medium.com/@albert_88839/large-language-model-settings-temperature-top-p-and-max-tokens-1a0b54dcb25e \r\n[UNDERSTANDING_TRANSFORMERS]: https://poloclub.github.io/transformer-explainer/",
        "Embedding": []
      },
      {
        "Id": 8,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "experience. I generally start with the commercial models when exploring a use case. When\r\nI understand a use case well enough, I will try an open-source model to see whether we\r\ncan lower the price point of our solution.\r\n\r\nHaving said that, I\u0027ve had great experiences with these models:\r\n\r\n- **LLaMA 3.3:** A general purpose model offered by Meta through [Hugging\r\n  Face][HUGGINGFACE_LLAMA]. This model comes in two types: a pre-trained variant that\r\n  you can fine-tune yourself to perform specific tasks and an instruction-tuned model\r\n  that Meta recommends using for chat applications. But don\u0027t let yourself be limited by\r\n  what Meta says because the instruction-tuned version is also useful for non-chat\r\n  purposes, as long as you have an instruction-based use case.\r\n\r\n- **Mistral:** The Mistral model by the identically named company is a fast open-source\r\n  LLM mostly used for chat purposes. This model is generally less capable than the LLaMA\r\n  model variants, but its speed makes up for that. This model is also hosted on\r\n  [HuggingFace][HUGGINGFACE_MISTRAL] and has many fine-tuned variants.\r\n\r\n- **Gemma2:** Google published this model in February 2024 and trained it using a\r\n  [teacher/student technique][HUGGINGFACE_GEMMA2]. The training technique looks very\r\n  interesting, but Gemma2 isn\u0027t quite as good for many of the tasks I worked on as the\r\n  other models in the open-source space.\r\n\r\n- **Phi 4:** Is a new model that was [introduced by Microsoft in December\r\n  2024][PHI4_ANNOUNCEMENT]. It is similar to the Mistral and Gemma2 models, 14 billion\r\n  parameters, but shows higher performance in [the benchmarks][PHI4_BENCHMARKS]. While\r\n  this doesn\u0027t tell the whole story, it\u0027s worth trying this model for a smaller\r\n  open-source model.\r\n\r\n- **DeepSeek** is another new model that\u0027s showing great promise. The R1 variant of\r\n  DeepSeek is great at reasoning and shows similar performance to the OpenAI o1 models.\r\n  The V3 variant of the model is great too, for more general-purpose tasks. You should\r\n  keep in mind that the DeepSeek models only support English and Chinese though, so if\r\n  you need a response in other languages, your milage will vary.\r\n\r\n### Making practical choices\r\n\r\nIt\u0027s important to remember that the most potent model isn\u0027t always the best option. I\r\nfollow this general workflow when developing an LLM-based application that can be pretty\r\nhelpful if you\u0027re just starting:\r\n\r\n1. First, I choose a general-purpose model based on the cloud provider I\u0027m working with.\r\n   Most of the time, my clients already have a contract with either Microsoft Azure or\r\n   AWS to host their solutions. I use the existing environment to prototype the\r\n   solution.\r\n\r\n2. After the initial prototype, I\u0027ll examine any data privacy requirements the solution\r\n   may have. Depending on these requirements, I will determine the engineering and\r\n   contractual effort needed to make the solution production viable. Usually, I\u0027m the\r\n   person who talks about the technical requirements while one of our legal people looks\r\n   into contracts.\r\n\r\n3. After the initial prototype and requirements gathering, I\u0027ll deploy the solution to\r\n   production for a smaller group of people to gather initial user feedback and monitor\r\n   performance and costs. Based on this information, I decide whether we should optimize\r\n   the solution or replace the model with something else.\r\n\r\n4. Once the solution is optimized and running in production for an organization\u0027s\r\n   general population, I\u0027ll monitor it for sudden changes in the quality of the\r\n   responses or performance. We regularly test and update the models to improve the\r\n   solution\u0027s overall performance.\r\n\r\nThis general workflow has helped me quite well over the past few years to deploy\r\nsolutions in production. In the next section, we\u0027ll dive into key concepts and\r\nterminology for using LLMs effectively. Understanding these fundamentals will help you",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "What coding language is shown in the example?",
    "Answer": "The coding language shown in the example is **C#**.",
    "Context": [
      {
        "Id": 56,
        "OriginalFileName": "Content\\06-enhancing-llms-with-tools.md",
        "Content": "the LLM will call the function \u0060generate_outline\u0060 with the input variable \u0060topic\u0060 set to\r\n\u0060AI.\u0060 The function will then generate an outline for a blogpost about AI. This output is\r\nthen returned to the LLM, which will use it to generate a final response to the user.\r\nThis all happens thanks to the function calling loop as shown in\r\n[#s](#function-calling-loop-reminder).\r\n\r\n{#function-calling-loop-reminder}\r\n![Function calling loop](function-calling-loop.png)\r\n\r\nThis function calling loop is not part of the LLM, but something that Semantic Kernel\r\nprovides. It works like this:\r\n\r\n1. First, We call the LLM with one or more messages and tools that the LLM can use.\r\n2. Next, the LLM will detect that it needs to call a tool to generate a proper response,\r\n   resulting in a tool_call response.\r\n3. Then, Semantic Kernel parses the response and calls the tool with the data provided\r\n   by the LLM.\r\n4. After that, the tool\u0027s output is added to the chat history, and the whole thing is\r\n   returned to the LLM.\r\n5. Finally, if the LLM is satisfied and doesn\u0027t need to call more tools, it will\r\n generate a final response.\r\n\r\nNote that Semantic Kernel can repeat the loop multiple times if the LLM detects multiple\r\ntool calls in sequence. The LLM can call various tools within the scope of processing a\r\nprompt.\r\n\r\n{#calling-functions}\r\n### Creating a kernel function in C#\r\n\r\nIt\u0027s helpful to have the option to run code for things you can\u0027t do with prompts. You\r\ncan build functions using C#. The following code demonstrates a basic function that\r\nreturns the current date/time.\r\n\r\n\u0060\u0060\u0060csharp\r\npublic class TimePlugin\r\n{\r\n\u00A0 \u00A0 [KernelFunction(\u0022get_current_time\u0022)]\r\n\u00A0 \u00A0 [Description(\u0022Get the current date and time.\u0022)]\r\n\u00A0 \u00A0 public string GetCurrentTime()\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 return DateTime.Now.ToString(\u0022yyyy-MM-dd HH:mm:ss\u0022);\r\n\u00A0 \u00A0 }\r\n}\r\n\u0060\u0060\u0060\r\n\r\nThis code does the following:\r\n\r\n1. First, we create a plugin class, \u0060TimePlugin\u0060, that contains a single function\r\n   \u0060GetCurrentTime\u0060.\r\n2. Next, we add the \u0060KernelFunction\u0060 attribute to the function to tell Semantic Kernel\r\n   that this is a function that it can use.\r\n3. Finally, we add the \u0060Description\u0060 attribute to describe what the function does.\r\n\r\nNotice the name of the function \u0060get_current_time\u0060. You would expect me to use\r\n\u0060GetCurrentTime\u0060 as the function\u0027s name because that is what the function is called in\r\nC#. However, I learned that functions are detected much better using snake casing. LLMs\r\nare trained mostly on Python code and are much better at detecting snake cased\r\nfunctions.\r\n\r\nTo add the function to the kernel, you can use the following code:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar kernelBuilder = Kernel.CreateBuilder()\r\n    .AddAzureOpenAIChatCompletion(\r\n        deploymentName: configuration[\u0022LanguageModel:DeploymentName\u0022]!,\r\n        endpoint: configuration[\u0022LanguageModel:Endpoint\u0022]!,\r\n        apiKey: configuration[\u0022LanguageModel:ApiKey\u0022]!\r\n\u00A0 \u00A0 );\r\n\r\nkernelBuilder.Plugins.AddFromType\u003CTimePlugin\u003E();\r\n\r\nvar kernel = kernelBuilder.Build();\r\n\r\nvar chatCompletionService =\r\n    kernel.Services.GetRequiredService\u003CIChatCompletionService\u003E();\r\n\r\nvar history = new ChatHistory();\r\nhistory.AddSystemMessage(\u0022You\u0027re a digital assistant\u0022);\r\nhistory.AddUserMessage(\u0022What time is it?\u0022);\r\n\r\nvar executionSettings = new AzureOpenAIPromptExecutionSettings\r\n{\r\n    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto()\r\n};\r\n\r\nvar response = await chatCompletionService.GetChatMessageContentAsync(\r\n    history, executionSettings, kernel);\r\n\r\nConsole.WriteLine(response.ToString());\r\n\u0060\u0060\u0060\r\n\r\nIn this code we perform the following steps:\r\n\r\n1. First, we create a new kernel builder object to construct a kernel with GPT-4o as the\r\n   model based on configuration.\r\n2. Next, we configure a plugin based on a C# type, and use our class \u0060TimePlugin\u0060.\r\n3. Then, we create a new kernel and ask for the current time by sending a message to the\r\n LLM.\r\n\r\nWe have to tell the Semantic Kernel that it is allowed to call functions as an extra",
        "Embedding": []
      },
      {
        "Id": 40,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "In this chapter, we\u0027ve examined the art and nonsense of prompt engineering. We discussed the important role prompts play in LLM-based applications and how to write effective and safe prompts.\r\n\r\nIn the next chapter, we\u0027ll look at testing and monitoring prompts in your LLM-based application.\r\n\r\n## Running the samples in this chapter\r\n\r\nCheck out the samples for this chapter in the [Github repository][GH_SAMPLE_DIR]. Each sample has a README file containing system requirements and instructions on how to run the sample.\r\n\r\n[CONTEXT_WINDOW_PAPER]: https://arxiv.org/abs/2307.03172\r\n[LLM_ALIGNMENT]: https://medium.com/@madalina.lupu.d/align-llms-with-reinforcement-learning-from-human-feedback-595d61f160d5\r\n[TOP_P_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/notebooks\r\n[SK_TEMPLATE_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-04/csharp/Chapter4.SemanticKernelTemplates\r\n[HB_TEMPLATE_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-04/csharp/Chapter4.HandleBarsTemplates\r\n[KF_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-04/csharp/Chapter4.KernelFunctionPrompts\r\n[HB_MANUAL]: https://handlebarsjs.com/guide/\r\n[STREAMING_SAMPLE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-04/csharp/Chapter4.StreamingChatCompletions\r\n[HISTORY_REDUCERS]: https://learn.microsoft.com/en-us/semantic-kernel/concepts/ai-services/chat-completion/chat-history?pivots=programming-language-csharp#chat-history-reduction\r\n[AZ_PII_DETECTION]: https://learn.microsoft.com/en-us/azure/ai-services/language-service/personally-identifiable-information/how-to-call\r\n[GA_PII_DETECTION]: https://cloud.google.com/sensitive-data-protection/docs/deidentify-sensitive-data\r\n[AWS_PII_DETECTION]: https://docs.aws.amazon.com/comprehend/latest/dg/how-pii.html\r\n[OWASP_LLM]: https://owasp.org/www-project-top-10-for-large-language-model-applications/\r\n[MITRE]: https://atlas.mitre.org/matrices/ATLAS/\r\n[GH_SAMPLE_DIR]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-04/\r\n[INCONTEXT_LEARNING]: https://ai.stanford.edu/blog/understanding-incontext/",
        "Embedding": []
      },
      {
        "Id": 33,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "The frequency penalty is the last hyperparameter we need to discuss. This hyperparameter looks similar to the presence penalty hyperparameter but uses a different approach. Instead of applying a flat penalty to tokens that occurred before in the output, a frequency penalty applies a penalty to the probability of tokens that frequently appear in the output. The more often a token appears in the output, the higher the penalty and the less likely the token is to be selected during the Top-P sampling process.\r\n\r\n#### What to choose for each of the hyperparameters\r\n\r\nI know this is a lot to take in, so let me give you some direction on what to choose for each parameter.\r\n\r\nCoding requires a more repetitive and boring output to be effective, so you want to choose a lower value for Top-P and a lower value for temperature. The presence penalty and frequency penalty should be set to 0 because we don\u0027t want to apply any penalties. Coding is repetitive by nature.\r\n\r\nI recommend setting the Top-P value higher and applying a higher temperature for general-purpose tasks like generating marketing content. The presence penalty and frequency penalty can help make the text more varied. Don\u0027t go overboard, though; a low value is often enough. Setting the presence and frequency penalty to a high value leads to fascinating but often nonsensical output.\r\n\r\nYou must test your prompts with various inputs and settings to establish what works for most cases. Because the probability distribution values ultimately depend on your prompt\u0027s content, you likely need to adjust the hyperparameters a little bit based on that.\r\n\r\nOnce you\u0027ve written a good-quality prompt, you may want to keep it around for longer. For this, it\u0027s nice to have some sort of templating system in place. Let\u0027s take a look at what Semantic Kernel has to offer.\r\n\r\n{#prompt-templates} \r\n## Writing prompt templates for reusability\r\n\r\nWriting your prompts in line with other C# codes is never a good plan. It\u0027s hard to read and maintain, and you can\u0027t reuse it. That\u0027s why Semantic Kernel offers a way to write prompt templates using a variety of templating languages:\r\n\r\n- Semantic Kernel Templates: The internal format developed by Microsoft.\r\n- Handlebars: A popular templating language that is available for many\r\nprogramming languages.\r\n- Liquid: An alternative to handlebars that is also available for several programming languages.\r\n\r\nThe prompt templating feature in Semantic Kernel is quite powerful. Let\u0027s first examine the internal templating engine.\r\n\r\n### Creating a prompt template in Semantic Kernel\r\n\r\nThe Semantic Kernel templating language is a text-based language. You can write a basic prompt template like this:\r\n\r\n\u0060\u0060\u0060text\r\nHelp me cook something nice, give me a recipe for {{ $dish }}\r\n\u0060\u0060\u0060\r\n\r\nIn this template, we ask for a basic recipe for a dish. Dish is a variable identified by \u0060{{ \\$dish }}\u0060. We can fill this variable later when we invoke the prompt.\r\n\r\nYou can invoke the prompt template using the following code:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar promptTemplate = File.ReadAllText(\r\n    Path.Join(Directory.GetCurrentDirectory(), \u0022prompt.txt\u0022)\r\n);\r\n\r\nvar result = await kernel.InvokePromptAsync(promptTemplate,\r\n    arguments: new KernelArguments\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 [\u0022dish\u0022] = \u0022pizza\u0022\r\n\u00A0 \u00A0 },\r\n    templateFormat: \u0022semantic-kernel\u0022);\r\n\r\nConsole.WriteLine(result);\r\n\u0060\u0060\u0060\r\n\r\nLet\u0027s go over this code to understand what\u0027s happening:\r\n\r\n1. First, we load a prompt file from disk using the standard .NET I/O functions.\r\n2. Then, we call the \u0060InvokePromptAsync\u0060 method on the kernel instance to execute the\r\nprompt template providing \u0060arguments\u0060, and the \u0060templateFormat\u0060.\r\n3. Finally, we print the result to the console.\r\n\r\nThe code sample uses a kernel instance as described in [#s](#setting-up-semantic-kernel). The full source code for this sample is in the [GitHub repository][SK_TEMPLATE_SAMPLE].",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "Is it advisable to use the OpenTelemetry console exporter in production?",
    "Answer": "No, it is **not advisable** to use the OpenTelemetry console exporter in production. The provided context explains that while the console exporter is a useful method to check that everything is configured as intended, it should not be used in production because:\n\n1. **Performance Impact**: It will slow down your application.\n2. **Lack of Durability**: The console exporter is not durable and is unsuitable for long-term monitoring needs.\n\nInstead, a more robust and durable exporter, such as the Azure Monitor exporter for Application Insights, should be used in production environments.",
    "Context": [
      {
        "Id": 48,
        "OriginalFileName": "Content\\05-testing-and-monitoring-prompts.md",
        "Content": "We haven\u0027t covered logging yet; let\u0027s look at that before we dive into exporting traces to a monitoring tool like Application Insights.\r\n\r\n### Logging in LLM-based applications\r\n\r\nYou can configure logging as a separate piece of monitoring in Semantic Kernel. When you\u0027re building a console application, you\u0027ll need to add the following code to the startup:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar loggerFactory = LoggerFactory.Create(builder =\u003E\r\n{\r\n    builder.AddOpenTelemetry(options =\u003E\r\n\u00A0 \u00A0 {\r\n        options.SetResourceBuilder(resourceBuilder);\r\n        options.AddConsoleExporter();\r\n        options.IncludeFormattedMessage = true;\r\n        options.IncludeScopes = true;\r\n\u00A0 \u00A0 });\r\n\r\n    builder.SetMinimumLevel(LogLevel.Information);\r\n});\r\n\u0060\u0060\u0060\r\n\r\nThe code performs the following steps:\r\n\r\n1. First, \u00A0we create a new logger factory using the \u0060LoggerFactory\u0060 class.\r\n2. Next, we configure Open Telemetry as a logging provider, using the same OpenTelemetry settings as we did for traces and metrics. We then use the console exporter to write the logs to the terminal.\r\n3. Finally, we set the minimum log level to \u0060Information\u0060.\r\n\r\nYou can write log messages straight to the terminal without using OpenTelemetry, but this will break monitoring when you want to use a tool like Application Insights. Using OpenTelemetry with the logging factory connects the log messages as events to the traces generated by the application. This way, you can view traces and log messages as a single unit in the monitoring tool.\r\n\r\nThe OpenTelemetry console exporter provides a nice method to check that everything is configured as intended, but I don\u0027t recommend using it in production. It will slow down your application, and it\u0027s not durable. Let\u0027s look at how to configure Application Insights as an exporter and how to configure a dashboard for your LLM-based application using this tool.\r\n\r\n### Writing monitoring data to application insights\r\n\r\nConfiguring Application Insights as an exporter for traces, metrics, and logs requires an additional package. Add the \u0060Azure.Monitor.OpenTelemetry.Exporter\u0060 package to your project. Use the following terminal command to add the package to your project:\r\n\r\n\u0060\u0060\u0060bash\r\ndotnet add package Azure.Monitor.OpenTelemetry.Exporter\r\n\u0060\u0060\u0060\r\n\r\nThe following code demonstrates how to configure the exporter in a console application:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar resourceBuilder = ResourceBuilder.CreateDefault()\r\n\u00A0 \u00A0 .AddService(\u0022Chapter5.Telemetry\u0022);\r\n\r\nusing var traceProvider = Sdk.CreateTracerProviderBuilder()\r\n\u00A0 \u00A0 .SetResourceBuilder(resourceBuilder)\r\n\u00A0 \u00A0 .AddSource(\u0022Microsoft.SemanticKernel*\u0022)\r\n\u00A0 \u00A0 .AddAzureMonitorTraceExporter(options =\u003E\r\n\u00A0 \u00A0 {\r\n        options.ConnectionString =\r\n        configuration[\u0022Monitoring:ConnectionString\u0022];\r\n\u00A0 \u00A0 })\r\n\u00A0 \u00A0 .Build();\r\n\r\nusing var meterProvider = Sdk.CreateMeterProviderBuilder()\r\n\u00A0 \u00A0 .SetResourceBuilder(resourceBuilder)\r\n\u00A0 \u00A0 .AddMeter(\u0022Microsoft.SemanticKernel*\u0022)\r\n\u00A0 \u00A0 .AddAzureMonitorMetricExporter(options =\u003E\r\n\u00A0 \u00A0 {\r\n        options.ConnectionString = \r\n        configuration[\u0022Monitoring:ConnectionString\u0022];\r\n\u00A0 \u00A0 })\r\n\u00A0 \u00A0 .Build();\r\n\u0060\u0060\u0060\r\n\r\nThis code performs the following steps:\r\n\r\n1. First, we configure the resource builder for the service metadata to include in the metrics and traces.\r\n2. Then, we configure the trace provider as before, but we\u0027re adding the Azure Monitor exporter this time. We use the connection string from the configuration to connect to Application Insights.\r\n3. Next, we configure the meter provider as before, but this time, we add the Azure Monitor exporter using the same connection string we used for the tracer provider.\r\n\r\nYou can find the complete source code for this sample on [Github][MONITORING_SAMPLE].\r\n\r\nWith this code in place, you can start monitoring your LLM-based application in Application Insights. You need to set up an Application Insights resource in Azure to obtain the connection string for the exporters. A great manual for this is available in [the Application Insights documentation][AI_DOCS].",
        "Embedding": []
      },
      {
        "Id": 45,
        "OriginalFileName": "Content\\05-testing-and-monitoring-prompts.md",
        "Content": "After reading and reproducing the papers for G-Eval and GPTScore, I can tell you that the LLM evaluation is strange but shows remarkable agreement with human experts. And that is all you can ask for at the moment. So why are we doing this? It\u0027s because it\u0027s the best we have at the moment.\r\n\r\nWith this warning in mind, we need to conclude that testing is a great first step in sustainable LLM-based application development. But we need additional safety nets. Let\u0027s look at how you can monitor prompt interactions in production.\r\n\r\n## Monitoring prompt interactions in production\r\n\r\nIn the previous section, we learned how to test prompts in a controlled environment. Unfortunately, testing only gets you so far. The samples we collected in the last section are limited to what you can come up with and might not represent what users will do. The only way to get test data representative of the real world is to collect it from production.\r\n\r\nA\u003E Gathering telemetry data in Semantic Kernel is experimental at the moment. There are some valuable bits of information that you can collect today, but they are somewhat incomplete. I\u0027m sure they\u0027ll improve this in the coming months.\r\n\r\n### Safety precautions when collecting telemetry\r\n\r\nBefore collecting telemetry data from your LLM-based application, you must ensure you can. Users will enter all sorts of information into your application, and you must take the proper precautions before collecting any of that data.\r\n\r\nFor my projects, I notify users when I\u0027m collecting data and what I\u0027m collecting. I also make sure that I\u0027m not collecting any personal information. For most projects, I add a switch in the configuration so I can toggle the collection of prompts and responses. I enable the switch in a separate environment and only collect data for a limited time.\r\n\r\nThis approach limits what I can achieve, but it\u0027s a good compromise between respecting the users\u0027 privacy and collecting enough data to give them the best product possible.\r\n\r\nThere\u0027s another reason to limit your data collection efforts. Most of the data you\u0027ll get from production doesn\u0027t add value for testing, so it\u0027s better to target your collection efforts as much as possible.\r\n\r\nWith that in mind, let\u0027s look at enabling telemetry collection in Semantic Kernel.\r\n\r\n### Enabling tracing in your LLM-based application\r\n\r\nThe Semantic Kernel generates telemetry data in the [OpenTelemetry][OPEN_TELEMETRY] format through the .NET diagnostics stack. Let me give you a brief overview of these concepts.\r\n\r\nOpenTelemetry is a set of APIs, libraries, and tools for instrumenting applications to generate telemetry and process that telemetry into useful insights. The standard is implemented in many places for many languages. OpenTelemetry doesn\u0027t provide monitoring tools; it just uses libraries to get data into a monitoring tool.\r\n\r\nMicrosoft implements OpenTelemetry in .NET. Because the .NET stack is quite old, Microsoft hasn\u0027t adopted the same terminology as OpenTelemetry. The OpenTelemetry standard came after .NET was invented. That\u0027s why you\u0027ll find that the .NET stack uses different terminology for the same things. I made the following table to help you understand the differences:\r\n\r\n| Concept | .NET Equivalent | Description \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 |\r\n| ------- | --------------- | ------------------------------------------------------------------------- |\r\n| Span \u00A0 \u00A0| Activity \u00A0 \u00A0 \u00A0 \u00A0| A span is a unit of work in a trace. It has a start time and an end time. |\r\n| Tracer \u00A0| ActivitySource \u00A0| A span/activity is written to a tracer that processes it further \u00A0 \u00A0 \u00A0 \u00A0 \u00A0|\r\n| Event \u00A0 | Log message \u00A0 \u00A0 | An event with a description and attributes \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0|\r\n| Metric \u00A0| Meter \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 | A metric is a value that changes over time. \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 |",
        "Embedding": []
      },
      {
        "Id": 46,
        "OriginalFileName": "Content\\05-testing-and-monitoring-prompts.md",
        "Content": "Multiple spans will form a distributed trace in OpenTelemetry. So you can have calls to the LLM produce spans and other methods in your application. The spans are linked through a Trace ID. You can see related spans in a single trace when you write the traces to a monitoring application like Application Insights.\r\n\r\nLet\u0027s explore tracing in .NET more with an example. You can generate a trace in your code using an \u0060\u0060ActivitySource\u0060 using the following code:\r\n\r\n\u0060\u0060\u0060csharp\r\n\r\npublic class MyApplicationService\r\n{\r\n\u00A0 \u00A0 private static readonly activitySource = \r\n\u00A0 \u00A0 \u00A0 \u00A0 new ActivitySource(\u0022My.ActivitySource\u0022, \u00221.0.0\u0022);\r\n\r\n\u00A0 \u00A0 public void DoSomething()\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 using var activity = activitySource.StartActivity(\u0022DoSomething\u0022);\r\n\u00A0 \u00A0 \u00A0 \u00A0 // Do something\r\n\u00A0 \u00A0 }\r\n}\r\n\u0060\u0060\u0060\r\n\r\nThis code generates a trace with the name \u0060DoSomething\u0060 when you call the \u0060DoSomething\u0060 method. The trace is generated by the activity source \u0060My.ActivitySource\u0060 with version \u00601.0.0\u0060.\r\n\r\nActivities should always be created with a \u0060using\u0060 statement to ensure that they are collected by the garbage collector as soon as we\u0027re done. This is important because when you clean up an activity, the tracer records it, including how long it ran. So, cleaning up is a time-sensitive action.\r\n\r\nThe span produced by the activity is written to a tracer called an \u0060ActivitySource\u0060 in .NET. The activity source is responsible for writing the span to an exporter. We haven\u0027t configured one in this code because it\u0027s up to the consumer of our code to choose how and where to collect telemetry. You can add the following code to the startup of a console application to build an essential trace exporter that writes traces to the console:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar resourceBuilder = ResourceBuilder.CreateDefault()\r\n\u00A0 \u00A0 .AddService(\u0022Chapter5.Telemetry\u0022);\r\n\r\nusing var traceProvider = Sdk.CreateTracerProviderBuilder()\r\n\u00A0 \u00A0 .SetResourceBuilder(resourceBuilder)\r\n\u00A0 \u00A0 .AddSource(\u0022My.ActivitySource\u0022)\r\n\u00A0 \u00A0 .AddConsoleExporter()\r\n\u00A0 \u00A0 .Build();\r\n\u0060\u0060\u0060\r\n\r\nIn this code, we perform the following steps:\r\n\r\n1. First, we create a resource builder that adds metadata to the traces, such as the application name and version.\r\n2. Next, we create a tracer provider builder to configure with a source and an exporter, including the activity source we want to process and export the traces to the terminal.\r\n\r\nYou can configure multiple sources to the tracer provider. Only traces of sources you\u0027ve added to the tracer provider will be exported. In the sample, we only export our custom source. You can add sources using a prefix notation to say: \u0060My*\u0060 to export anything that starts with \u0060My\u0060.\r\n\r\nThe exporter is responsible for writing traces to a monitoring system. In this case, we\u0027re writing traces to the terminal, but you want something more durable in production. For example, an Azure Monitor exporter can write the traces to Application Insights. But you can also use other exporters. There are a lot of packages available on Nuget.\r\n\r\nWe\u0027ll need to modify the code to export traces from Semantic Kernel. The following code shows how to add telemetry to the Semantic Kernel:\r\n\r\n\u0060\u0060\u0060csharp\r\nusing var traceProvider = Sdk.CreateTracerProviderBuilder()\r\n\u00A0 \u00A0 .SetResourceBuilder(resourceBuilder)\r\n\u00A0 \u00A0 .AddSource(\u0022Microsoft.SemanticKernel*\u0022)\r\n\u00A0 \u00A0 .AddConsoleExporter()\r\n\u00A0 \u00A0 .Build();\r\n\u0060\u0060\u0060\r\n\r\nThis code adds the \u0060Microsoft.SemanticKernel*\u0060 prefix as a source to the tracer provider. And that\u0027s all you need to do to enable tracing for Semantic Kernel.\r\n\r\nBut what does Semantic Kernel export to the tracer provider? Spans created by Semantic Kernel contain data as defined in [the semantic conventions for generative AI systems][GENAI_STANDARD]. This experimental standard guides what information is needed to monitor LLM-based applications properly. It provides the following attributes, among others, for spans generated when calling an LLM:\r\n\r\n- \u0060gen_ai.system\u0060: The LLM provider you called in the code.",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "What settings are used for \u0027executionSettings\u0027?",
    "Answer": "The \u0060execution_settings\u0060 in the context of the book are used to describe hyperparameters for the prompt execution. These settings can vary depending on the LLM provider being used. Here are the settings demonstrated in the provided context:\n\n1. **Default Settings**: These apply to all LLM providers unless overridden.\n2. **Provider-Specific Settings**: These apply to specific LLM providers, identified by a \u0060ServiceId\u0060.\n\nThe following parameters are included in the \u0060execution_settings\u0060:\n\n- **top_p**: Controls nucleus sampling, which determines the cumulative probability of token selection.\n- **temperature**: Controls the randomness of the output. Lower values result in more deterministic responses, while higher values allow for more creativity.\n- **presence_penalty**: Penalizes the model for including tokens already present in the input to encourage diversity.\n- **frequency_penalty**: Penalizes the model for repeating tokens to reduce redundancy.\n- **max_tokens**: Sets the maximum number of tokens the model can generate.\n\nExample from the YAML configuration:\n\n\u0060\u0060\u0060yaml\nexecution_settings:\n  default:\n    top_p: 0.98\n    temperature: 0.7\n    presence_penalty: 0.0\n    frequency_penalty: 0.0\n    max_tokens: 1200\n  azure_openai:\n    top_p: 0.9\n    temperature: 0.7\n    presence_penalty: 0.0\n    frequency_penalty: 0.0\n    max_tokens: 1200\n\u0060\u0060\u0060\n\nIn this example:\n- The \u0060default\u0060 settings apply to any LLM provider unless overridden.\n- The \u0060azure_openai\u0060 settings are specifically configured for the Azure OpenAI provider.\n\nThese settings allow for fine-tuned control over the behavior of the LLM during execution.",
    "Context": [
      {
        "Id": 37,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "var serviceSelection = new PromptExecutionSettings()\r\n{\r\n    ServiceId = \u0022azure_openai\u0022\r\n};\r\n\r\nvar result = await kernel.InvokeAsync(prompt,\r\n    arguments: new KernelArguments(serviceSelection)\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 [\u0022dish\u0022] = \u0022pizza\u0022,\r\n\u00A0 \u00A0 \u00A0 \u00A0 [\u0022ingredients\u0022] = new List\u003Cstring\u003E\r\n\u00A0 \u00A0 \u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u0022pepperoni\u0022,\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u0022mozzarella\u0022,\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u0022spinach\u0022 \r\n\u00A0 \u00A0 \u00A0 \u00A0 }\r\n\u00A0 \u00A0 });\r\n\u0060\u0060\u0060\r\n\r\nIn this code, we stack an additional set of execution settings on top of the ones specified in the YAML file. You only need to select the name of the LLM provider you want to use in the \u0060ServiceId\u0060 property of the \u0060PromptExecutionSettings\u0060 object and pass the execution settings into the arguments of the \u0060InvokeAsync\u0060 method.\r\n\r\nSwitching between LLM providers is now as simple as setting a different value for the \u0060ServiceId\u0060 property in the execution settings.\r\n\r\nThe YAML format does have some limitations. I\u0027ve found that it is sensitive to mismatches in spaces and tabs. Editing the template text in the YAML file can be a bit cumbersome. But it\u0027s nice to have the option to store prompts in a single file with multiple sets of execution settings.\r\n\r\nWorking with multiple LLM providers is experimental at the time of writing. You must add \u0060\u003CNoWarn\u003ESKEXP0001\u003C/NoWarn\u003E\u0060 to your project file\u0027s \u0060PropertyGroup\u0060 section to suppress the build error telling you that the feature is experimental. The following code shows how to turn off the warning:\r\n\r\n\u0060\u0060\u0060xml\r\n\u003CPropertyGroup\u003E\r\n    \u003CNoWarn\u003ESKEXP0001\u003C/NoWarn\u003E\r\n\u003C/PropertyGroup\u003E\r\n\u0060\u0060\u0060\r\n\r\n{#working-with-chat-completion}\r\n## Using the chat history to your advantage\r\n\r\nIn [#s](#prompt-templates), we discussed using single prompts with Semantic Kernel. While this is useful for non-chat-based scenarios, I think it\u0027s important to also discuss chat-based scenarios. When talking about chat-based scenarios, you have to keep in mind that you don\u0027t necessarily need a chat user interface. We\u0027re strictly talking about a list of prompts and responses that form a conversation.\r\n\r\nYou\u0027ll need a different approach when working with chat history. We\u0027re no longer dealing with a scenario where we must generate a single response. Instead, we\u0027ll build a conversation that can have multiple turns and use that as the central unit of content to work with.\r\n\r\nLet\u0027s look at the chat history object first, as this will be the central unit we\u0027ll work with. You can build a conversation using the following piece of code:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar chatHistory = new ChatHistory();\r\n\r\nchatHistory.AddSystemMessage(\r\n\u00A0 \u00A0 \u0022You\u0027re a digital chef; help me cook. Your name is Flora.\u0022\r\n);\r\n\r\nchatHistory.AddUserMessage(\r\n\u00A0 \u00A0 \u0022Hi, I\u0027d like a nice recipe for a French-style apple pie.\u0022\r\n);\r\n\u0060\u0060\u0060\r\n\r\nThis code performs the following steps:\r\n\r\n1. First, we\u0027ll create a new chat history object.\r\n2. Then, we add a new system message containing instructions for the assistant.\r\n3. Finally, we add the first user message asking for an apple pie recipe.\r\n\r\nThe system instructions ensure that the LLM generates responses that are in line with the application\u0027s goal. They typically also provide instructions regarding the style of communication and things that we don\u0027t like to see in the response.\r\n\r\nYou can have three different types of messages in the chat history:\r\n\r\n1. System - Reserved for providing system-level instructions that should not be visible to a user.\r\n2. User - Content submitted by the user\r\n3. Assistant - Content generated by the LLM\r\n\r\nYou can add as many messages as you like to the chat history. Once we have a chat history object, we can use it to generate a response using the \u0060IChatCompletionService\u0060. The following code fragment demonstrates how this works:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar chatCompletionService = \r\n    kernel.Services.GetService\u003CIChatCompletionService\u003E();\r\n\r\nvar response = await chatCompletionService!\r\n    .GetChatMessageContentsAsync(chatHistory);\r\n\r\nConsole.WriteLine(response[0].Content);\r\n\u0060\u0060\u0060\r\n\r\nThe code fragment performs the following steps:",
        "Embedding": []
      },
      {
        "Id": 36,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "There\u0027s a lot to unpack here. Let\u0027s go over the essential properties:\r\n\r\n1. \u0060name\u0060 Determines the name of the kernel function that we\u0027ll create from the YAML\r\nfile.\r\n2. \u0060template\u0060 Contains the prompt template for the prompt. This is the same as the prompt template we used in the previous sections.\r\n3. \u0060template_format\u0060 Specifies the template format we use for the YAML prompt.\r\n4. \u0060input_variables\u0060 Describe the expected input data for the prompt.\r\n5. \u0060execution_settings\u0060 Describe the hyperparameters for the prompt.\r\n\r\nDepending on your use case, you\u0027ll want to configure more settings. For a complete description of the YAML format, you can refer to the [Semantic Kernel documentation](https://learn.microsoft.com/en-us/semantic-kernel/concepts/prompts/yaml-schema).\r\n\r\nYou\u0027ll need to add the \u0060Microsoft.SemanticKernel.Yaml\u0060 package to your project to use the YAML-based prompt files. After you\u0027ve added it to your project, you can load the YAML-based prompt with the following code:\r\n\r\n\u0060\u0060\u0060csharp\r\n\r\nvar promptTemplate = File.ReadAllText(\r\n    Path.Join(Directory.GetCurrentDirectory(), \u0022prompt.yaml\u0022)\r\n);\r\n\r\nvar prompt = kernel.CreateFunctionFromPromptYaml(\r\n    promptTemplate, \r\n\u00A0 \u00A0 new HandlebarsPromptTemplateFactory());\r\n\u0060\u0060\u0060\r\n\r\nIn this code fragment, we load the prompt YAML file from the disk and then use the \u0060CreateFunctionFromPromptYaml\u0060 method on the kernel to create a kernel function from the YAML file. As the Handlebars format we\u0027ve used for the prompt template isn\u0027t readily available, we have to explicitly tell Semantic Kernel that we want to use it for the prompt template.\r\n\r\nThe \u0060prompt\u0060 variable now contains a kernel function you can use in your application.\r\n\r\nThe YAML format is interesting because it allows you to store prompts with their execution settings in a single file. I find the \u0060execution_settings\u0060 option the best option for this format because you can configure different execution settings depending on the LLM provider you\u0027re using.\r\n\r\nLet me explain why having multiple execution settings is helpful with a bit more detail. Remember from [#s](#llomops-failover-strategies) that having a failover option in your application can be beneficial. Using the YAML file, you can specify multiple execution settings for different LLM providers.\r\n\r\nTo support the failover scenario, we need to modify the YAML file to include extra execution settings:\r\n\r\n\u0060\u0060\u0060yaml\r\nname: GenerateRecipe\r\ndescription: Generates a recipe based on ingredients in your fridge\r\ntemplate: |\r\n   Help me cook something nice, give me a recipe for {{ dish }}.\r\n   Use the ingredients I have in the fridge: \r\n\r\n\u00A0  {{#each ingredients}}\r\n\u00A0 \u00A0- {{ . }}\r\n \u00A0 {{/each}}\r\ntemplate_format: handlebars\r\ninput_variables:\r\n\u00A0 - name: dish\r\n\u00A0 \u00A0 description: The name of the dish you want to make\r\n\u00A0 \u00A0 is_required: true\r\n\u00A0 - name: ingredients\r\n\u00A0 \u00A0 description: A list of ingredient names you have in the fridge\r\n\u00A0 \u00A0 is_required: true\r\nexecution_settings:\r\n\u00A0 default:\r\n\u00A0 \u00A0 top_p: 0.98\r\n\u00A0 \u00A0 temperature: 0.7\r\n\u00A0 \u00A0 presence_penalty: 0.0\r\n\u00A0 \u00A0 frequency_penalty: 0.0\r\n\u00A0 \u00A0 max_tokens: 1200\r\n\u00A0 azure_openai:\r\n\u00A0 \u00A0 top_p: 0.9\r\n\u00A0 \u00A0 temperature: 0.7\r\n\u00A0 \u00A0 presence_penalty: 0.0\r\n\u00A0 \u00A0 frequency_penalty: 0.0\r\n\u00A0 \u00A0 max_tokens: 1200\r\n\u0060\u0060\u0060\r\n\r\nThe first set of execution settings specifies defaults for all LLM providers. The second set of execution settings is only valid for an LLM provider we registered with a service ID of \u0060azure_openai\u0060.\r\n\r\nWhen creating a kernel, we can add a chat completion service with a Service ID. Here\u0027s the code to do so:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar kernel = Kernel.CreateBuilder()\r\n\u00A0 \u00A0 .AddAzureOpenAIChatCompletion(\r\n        configuration[\u0022LanguageModel:DeploymentName\u0022]!,\r\n        endpoint: configuration[\u0022LanguageModel:Endpoint\u0022]!,\r\n        apiKey: configuration[\u0022LanguageModel:ApiKey\u0022]!,\r\n        serviceId: \u0022azure_openai\u0022\r\n\u00A0 \u00A0 )\r\n\u00A0 \u00A0 .Build();\r\n\u0060\u0060\u0060\r\n\r\nNow, when we want to execute a YAML-based prompt with the Azure OpenAI LLM provider, we can use the following code:\r\n\r\n\u0060\u0060\u0060csharp",
        "Embedding": []
      },
      {
        "Id": 9,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "make better decisions during the development process of your LLM-based applications.\r\n\r\n## Key concepts and terminology\r\n\r\nBefore we dive into building applications with LLMs, let\u0027s cover some essential concepts\r\nyou\u0027ll need to understand. Don\u0027t worry if some of these seem abstract at first \u2013 we\u0027ll\r\nput them into practice throughout the rest of the book.\r\n\r\n### Essential terminology\r\n\r\n#### Tokens and tokenization\r\n\r\nBefore an LLM can process your text, it needs to break it down into tokens. Think of\r\ntokens as the building blocks the model uses to understand text. A token can be:\r\n\r\n- A complete word\r\n- Part of a word\r\n- A number\r\n- A special character\r\n- A delimiter\r\n\r\nThe process works like this: your text gets split into tokens, then converted into\r\nnumbers using the model\u0027s vocabulary. This conversion is necessary because the\r\ntransformer-based neural network that powers the LLM can only process numbers, not raw\r\ntext.\r\n\r\nFor example, the word \u0022tokenization\u0022 might be split like this:\r\n\r\n\u0060\u0060\u0060plaintext\r\n\u0022tokenization\u0022 -\u003E [\u0022token\u0022, \u0022ization\u0022]\r\n\u0060\u0060\u0060\r\n\r\nWhen the model generates a response, the process happens in reverse \u2013 tokens are\r\nconverted back into text. This is why sometimes you might see slightly odd word splits\r\nin responses, especially with technical terms or rare words.\r\n\r\n#### Embeddings\r\n\r\nAt the input side of almost all LLMs is something called an embedding layer. This\r\ncomponent turns tokens into dense vectors that capture the semantic meaning of the text.\r\nThese are called embeddings or embedding vectors. Embedding vectors are interesting\r\nbecause they can represent the relationships between words in a mathematical space.\r\n\r\nThe embedding layer isn\u0027t just a random part of the model \u2013 it\u0027s trained on vast amounts\r\nof text to understand how words relate to each other based on how they are used. Think\r\nof it as a map where similar words or concepts are located close to each other.\r\n\r\nThe embedding concept can be challenging without seeing it in motion. [This\r\nwebsite][UNDERSTANDING_TRANSFORMERS] does a great job visualizing the concept.\r\n\r\nYou\u0027ll work directly with embeddings later when we implement the Retrieval Augmented\r\nGeneration (RAG) pattern in Chapter 5. For now, just know they\u0027re essential for LLMs\u0027\r\nunderstanding text.\r\n\r\n#### Context window\r\n\r\nEvery LLM has a limit to how much text it can consider at once \u2013 this is called the\r\ncontext window. It\u0027s essentially the model\u0027s short-term memory, including your input and\r\noutput.\r\n\r\nContext windows have grown significantly: Most modern commercial models handle 100K-250K\r\ninput tokens and around 4K output tokens. This translates to roughly 100K words on\r\naverage, which is the size of a full book. \r\n\r\nOpen-source models typically have smaller windows due to the more limited number of\r\nparameters in the neural network used in these models.\r\n\r\nThe exact size is found in the model\u0027s documentation (often called a model card). As\r\nwe\u0027ll see in later chapters, effectively managing this context window becomes crucial\r\nwhen building applications.\r\n\r\n{#llm-output-sampling} \r\n#### Output sampling and temperature\r\n\r\nLLMs aim to produce human-like text; one way they do this is through output sampling.\r\nWhen generating each token, the model doesn\u0027t just pick the most likely option; it\r\nsamples from a distribution of possibilities.\r\n\r\nTemperature is your main control over this sampling process:\r\n\r\n- Low temperature (0.1-0.3): More focused, deterministic responses\r\n- High temperature (0.7-0.9): More creative, varied output\r\n\r\nHere\u0027s how I typically set the temperature:\r\n\r\n- Code generation: 0.2 (we want precision)\r\n- Content creation: 0.7 (we want creativity)\r\n- Factual responses: 0.1 (we want consistency)\r\n\r\nWhile temperature is the most common setting you\u0027ll adjust, other sampling parameters\r\nare available. For a deeper dive into all the options, I recommend checking out [this\r\narticle][PARAMETER_EXPLANATION].\r\n\r\n#### Few-shot learning",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "What is affected by the probability distribution values in text generation?",
    "Answer": "In text generation, the probability distribution values affect **the selection of the next token** in the generated output. Specifically:\n\n1. **Top-P Sampling**: The probability distribution determines which tokens are included in the candidate pool for selection. Tokens are sorted by probability, and their cumulative probabilities are summed until the threshold (Top-P value) is reached. Only tokens within this cumulative probability range are considered, and one of them is randomly selected as the next token.\n\n2. **Temperature**: The probability distribution is adjusted (scaled) by the temperature value before performing Top-P sampling. Higher temperature results in a softer probability distribution, increasing the variety of candidate tokens. Lower temperature sharpens the distribution, making the model more deterministic and reducing the variety of tokens.\n\nIn summary, the probability distribution values directly influence **the creativity, variety, and determinism** of the generated text.",
    "Context": [
      {
        "Id": 32,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "#### Top-P\r\n\r\nLet\u0027s start with the Top-P setting, which refers to how LLMs select the output token that should come next in the response. The neural network of the LLM doesn\u0027t predict words; it predicts numbers that are then translated to tokens (words and parts of words) using a dictionary lookup. The LLM translates the numbers in the output layer to a probability distribution to select the next output token in the response. Each possible token is assigned a value representing the likelihood that it will be the next token. The higher the value, the more likely the token is to be the next token in the output.\r\n\r\nWe could select the token with the highest probability as the output from this probability distribution. However, this leads to repetitive and boring output. We can fix this by using a smarter sampling technique.\r\n\r\nAll modern LLMs use Top-P sampling. Here\u0027s how it works: First, we establish a threshold for the cumulative probability. We then sort the tokens from highest to lowest probability and start collecting tokens, adding their probabilities to a cumulative probability. We collect tokens until the cumulative probability value reaches the threshold. The process is demonstrated in [#s](#top-p-sampling).\r\n\r\n{#top-p-sampling} \r\n![Top-P sampling](probability-visualization.png)\r\n\r\nFor the samples in [#s](#top-p-sampling), we\u0027ve established a threshold of 0.9. The tokens with the highest probability are selected until the cumulative probability reaches 0.9. The tokens with the lowest probability are removed from the distribution.\r\n\r\nOnce we\u0027ve selected the candidate tokens in the distribution, we randomly select one of them as the output, resulting in a more varied output.\r\n\r\nThe key thing to remember here is that a higher Top-P results in more tokens being selected as candidates. A lower value will result in a more focused, repetitive output. This is ideal for generating code but less so for marketing content where you want greater variety.\r\n\r\n#### Temperature\r\n\r\nLLMs don\u0027t just use Top-P sampling. The inventors of these types of models found that Top-P sampling isn\u0027t enough to get natural text. So, they decided to add temperature to the mix. Temperature controls the shape of the probability distribution used to perform Top-P sampling.\r\n\r\nHere\u0027s how temperature works. Before performing the Top-P sampling, the raw values for the candidate tokens are divided by the temperature value. After that, the probability distribution is determined as usual. In [#s](#temperature-effects), you can see this in action with a before and after.\r\n\r\n{#temperature-effects} \r\n![The effect of temperature on the probability distribution](scaled-probability-comparison.png)\r\n\r\nThe higher the temperature, the softer the curve is. It takes longer for the cumulative probability function to reach the threshold, so more candidate tokens make it into the selection. Conversely, the lower the temperature, the harder the curve is. It takes less time to reach the threshold.\r\n\r\nAt this point, I understand that you\u0027re probably thinking that this is a lot to get through. And it is. I\u0027ve included an interactive notebook that demonstrates Top-P sampling and temperature scaling in the [GitHub repository][TOP_P_SAMPLE]. You can run the notebook inside Visual Studio Code to explore the effects of temperature and Top-P sampling and better understand these concepts.\r\n\r\n#### Presence Penalty\r\n\r\nAs you can tell by now, the sampling techniques used in LLMs are complex. And it gets even more complicated. Let me add one more variable to the mix: presence penalty. A positive presence penalty reduces the probability value of a token seen before in the output. This forces candidate tokens lower in the selection ranking for the Top-P sampling process. It is less likely that you see the same token in the output if you provide a positive value for the presence penalty.\r\n\r\n#### Frequency Penalty",
        "Embedding": []
      },
      {
        "Id": 9,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "make better decisions during the development process of your LLM-based applications.\r\n\r\n## Key concepts and terminology\r\n\r\nBefore we dive into building applications with LLMs, let\u0027s cover some essential concepts\r\nyou\u0027ll need to understand. Don\u0027t worry if some of these seem abstract at first \u2013 we\u0027ll\r\nput them into practice throughout the rest of the book.\r\n\r\n### Essential terminology\r\n\r\n#### Tokens and tokenization\r\n\r\nBefore an LLM can process your text, it needs to break it down into tokens. Think of\r\ntokens as the building blocks the model uses to understand text. A token can be:\r\n\r\n- A complete word\r\n- Part of a word\r\n- A number\r\n- A special character\r\n- A delimiter\r\n\r\nThe process works like this: your text gets split into tokens, then converted into\r\nnumbers using the model\u0027s vocabulary. This conversion is necessary because the\r\ntransformer-based neural network that powers the LLM can only process numbers, not raw\r\ntext.\r\n\r\nFor example, the word \u0022tokenization\u0022 might be split like this:\r\n\r\n\u0060\u0060\u0060plaintext\r\n\u0022tokenization\u0022 -\u003E [\u0022token\u0022, \u0022ization\u0022]\r\n\u0060\u0060\u0060\r\n\r\nWhen the model generates a response, the process happens in reverse \u2013 tokens are\r\nconverted back into text. This is why sometimes you might see slightly odd word splits\r\nin responses, especially with technical terms or rare words.\r\n\r\n#### Embeddings\r\n\r\nAt the input side of almost all LLMs is something called an embedding layer. This\r\ncomponent turns tokens into dense vectors that capture the semantic meaning of the text.\r\nThese are called embeddings or embedding vectors. Embedding vectors are interesting\r\nbecause they can represent the relationships between words in a mathematical space.\r\n\r\nThe embedding layer isn\u0027t just a random part of the model \u2013 it\u0027s trained on vast amounts\r\nof text to understand how words relate to each other based on how they are used. Think\r\nof it as a map where similar words or concepts are located close to each other.\r\n\r\nThe embedding concept can be challenging without seeing it in motion. [This\r\nwebsite][UNDERSTANDING_TRANSFORMERS] does a great job visualizing the concept.\r\n\r\nYou\u0027ll work directly with embeddings later when we implement the Retrieval Augmented\r\nGeneration (RAG) pattern in Chapter 5. For now, just know they\u0027re essential for LLMs\u0027\r\nunderstanding text.\r\n\r\n#### Context window\r\n\r\nEvery LLM has a limit to how much text it can consider at once \u2013 this is called the\r\ncontext window. It\u0027s essentially the model\u0027s short-term memory, including your input and\r\noutput.\r\n\r\nContext windows have grown significantly: Most modern commercial models handle 100K-250K\r\ninput tokens and around 4K output tokens. This translates to roughly 100K words on\r\naverage, which is the size of a full book. \r\n\r\nOpen-source models typically have smaller windows due to the more limited number of\r\nparameters in the neural network used in these models.\r\n\r\nThe exact size is found in the model\u0027s documentation (often called a model card). As\r\nwe\u0027ll see in later chapters, effectively managing this context window becomes crucial\r\nwhen building applications.\r\n\r\n{#llm-output-sampling} \r\n#### Output sampling and temperature\r\n\r\nLLMs aim to produce human-like text; one way they do this is through output sampling.\r\nWhen generating each token, the model doesn\u0027t just pick the most likely option; it\r\nsamples from a distribution of possibilities.\r\n\r\nTemperature is your main control over this sampling process:\r\n\r\n- Low temperature (0.1-0.3): More focused, deterministic responses\r\n- High temperature (0.7-0.9): More creative, varied output\r\n\r\nHere\u0027s how I typically set the temperature:\r\n\r\n- Code generation: 0.2 (we want precision)\r\n- Content creation: 0.7 (we want creativity)\r\n- Factual responses: 0.1 (we want consistency)\r\n\r\nWhile temperature is the most common setting you\u0027ll adjust, other sampling parameters\r\nare available. For a deeper dive into all the options, I recommend checking out [this\r\narticle][PARAMETER_EXPLANATION].\r\n\r\n#### Few-shot learning",
        "Embedding": []
      },
      {
        "Id": 72,
        "OriginalFileName": "Content\\07-retrieval-augmented-generation.md",
        "Content": "As luck would have it there\u0027s a tool called [Ragas][RAGAS] that offers a way to measure how faithful a response is using the faithfulness metric.\r\n\r\nThe faithfulness metric depends on two pieces of information:\r\n\r\n- The context information retrieved from the vector database\r\n- The answer generated by the LLM\r\n\r\nTo get these two pieces of information, we\u0027ll need a question that a user would ask, and we\u0027ll need to record what information was used to answer the question. We\u0027ll get the information as part of a validation workflow that looks like this:\r\n\r\n1. First, we need to create a validation dataset containing questions we can ask.\r\n2. Next, we need to generate output based on the validation questions recording the answer, the retrieved context information, and the question we asked.\r\n3. Finally, we can measure the faithfulness of the responses by looking at how many terms from the retrieved context are used in the final response.\r\n\r\nSteps 1 and 2 you can do with code in C#, while step 3 we need to build with Python and Ragas.\r\n\r\nLet\u0027s start by writing code to generate validation data for the RAG pipeline.\r\n\r\n### Generating validation data\r\n\r\nI\u0027ve found that generating synthetic data is a great way to get validation questions fast. You can of course gather questions from users or monitoring data but interviewing users takes up a lot of time, and gathering data from the monitoring system is only feasible if you\u0027re running in production. \r\n\r\nLet\u0027s look at how to generate synthetic questions to help you get started validating your RAG pipeline. To generate synthetic questions, you can use the following prompt:\r\n\r\n\u0060\u0060\u0060text\r\nHelp prepare QnA pairs with short answers for a topic by\r\nextracting them from the given text. Make sure to write detailed\r\nquestions about the given text with a detailed answer.\r\nOutput must list the specified number of QnA pairs in JSON.\r\n\r\nText:\r\n\u003C|text_start|\u003E\r\n{{context}}\r\n\u003C|text_end|\u003E\r\n\r\nOutput with {{count}} QnA pairs with detailed answers:  \r\n\u0060\u0060\u0060\r\n\r\nIn this prompt we instruct the LLM to generate questions and answers for a given piece of context information. We then give it the context information we want to base the questions on and finally hint that we want a specific number of QnA pairs.\r\n\r\nThe code to execute the prompt looks like this:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar prompt = kernel.CreateFunctionFromPromptYaml(\r\n    EmbeddedResource.Read(\u0022Prompts.LongAnswerQuestion.yaml\u0022), \r\n    new HandlebarsPromptTemplateFactory());\r\n\r\nvar promptExecutionSettings = new OpenAIPromptExecutionSettings\r\n{\r\n    ResponseFormat = typeof(QuestionGeneratorResult)\r\n};\r\n\r\nvar promptExecution = await prompt.InvokeAsync(kernel, new KernelArguments(promptExecutionSettings)\r\n{\r\n    [\u0022context\u0022] = content,\r\n    [\u0022count\u0022] = numberOfQuestions\r\n});\r\n\r\nvar responseData = JsonSerializer.Deserialize\u003CQuestionGeneratorResult\u003E(\r\n    promptExecution.GetValue\u003Cstring\u003E()!);\r\n\u0060\u0060\u0060\r\n\r\nIn this code we perform the following steps:\r\n\r\n1. First, we load the prompt as a kernel function\r\n2. Next, we create a set of prompt execution settings in which I specify the structure of the answer I\u0027m expecting from the LLM.\r\n3. Then, we execute the prompt with a piece of context information and the number of questions we want generated.\r\n4. Finally, we deserialize the content using the JSON serializer into the question generator result.\r\n\r\nThe \u0060QuestionGeneratorResult\u0060 class looks like this:\r\n\r\n\u0060\u0060\u0060csharp\r\npublic class QuestionGeneratorResult\r\n{\r\n    public List\u003CQuestionAnswerPair\u003E QuestionAnswerPairs { get; set; } = new();\r\n}\r\n\r\npublic class QuestionAnswerPair\r\n{\r\n    public string Question { get; set; } = default!;\r\n    public string Answer { get; set; } = default!;\r\n}\r\n\u0060\u0060\u0060\r\n\r\nWe haven\u0027t discussed structured output yet, but here\u0027s the important part. In the C# sample code we\u0027re setting the \u0060ResponseFormat\u0060 property to the  \u0060QuestionGeneratorResult\u0060 class. This tells the LLM to generate JSON output in the shape of this class.",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "How can a console application be configured to write traces to the console in .NET?",
    "Answer": "To configure a console application to write traces to the console in .NET, you can use the OpenTelemetry SDK with a console exporter. Here\u0027s how you can do it:\n\n### Code Example:\n\u0060\u0060\u0060csharp\nvar resourceBuilder = ResourceBuilder.CreateDefault()\n\u00A0 \u00A0 .AddService(\u0022YourAppName\u0022);\n\nusing var traceProvider = Sdk.CreateTracerProviderBuilder()\n\u00A0 \u00A0 .SetResourceBuilder(resourceBuilder)\n\u00A0 \u00A0 .AddSource(\u0022Your.ActivitySource\u0022)\n\u00A0 \u00A0 .AddConsoleExporter()\n\u00A0 \u00A0 .Build();\n\u0060\u0060\u0060\n\n### Steps Explained:\n1. **Resource Metadata**:\n   - Use \u0060ResourceBuilder.CreateDefault()\u0060 to create a resource builder and add metadata about your application (e.g., service name or version) using \u0060.AddService(\u0022YourAppName\u0022)\u0060.\n\n2. **Configure Tracer Provider**:\n   - Use \u0060Sdk.CreateTracerProviderBuilder()\u0060 to create a tracer provider builder.\n   - Use \u0060.SetResourceBuilder(resourceBuilder)\u0060 to attach the resource metadata to the traces.\n   - Add the activity source(s) you want to trace using \u0060.AddSource(\u0022Your.ActivitySource\u0022)\u0060. Replace \u0060\u0022Your.ActivitySource\u0022\u0060 with the name of the activity source you want to trace.\n\n3. **Add Console Exporter**:\n   - Use \u0060.AddConsoleExporter()\u0060 to configure the traces to be exported to the console.\n\n4. **Build the Tracer Provider**:\n   - Call \u0060.Build()\u0060 to finalize the tracer provider configuration.\n\n### Important Notes:\n- Only traces from activity sources you explicitly add using \u0060.AddSource()\u0060 will be exported.\n- The console exporter is useful for debugging or development purposes, but it is not recommended for production due to performance and durability concerns.\n- Replace \u0060\u0022Your.ActivitySource\u0022\u0060 with the name of your activity source (e.g., \u0060\u0022My.ActivitySource\u0022\u0060 as used in the example in the context).\n\nThis setup ensures that trace data from your application is written to the console in a readable format.",
    "Context": [
      {
        "Id": 46,
        "OriginalFileName": "Content\\05-testing-and-monitoring-prompts.md",
        "Content": "Multiple spans will form a distributed trace in OpenTelemetry. So you can have calls to the LLM produce spans and other methods in your application. The spans are linked through a Trace ID. You can see related spans in a single trace when you write the traces to a monitoring application like Application Insights.\r\n\r\nLet\u0027s explore tracing in .NET more with an example. You can generate a trace in your code using an \u0060\u0060ActivitySource\u0060 using the following code:\r\n\r\n\u0060\u0060\u0060csharp\r\n\r\npublic class MyApplicationService\r\n{\r\n\u00A0 \u00A0 private static readonly activitySource = \r\n\u00A0 \u00A0 \u00A0 \u00A0 new ActivitySource(\u0022My.ActivitySource\u0022, \u00221.0.0\u0022);\r\n\r\n\u00A0 \u00A0 public void DoSomething()\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 using var activity = activitySource.StartActivity(\u0022DoSomething\u0022);\r\n\u00A0 \u00A0 \u00A0 \u00A0 // Do something\r\n\u00A0 \u00A0 }\r\n}\r\n\u0060\u0060\u0060\r\n\r\nThis code generates a trace with the name \u0060DoSomething\u0060 when you call the \u0060DoSomething\u0060 method. The trace is generated by the activity source \u0060My.ActivitySource\u0060 with version \u00601.0.0\u0060.\r\n\r\nActivities should always be created with a \u0060using\u0060 statement to ensure that they are collected by the garbage collector as soon as we\u0027re done. This is important because when you clean up an activity, the tracer records it, including how long it ran. So, cleaning up is a time-sensitive action.\r\n\r\nThe span produced by the activity is written to a tracer called an \u0060ActivitySource\u0060 in .NET. The activity source is responsible for writing the span to an exporter. We haven\u0027t configured one in this code because it\u0027s up to the consumer of our code to choose how and where to collect telemetry. You can add the following code to the startup of a console application to build an essential trace exporter that writes traces to the console:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar resourceBuilder = ResourceBuilder.CreateDefault()\r\n\u00A0 \u00A0 .AddService(\u0022Chapter5.Telemetry\u0022);\r\n\r\nusing var traceProvider = Sdk.CreateTracerProviderBuilder()\r\n\u00A0 \u00A0 .SetResourceBuilder(resourceBuilder)\r\n\u00A0 \u00A0 .AddSource(\u0022My.ActivitySource\u0022)\r\n\u00A0 \u00A0 .AddConsoleExporter()\r\n\u00A0 \u00A0 .Build();\r\n\u0060\u0060\u0060\r\n\r\nIn this code, we perform the following steps:\r\n\r\n1. First, we create a resource builder that adds metadata to the traces, such as the application name and version.\r\n2. Next, we create a tracer provider builder to configure with a source and an exporter, including the activity source we want to process and export the traces to the terminal.\r\n\r\nYou can configure multiple sources to the tracer provider. Only traces of sources you\u0027ve added to the tracer provider will be exported. In the sample, we only export our custom source. You can add sources using a prefix notation to say: \u0060My*\u0060 to export anything that starts with \u0060My\u0060.\r\n\r\nThe exporter is responsible for writing traces to a monitoring system. In this case, we\u0027re writing traces to the terminal, but you want something more durable in production. For example, an Azure Monitor exporter can write the traces to Application Insights. But you can also use other exporters. There are a lot of packages available on Nuget.\r\n\r\nWe\u0027ll need to modify the code to export traces from Semantic Kernel. The following code shows how to add telemetry to the Semantic Kernel:\r\n\r\n\u0060\u0060\u0060csharp\r\nusing var traceProvider = Sdk.CreateTracerProviderBuilder()\r\n\u00A0 \u00A0 .SetResourceBuilder(resourceBuilder)\r\n\u00A0 \u00A0 .AddSource(\u0022Microsoft.SemanticKernel*\u0022)\r\n\u00A0 \u00A0 .AddConsoleExporter()\r\n\u00A0 \u00A0 .Build();\r\n\u0060\u0060\u0060\r\n\r\nThis code adds the \u0060Microsoft.SemanticKernel*\u0060 prefix as a source to the tracer provider. And that\u0027s all you need to do to enable tracing for Semantic Kernel.\r\n\r\nBut what does Semantic Kernel export to the tracer provider? Spans created by Semantic Kernel contain data as defined in [the semantic conventions for generative AI systems][GENAI_STANDARD]. This experimental standard guides what information is needed to monitor LLM-based applications properly. It provides the following attributes, among others, for spans generated when calling an LLM:\r\n\r\n- \u0060gen_ai.system\u0060: The LLM provider you called in the code.",
        "Embedding": []
      },
      {
        "Id": 48,
        "OriginalFileName": "Content\\05-testing-and-monitoring-prompts.md",
        "Content": "We haven\u0027t covered logging yet; let\u0027s look at that before we dive into exporting traces to a monitoring tool like Application Insights.\r\n\r\n### Logging in LLM-based applications\r\n\r\nYou can configure logging as a separate piece of monitoring in Semantic Kernel. When you\u0027re building a console application, you\u0027ll need to add the following code to the startup:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar loggerFactory = LoggerFactory.Create(builder =\u003E\r\n{\r\n    builder.AddOpenTelemetry(options =\u003E\r\n\u00A0 \u00A0 {\r\n        options.SetResourceBuilder(resourceBuilder);\r\n        options.AddConsoleExporter();\r\n        options.IncludeFormattedMessage = true;\r\n        options.IncludeScopes = true;\r\n\u00A0 \u00A0 });\r\n\r\n    builder.SetMinimumLevel(LogLevel.Information);\r\n});\r\n\u0060\u0060\u0060\r\n\r\nThe code performs the following steps:\r\n\r\n1. First, \u00A0we create a new logger factory using the \u0060LoggerFactory\u0060 class.\r\n2. Next, we configure Open Telemetry as a logging provider, using the same OpenTelemetry settings as we did for traces and metrics. We then use the console exporter to write the logs to the terminal.\r\n3. Finally, we set the minimum log level to \u0060Information\u0060.\r\n\r\nYou can write log messages straight to the terminal without using OpenTelemetry, but this will break monitoring when you want to use a tool like Application Insights. Using OpenTelemetry with the logging factory connects the log messages as events to the traces generated by the application. This way, you can view traces and log messages as a single unit in the monitoring tool.\r\n\r\nThe OpenTelemetry console exporter provides a nice method to check that everything is configured as intended, but I don\u0027t recommend using it in production. It will slow down your application, and it\u0027s not durable. Let\u0027s look at how to configure Application Insights as an exporter and how to configure a dashboard for your LLM-based application using this tool.\r\n\r\n### Writing monitoring data to application insights\r\n\r\nConfiguring Application Insights as an exporter for traces, metrics, and logs requires an additional package. Add the \u0060Azure.Monitor.OpenTelemetry.Exporter\u0060 package to your project. Use the following terminal command to add the package to your project:\r\n\r\n\u0060\u0060\u0060bash\r\ndotnet add package Azure.Monitor.OpenTelemetry.Exporter\r\n\u0060\u0060\u0060\r\n\r\nThe following code demonstrates how to configure the exporter in a console application:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar resourceBuilder = ResourceBuilder.CreateDefault()\r\n\u00A0 \u00A0 .AddService(\u0022Chapter5.Telemetry\u0022);\r\n\r\nusing var traceProvider = Sdk.CreateTracerProviderBuilder()\r\n\u00A0 \u00A0 .SetResourceBuilder(resourceBuilder)\r\n\u00A0 \u00A0 .AddSource(\u0022Microsoft.SemanticKernel*\u0022)\r\n\u00A0 \u00A0 .AddAzureMonitorTraceExporter(options =\u003E\r\n\u00A0 \u00A0 {\r\n        options.ConnectionString =\r\n        configuration[\u0022Monitoring:ConnectionString\u0022];\r\n\u00A0 \u00A0 })\r\n\u00A0 \u00A0 .Build();\r\n\r\nusing var meterProvider = Sdk.CreateMeterProviderBuilder()\r\n\u00A0 \u00A0 .SetResourceBuilder(resourceBuilder)\r\n\u00A0 \u00A0 .AddMeter(\u0022Microsoft.SemanticKernel*\u0022)\r\n\u00A0 \u00A0 .AddAzureMonitorMetricExporter(options =\u003E\r\n\u00A0 \u00A0 {\r\n        options.ConnectionString = \r\n        configuration[\u0022Monitoring:ConnectionString\u0022];\r\n\u00A0 \u00A0 })\r\n\u00A0 \u00A0 .Build();\r\n\u0060\u0060\u0060\r\n\r\nThis code performs the following steps:\r\n\r\n1. First, we configure the resource builder for the service metadata to include in the metrics and traces.\r\n2. Then, we configure the trace provider as before, but we\u0027re adding the Azure Monitor exporter this time. We use the connection string from the configuration to connect to Application Insights.\r\n3. Next, we configure the meter provider as before, but this time, we add the Azure Monitor exporter using the same connection string we used for the tracer provider.\r\n\r\nYou can find the complete source code for this sample on [Github][MONITORING_SAMPLE].\r\n\r\nWith this code in place, you can start monitoring your LLM-based application in Application Insights. You need to set up an Application Insights resource in Azure to obtain the connection string for the exporters. A great manual for this is available in [the Application Insights documentation][AI_DOCS].",
        "Embedding": []
      },
      {
        "Id": 51,
        "OriginalFileName": "Content\\05-testing-and-monitoring-prompts.md",
        "Content": "Provide a descriptive name for the rule, and click *Next* to go to the next step as shown in [#s](#export-rule-step-2). You can select the tables you want to export on the next screen. The \u0060AppTraces\u0060 table contains the data for the prompts and responses. Select this table and click *Next* to the destination configuration screen.\r\n\r\n{#export-rule-step-2} \r\n![Table selection screen](azure-law-export-rules-step-2.png)\r\n\r\nYou have two choices for the data\u0027s destination. We\u0027ll store the exported data in a storage account. [#s](#export-rule-step-3) shows the screen to configure the destination.\r\n\r\n{#export-rule-step-3} \r\n![Destination configuration screen](azure-law-export-rules-step-3.png)\r\n\r\nSelect the storage account where you want to store the exported data and complete the configuration.\r\n\r\nAfter you\u0027ve completed the configuration, any new data coming into the Log Analytics Workspace is automatically exported to your storage account. The export job in Azure will create a new container for each table you\u0027ve selected. Since we selected the application insights traces, we\u0027ll get a container called \u0060am-apptraces\u0060.\r\n\r\nYou can download the data from the storage account using the [Azure Storage Explorer][STORAGE_EXPLORER] or programmatically using the [Azure Blob Storage Package][NUGET_BLOB_STORAGE].\r\n\r\nLet\u0027s look at how you can use the Blob Storage package to download the data from the storage account. The following code demonstrates how to set up a connection to the storage account and download all the data:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar configuration = new ConfigurationBuilder()\r\n\u00A0 \u00A0 .AddUserSecrets\u003CProgram\u003E()\r\n\u00A0 \u00A0 .Build();\r\n\r\nvar blobServiceClient = new BlobServiceClient(\r\n    configuration[\u0022ConnectionStrings:BlobStorage\u0022]);\r\n\r\nvar traceContainerClient = blobServiceClient.GetBlobContainerClient(\r\n\u00A0 \u00A0 \u0022am-apptraces\u0022);\r\n\u0060\u0060\u0060\r\n\r\nThe code performs the following steps:\r\n\r\n1. First, we build a configuration object to securely load the connection string for the blob storage account.\r\n2. Next, we create a new \u0060BlobServiceClient\u0060 object, providing a connection string for the storage account containing the trace data.\r\n3. Finally, we get a reference to the container that contains the trace data.\r\n\r\nAfter we\u0027ve obtained a connection, we can start processing the JSON files in the storage container one by one:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar processor = new TraceEventDataProcessor();\r\n\r\nvar serializerSettings = new JsonSerializerOptions()\r\n{\r\n    PropertyNameCaseInsensitive = true,\r\n    DefaultIgnoreCondition = JsonIgnoreCondition.WhenWritingNull,\r\n    UnknownTypeHandling = JsonUnknownTypeHandling.JsonElement\r\n};\r\n\r\nawait foreach (var blobItem in traceContainerClient.GetBlobsAsync())\r\n{\r\n\u00A0 \u00A0 if (blobItem.Name.EndsWith(\u0022.json\u0022))\r\n\u00A0 \u00A0 {\r\n        var blobClient = traceContainerClient.GetBlobClient(blobItem.Name);\r\n        using var reader = new StreamReader(blobClient.OpenRead());\r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 while (!reader.EndOfStream)\r\n\u00A0 \u00A0 \u00A0 \u00A0 {\r\n            var rawEventData = reader.ReadLine();\r\n\r\n            var eventData = JsonSerializer.Deserialize\u003CTraceEventData\u003E(\r\n            rawEventData!, serializerSettings);\r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 if (eventData!.Message == \u0022gen_ai.content.completion\u0022 || \r\n                eventData.Message == \u0022gen_ai.content.prompt\u0022)\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 {\r\n                processor.ProcessEvent(eventData);\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 }\r\n\u00A0 \u00A0 \u00A0 \u00A0 }\r\n\u00A0 \u00A0 }\r\n}\r\n\u0060\u0060\u0060\r\n\r\nThis code performs the following steps:\r\n\r\n1. First, we create a new \u0060TraceEventDataProcessor\u0060 object to process the trace data into functional test samples.\r\n2. Next, we iterate over the files in the trace storage container and read the contents of each file. The content in the file is stored in JSON lines format. Each line is a JSON object.\r\n3. Then, we deserialize the JSON object data into a \u0060TraceEventData\u0060 object, which is a class that will represent the trace data.\r\n4. Finally, we process the trace data using the \u0060TraceEventDataProcessor\u0060 object.\r\n\r\nLet\u0027s look at the shape of the \u0060TraceEventData\u0060 class first:\r\n\r\n\u0060\u0060\u0060csharp",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "Can question and answer pairs be generated without using prompts?",
    "Answer": "Based on the provided context, **no**, question and answer pairs cannot be generated without using prompts. The context explicitly describes a process where prompts are used to instruct the LLM to generate QnA pairs. For example, a specific prompt is provided to guide the LLM in generating detailed QnA pairs based on a piece of context information.\n\nThe process involves crafting a prompt, feeding it to the LLM, and configuring the output to meet the desired structure. Prompts are an integral part of the workflow for generating synthetic QnA data, as described in the context.\n\n",
    "Context": [
      {
        "Id": 72,
        "OriginalFileName": "Content\\07-retrieval-augmented-generation.md",
        "Content": "As luck would have it there\u0027s a tool called [Ragas][RAGAS] that offers a way to measure how faithful a response is using the faithfulness metric.\r\n\r\nThe faithfulness metric depends on two pieces of information:\r\n\r\n- The context information retrieved from the vector database\r\n- The answer generated by the LLM\r\n\r\nTo get these two pieces of information, we\u0027ll need a question that a user would ask, and we\u0027ll need to record what information was used to answer the question. We\u0027ll get the information as part of a validation workflow that looks like this:\r\n\r\n1. First, we need to create a validation dataset containing questions we can ask.\r\n2. Next, we need to generate output based on the validation questions recording the answer, the retrieved context information, and the question we asked.\r\n3. Finally, we can measure the faithfulness of the responses by looking at how many terms from the retrieved context are used in the final response.\r\n\r\nSteps 1 and 2 you can do with code in C#, while step 3 we need to build with Python and Ragas.\r\n\r\nLet\u0027s start by writing code to generate validation data for the RAG pipeline.\r\n\r\n### Generating validation data\r\n\r\nI\u0027ve found that generating synthetic data is a great way to get validation questions fast. You can of course gather questions from users or monitoring data but interviewing users takes up a lot of time, and gathering data from the monitoring system is only feasible if you\u0027re running in production. \r\n\r\nLet\u0027s look at how to generate synthetic questions to help you get started validating your RAG pipeline. To generate synthetic questions, you can use the following prompt:\r\n\r\n\u0060\u0060\u0060text\r\nHelp prepare QnA pairs with short answers for a topic by\r\nextracting them from the given text. Make sure to write detailed\r\nquestions about the given text with a detailed answer.\r\nOutput must list the specified number of QnA pairs in JSON.\r\n\r\nText:\r\n\u003C|text_start|\u003E\r\n{{context}}\r\n\u003C|text_end|\u003E\r\n\r\nOutput with {{count}} QnA pairs with detailed answers:  \r\n\u0060\u0060\u0060\r\n\r\nIn this prompt we instruct the LLM to generate questions and answers for a given piece of context information. We then give it the context information we want to base the questions on and finally hint that we want a specific number of QnA pairs.\r\n\r\nThe code to execute the prompt looks like this:\r\n\r\n\u0060\u0060\u0060csharp\r\nvar prompt = kernel.CreateFunctionFromPromptYaml(\r\n    EmbeddedResource.Read(\u0022Prompts.LongAnswerQuestion.yaml\u0022), \r\n    new HandlebarsPromptTemplateFactory());\r\n\r\nvar promptExecutionSettings = new OpenAIPromptExecutionSettings\r\n{\r\n    ResponseFormat = typeof(QuestionGeneratorResult)\r\n};\r\n\r\nvar promptExecution = await prompt.InvokeAsync(kernel, new KernelArguments(promptExecutionSettings)\r\n{\r\n    [\u0022context\u0022] = content,\r\n    [\u0022count\u0022] = numberOfQuestions\r\n});\r\n\r\nvar responseData = JsonSerializer.Deserialize\u003CQuestionGeneratorResult\u003E(\r\n    promptExecution.GetValue\u003Cstring\u003E()!);\r\n\u0060\u0060\u0060\r\n\r\nIn this code we perform the following steps:\r\n\r\n1. First, we load the prompt as a kernel function\r\n2. Next, we create a set of prompt execution settings in which I specify the structure of the answer I\u0027m expecting from the LLM.\r\n3. Then, we execute the prompt with a piece of context information and the number of questions we want generated.\r\n4. Finally, we deserialize the content using the JSON serializer into the question generator result.\r\n\r\nThe \u0060QuestionGeneratorResult\u0060 class looks like this:\r\n\r\n\u0060\u0060\u0060csharp\r\npublic class QuestionGeneratorResult\r\n{\r\n    public List\u003CQuestionAnswerPair\u003E QuestionAnswerPairs { get; set; } = new();\r\n}\r\n\r\npublic class QuestionAnswerPair\r\n{\r\n    public string Question { get; set; } = default!;\r\n    public string Answer { get; set; } = default!;\r\n}\r\n\u0060\u0060\u0060\r\n\r\nWe haven\u0027t discussed structured output yet, but here\u0027s the important part. In the C# sample code we\u0027re setting the \u0060ResponseFormat\u0060 property to the  \u0060QuestionGeneratorResult\u0060 class. This tells the LLM to generate JSON output in the shape of this class.",
        "Embedding": []
      },
      {
        "Id": 30,
        "OriginalFileName": "Content\\04-the-art-and-nonsense-of-prompt-engineering.md",
        "Content": "Any LLM can reproduce patterns learned from the large body of text on which it was trained. But that body of text hopefully doesn\u0027t contain internal documents of the client or company you work for. So, if you need to answer questions about internal information, you must tell the LLM about the information.\r\n\r\nWe\u0027ll cover Retrieval Augmented Generation in greater depth in chapter 7. But for now, it\u0027s essential to understand that the LLM can\u0027t remember facts. It\u0027s a talking parrot that reproduces patterns it has seen before. This looks like a limitation, but you can turn this into a powerful trait.\r\n\r\nFor example, I want to answer a question about the company\u0027s policy on remote work. In that case, I can first look up documents related to the question by performing a similarity search using a search engine. The documents I can put into the prompt as additional context and then instruct the LLM to answer the question. A prompt for answering questions typically will look like this:\r\n\r\n\u0060\u0060\u0060text\r\nYou\u0027re a digital assistant for our company\u0027s HR department. \r\nPlease answer the employee\u0027s question based on the content \r\nprovided as context.\r\n\r\n## Context\r\n\r\n\u003CThe found fragments about the policy on remote work\u003E\r\n\r\n## Question\r\n\r\n\u003CThe employee question\u003E\r\n\u0060\u0060\u0060\r\n\r\nYou can consider the context heading in the prompt as samples that the model can use to generate a helpful response. Because of the talking parrot effect, the LLM will likely reproduce the content you provided. The model is \u0022learning\u0022 from information in the context. This is often called [in-context learning][INCONTEXT_LEARNING].\r\n\r\nIf the information in the context is incorrect, vague, or non-existent, then the LLM can\u0027t match the pattern, and you\u0027ll get an answer that doesn\u0027t make much sense or is plain misleading. So it\u0027s essential to ensure you only inject safe and sensible information.\r\n\r\nAdding samples to a prompt helps establish a good pattern for the LLM. You\u0027ll see many people add examples of their desired results to a prompt. Adding examples to your prompt is called one-shot learning or few-shot learning if you add more than one example of the expected output. Let\u0027s look at an example to understand what that looks like:\r\n\r\n~~~text\r\nYou\u0027re a world-class chef, and you\u0027re preparing a meal. Provide a recipe for apple pie please. Output the result in JSON; use the following example as a guide.\r\n\r\n## Example\r\n\u0060\u0060\u0060\r\n{\r\n   \u0022title\u0022: \u0022The recipe title\u0022,\r\n   \u0022ingredients\u0022: [\r\n       { \u0022unit\u0022: \u0022tbsp\u0022, \u0022amount\u0022: 1, \u0022name\u0022: \u0022the name of the ingredient\u0022 },\r\n       { \u0022unit\u0022: \u0022grams\u0022, \u0022amount\u0022: 12, \u0022name\u0022: \u0022another ingredient\u0022 },\r\n   ],\r\n   \u0022instructions\u0022: \u0022The instructions to prepare the recipe\u0022\r\n}\r\n\u0060\u0060\u0060\r\n\r\n~~~\r\n\r\nWe\u0027ll get a completely different response from the previous sample when we run the prompt. The LLM will now generate a JSON object for your recipe.\r\n\r\n~~~text\r\nCertainly! Here\u0027s a recipe for a classic apple pie in JSON format:\r\n\u0060\u0060\u0060json\r\n{\r\n \\\u0022title\\\u0022: \\\u0022Classic Apple Pie\\\u0022,\r\n \\\u0022ingredients\\\u0022: [\r\n { \\\u0022unit\\\u0022: \\\u0022cups\\\u0022, \\\u0022amount\\\u0022: 2.5, \\\u0022name\\\u0022: \\\u0022all-purpose flour\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022granulated sugar\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022salt\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022cups\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022unsalted butter, cold and diced\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 6, \\\u0022name\\\u0022: \\\u0022ice water\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022lbs\\\u0022, \\\u0022amount\\\u0022: 2, \\\u0022name\\\u0022: \\\u0022apples\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022cups\\\u0022, \\\u0022amount\\\u0022: 0.75, \\\u0022name\\\u0022: \\\u0022granulated sugar\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 2, \\\u0022name\\\u0022: \\\u0022all-purpose flour\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022ground cinnamon\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022pinch\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022ground nutmeg\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022lemon juice\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tsp\\\u0022, \\\u0022amount\\\u0022: 1, \\\u0022name\\\u0022: \\\u0022vanilla extract\\\u0022 },\r\n { \\\u0022unit\\\u0022: \\\u0022tbsp\\\u0022, \\\u0022amount\\\u0022: 2, \\\u0022name\\\u0022: \\\u0022unsalted butter (for filling)\\\u0022 },",
        "Embedding": []
      },
      {
        "Id": 64,
        "OriginalFileName": "Content\\07-retrieval-augmented-generation.md",
        "Content": "To generate a response, you\u0027ll need first to retrieve relevant information. You can use the user\u0027s prompt to find relevant chunks in the database. One of the chunks you retrieve likely contains the answer to the question. But in many cases, the answer is spread across multiple chunks. So, it\u0027s essential to recover more than one chunk when performing a search and inject all chunks into the prompt.\r\n\r\nIt\u0027s also essential to make sure that chunks are coherent. For example, you don\u0027t want a chunk to contain half sentences or end with half a word. So, while splitting Content into chunks is a good solution, it\u0027s more complex than just slicing Content into pieces. In [#s](#end-to-end-rag-pipeline-implementation), we\u0027ll discuss how to implement chunking to ensure you end up with sensible chunks.\r\n\r\nBalancing the amount of chunks to retrieve, the size of the chunks, and general content quality will determine the success of your RAG implementation. The LLM you use to generate the final answer doesn\u0027t influence the quality of the answers as much as the retrieval approach you choose. If you\u0027re getting weird results from a RAG implementation, you\u0027re likely having issues with the data quality in the vector database or search engine.\r\n\r\nHaving said that, if you don\u0027t inject the retrieved information in the right spot, you\u0027ll still end up with bad-quality responses. So, it\u0027s essential to ensure you use the retrieved information correctly.\r\n\r\n### Generating responses\r\n\r\nIn general, the generation component of the RAG pattern is straightforward. You take the information retrieved earlier and inject it into the prompt to generate a response.\r\n\r\nHowever, depending on the solution you\u0027re building, you\u0027ll need a different approach to getting the information into the prompt.\r\n\r\nIn chatbot scenarios, you\u0027ll want to implement the RAG pattern as a tool and include the tool\u0027s output in the chat history as a tool response. Using a tool gives the LLM-based application the flexibility to provide a straight answer without considering extra Content from an external source.\r\n\r\nFor example, one of the chatbots I built can answer questions from a knowledge base about building software and generate text for marketing purposes. It would be strange to mix technical information about building software with general marketing content\u2014unless you\u0027re writing marketing content about building software. It\u0027s nice that by using a tool, we can consider marketing on its own or combine it with technical information.\r\n\r\nIn non-chat scenarios, you\u0027ll want to create a specialized prompt to help guide the LLM in the right direction. The trick here is to use in-context learning to help the LLM generate the correct response, as discussed earlier in [#s](#few-shot-learning). A typical prompt for answering a question looks like this:\r\n\r\n\u0060\u0060\u0060text\r\nYou\u0027re answering questions about washing machines for technical support.\r\nPlease use the information in the context section to answer the question.\r\nIf you don\u0027t know the answer, say so. Don\u0027t make up answers.\r\n\r\n## Context\r\n\r\n{{technical_documentation}}\r\n\r\n## Question\r\n\r\n{{question}}\r\n\u0060\u0060\u0060\r\n\r\nIn this prompt, you can inject the relevant chunks you found in the database and the user\u0027s question. The output of the LLM will be a response to the question with some extra information. The additional information is usually not a problem, but if it is, you can add one extra line: \u0060## Answer\u0060 that will help the LLM focus on just the answer.\r\n\r\nLet\u0027s look at how to implement the theory with Semantic Kernel by building an end-to-end pipeline.\r\n\r\n{#end-to-end-rag-pipeline-implementation}\r\n## Building an end-to-end RAG pipeline with Semantic Kernel",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "Are LLMs ineffective when combined with traditional programming logic?",
    "Answer": "No, LLMs are not inherently ineffective when combined with traditional programming logic. However, the book emphasizes that integrating LLMs into applications requires a structured and thoughtful approach. LLMs are most effective at solving clearly defined problems involving text pattern matching and transformation. They can complement traditional programming logic when used appropriately.\n\nThe context highlights that LLMs benefit from a software engineering mindset rather than being treated as a pure AI project. This means structuring the problem, providing specific inputs, and incorporating human oversight. Additionally, the book notes challenges like non-deterministic outputs and model drift, which require additional testing and monitoring strategies.\n\nSo, while LLMs can work well with traditional programming logic, their effectiveness depends on how they are integrated, tested, and monitored in the application.",
    "Context": [
      {
        "Id": 5,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "Once I saw what LLMs could do, I went overboard with ChatGPT. I used it for everything\r\ntext-related. I even messed up two blog posts on my website with negative feedback to\r\nshow for it. LLMs produce flavorless and pretty mediocre content. They\u0027re trained to\r\nrepresent the average of what language has to offer. And that\u0027s pretty average and\r\nflavorless.\r\n\r\nI used LLMs for coding, too, as I am a developer. I wrote a complete application using\r\nonly AI. And it\u0027s used in production today. But it was hard to get there. The LLM\r\nfrequently steered into the wall with weird layouts and useless unit tests. I haven\u0027t\r\nbothered measuring how quickly I built the application. I was quicker but less satisfied\r\nwith the result because writing great code is a skill I\u0027m proud of.\r\n\r\nAfter learning about open-source LLMs, I decided to try them, too. It was very slow,\r\neven on a beefy Intel Core i9 machine with a massive graphics card. I quickly learned\r\nthat you need a lot of power to run an LLM on your machine and in the cloud. And with a\r\nprice tag of 3500 euros for a decent machine, it\u0027s not something you want to do for a\r\nhobby project.\r\n\r\nThere are plenty more experiences where I found the boundaries of what LLMs can do, but\r\nlet me finish with one final example. I tried using an LLM to upgrade program code from\r\na low-code solution to Typescript without human supervision. We quickly had to add human\r\nsupervision because it wouldn\u0027t help us without that.\r\n\r\n### Key lessons learned\r\n\r\nYou might wonder, why am I telling you my experiences? There are three key lessons that\r\nI want you to keep in mind while reading this book:\r\n\r\n1. Be specific in what you ask from the LLM. Don\u0027t just ask for an article about LLMs; provide specific instructions.\r\n2. Always review and understand the output of the LLM. Don\u0027t let your users use the output of the LLM unseen. The output will be wrong in all the weird ways you\u0027ve never thought of.\r\n3. Break big problems down into smaller problems. Instead of asking the LLM to perform 10 steps, ask it for just one step. It will be easier for the LLM to perform and easier for you to debug.\r\n4. Keep track of the context and provide it in focused chunks. LLMs have limited input and output space, so they can\u0027t keep track of a complete book or even a blog post.\r\n\r\n### Evolution of my understanding\r\n\r\nAfter the initial rollercoaster ride with LLMs, my understanding of them evolved. I\r\nstopped seeing them as a silver bullet that could solve all my language-related problems\r\nand started seeing them as a powerful pattern-matching engine capable of transforming\r\ntext.\r\n\r\nOne crucial moment was realizing that LLMs excel at clearly defined tasks that involve\r\nmatching a pattern in the source text and transforming it into other text. If you can\r\nfind a clear pattern in the input and clearly define the target structure, an LLM is\r\nlikely a good solution to your problem. The less clear the problem statement is, the\r\nmore issues you\u0027ll experience.\r\n\r\nYou\u0027ll want to balance humans and machines well to build a practical LLM application.\r\nHuman oversight is essential when using an LLM. Throughout the rest of the book, you\r\nwill find that I\u0027m using interaction patterns that promote human oversight because it\u0027s\r\nnecessary and improves the experience significantly.\r\n\r\n### How I integrated LLMs into real projects\r\n\r\nClearly defined problems and human oversight are essential when you view an LLM-based\r\napplication from a functional perspective. From a technical standpoint, you must\r\nconsider applying LLMs as a software engineering problem with an AI aspect rather than a\r\npure AI project.\r\n\r\nHere are three reasons why you should use a software engineering approach:\r\n\r\n- LLMs will behave better when you follow a structured approach. The more structure, the better.\r\n- LLM behavior changes when providers push new versions of the models; automated testing is your friend.",
        "Embedding": []
      },
      {
        "Id": 14,
        "OriginalFileName": "Content\\02-essential-llmops-knowledge.md",
        "Content": "Let\u0027s dive into each of these aspects in more detail to understand what they mean for your LLM-based application starting with testing LLM-based applications.\r\n\r\n## Testing LLM-based applications\r\n\r\nTesting LLM-based applications is different from testing regular software applications.\r\nIn regular software applications, you can use tests that assert a true or false\r\nstatement. Typically, you\u0027ll write a combination of unit tests, integration tests, and\r\npossibly even acceptance tests to validate that your application is working as expected,\r\nusing a boolean approach to verify that some rules hold for your system.\r\n\r\nYou need to approach testing LLM-based applications differently from regular software.\r\nIt isn\u0027t as black and white. LLMs give you a different response every time because of\r\nwhat we discussed in [#s](#understanding-llms); they use output sampling techniques. You\r\nneed a different testing approach to validate that your LLM-based application is\r\nworking.\r\n\r\nWhere unit and integration tests should already be standard practice, you must apply one\r\nextra layer of tests for LLM-based applications. LLM-based applications need\r\ntests that verify the structure of the LLMs output.\r\n\r\nVerifying the structure of the LLM output is challenging because the output is non-deterministic. You won\u0027t get the same answer every time you run the test. So, instead of testing for hard results, you want the correct answer most of the time. We\u0027re trying to lower our solution\u0027s risk as much as possible. We must accept that, just like regular software, the system sometimes fails for unknown reasons. But we have a good idea of where the problem is coming from for an LLM-based application.\r\n\r\nOne testing strategy that works great in an LLM-based is running multiple samples\r\nthrough the application and then having the test cast a majority vote to determine if\r\nthe call to the LLM produces the correct output.\r\n\r\nAnother great strategy is to use metrics with a value range to determine that the output\r\nis within acceptable limits for your chosen metric.\r\n\r\nI can imagine that knowing just these rules for testing LLM-based applications generates\r\nmore questions than answers. For now, it\u0027s essential to understand that you need to\r\nchange your perspective on testing. In [#s](#prompt-testing-and-monitoring) we\u0027ll cover\r\nhow to test prompts in more detail.\r\n\r\nYou can consider testing as a way to ensure you have a safety net to catch problems\r\nbefore they happen in production. Monitoring is the second safety net you need to catch\r\nproblems while running in production.\r\n\r\n{#llmops-monitoring}\r\n## Monitoring and evaluation of your application\r\n\r\nThere are two essential aspects to monitoring LLM solutions. Unsurprisingly, you must\r\nmonitor for any infrastructure and technical problems with your code. This is more of a\r\nDevOps practice than an LLMOps practice.\r\n\r\nThe LLMOps approach to monitoring focuses on the interaction between your application\r\nand the LLM. Machine-learning models like LLMs tend to fail silently in production.\r\nPeople change their interaction patterns as the model is used, and the model starts to\r\ngenerate less accurate responses. This shift is called model drift or concept drift.\r\n\r\nWhen applying patterns like Retrieval Augmented Generation (RAG), tracking interactions\r\nbetween the LLM and external data sources is essential. As information changes in your\r\ndata sources, the LLM may generate less accurate responses, which you could call data\r\ndrift.\r\n\r\nWe\u0027ll cover monitoring in greater detail in [#s](#prompt-testing-and-monitoring).\r\n\r\nMonitoring interactions is essential to keeping your LLM-based application running smoothly in production. When setting up monitoring, I recommend including monitoring costs as well.\r\n\r\n## Cost management and optimization\r\n\r\nCost management is a big part of LLMOps. When you host an LLM-based application in the\r\ncloud, you can quickly run into high costs if you build just the right kind of logic so",
        "Embedding": []
      },
      {
        "Id": 17,
        "OriginalFileName": "Content\\02-essential-llmops-knowledge.md",
        "Content": "LLMs are. I could write an entire book about this topic, but I\u0027ll have to keep this\r\nshorter because we need to talk about other things. But let me give you the basics so\r\nyou know what you\u0027re getting yourself into.\r\n\r\nThere are three main topics when we talk about security in LLMs:\r\n\r\n- **Data privacy:** Customers and users will talk about data privacy because they\u0027re\r\nworried about the provider stealing their data to train the LLM.\r\n- **Application security:** This is about protecting your application from being hacked.\r\nThis is a big topic because LLMs are very powerful and can be used to generate malicious\r\ncontent.\r\n- **User safety:** This is about protecting your users from being exposed to harmful\r\ncontent generated by the LLM. LLMs can generate harmful content or misleading content\r\nthat can be used to manipulate people.\r\n\r\nLet me go over each of these topics to give you a direction on how to think about them.\r\n\r\n{#llmops-data-privacy} \r\n### Data privacy\r\n\r\nBefore you start using an LLM, you need to think about the data you\u0027re sending to it.\r\n\r\nThe [Samsung incident][SAMSUNG_INCIDENT] teaches us an important lesson: If your data is\r\ngoing to be used for training, it is likely to end up on someone else\u0027s desk at some\r\npoint in the future. Hackers are actively probing with prompts to find out what data is\r\nbeing used to train the LLM.\r\n\r\nSometimes, it\u0027s better not to use an LLM if you\u0027re worried about company secrets being\r\nleaked. For most cases, however, it\u0027s good enough to agree with the LLM provider and ask\r\nit not to use your data for training. All LLM Providers offer this option today and have\r\na section about it in their legal agreements.\r\n\r\nIt is essential to understand that security is a trade-off. You can\u0027t have 100% security\r\nbecause then you wouldn\u0027t use LLMs in the first place and miss out on their value.\r\nHowever, you can\u0027t wholly live without security, either. Have an active discussion with\r\nyour client about the data you\u0027re sending to the LLM and the risks involved, and then\r\ndecide based on that.\r\n\r\nThere\u0027s another closely related topic to data privacy that I want to address here. In\r\nincreasingly increasing countries, some laws require you to be careful with personal\r\nidentifiable information (PII). For example, in Europe, you need to comply with GDPR. In\r\nCalifornia, you need to comply with CCPA. These laws require you to be careful with the\r\ndata you\u0027re processing.\r\n\r\nManaging personal data is a topic you should address alongside the other security\r\nrequirements when planning an LLM-based application.\r\n\r\nMany cloud providers offer standard tools to filter out PII from text. For example,\r\nAzure has a tool called [Text Analytics][PII_FILTER] that can filter out PII from text.\r\nThis way, you can avoid sending PII to the LLM or storing it in your application.\r\nConsider it an extra safety measure on top of ensuring people know they\u0027re not supposed\r\nto send in PII.\r\n\r\n{#llmops-application-security} \r\n### Application Security\r\n\r\nSecuring modern cloud-native applications is challenging. Adding an LLM to the mix opens\r\nup a new chapter in application security. And it\u0027s a relatively new chapter, so there\u0027s\r\nnot much information on securing an LLM-based application. I will cover some LLM\r\nsecurity strategies around security in greater detail in the following chapters, but I\r\nwant to make sure you have a good overview of the topic first.\r\n\r\nApplying a deep defense strategy is the best way to handle application security. You\u0027ll\r\nwant to use multiple layers of defenses to ensure that if a hacker gets through one\r\nlayer, it will have to work hard to get through the next. A layered defense also helps\r\nkeep an attack\u0027s blast radius to a minimum.\r\n\r\nOne of the best moves you can make in terms of application security is not giving your\r\napplication access to resources it shouldn\u0027t have access to. I see many people using\r\nCopilot 365 and oversharing information because of bad defaults and the admin not being",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "Does the retrieval component solely analyze washing machine manuals?",
    "Answer": "No, the retrieval component in the RAG pattern does not solely analyze washing machine manuals. The example of a washing machine manual is just a practical use case to explain how the retrieval component works. The retrieval component is designed to process and retrieve relevant information from any set of documents or data, depending on the application\u0027s use case. It can handle a wide variety of content beyond washing machine manuals.",
    "Context": [
      {
        "Id": 62,
        "OriginalFileName": "Content\\07-retrieval-augmented-generation.md",
        "Content": "{#retrieval-augmented-generation}\r\n# Retrieval augmented generation\r\n\r\nIn previous chapters, we focused on learning the basic building blocks of an LLM-based application. We covered the basics of LLMs, prompting, testing, and using tools. Now, it\u0027s time to move on to more advanced topics. In this chapter and the ones after it, we\u0027ll focus on common patterns you can use to build more elaborate LLM-based applications. We\u0027ll use the knowledge learned in previous chapters to help implement the patterns.\r\n\r\nIn this chapter, you\u0027ll learn how to use Retrieval Augmented Generation to answer questions based on internal knowledge that has not previously been trained in the LLM. You\u0027ll learn how to index documents for retrieval and use indexed Content in the LLMs response.\r\n\r\nWe\u0027ll cover the following topics:\r\n\r\n- What is Retrieval Augmented Generation (RAG)\r\n- Building an end-to-end RAG pipeline with Semantic Kernel\r\n- A practical approach to validating the RAG pipeline\r\n- Optimizing retrieval for RAG\r\n- Variations on the RAG pattern\r\n\r\nLet\u0027s start by discussing Retrieval Augmented Generation (RAG) and learning what components are involved in this pattern.\r\n\r\n## What is Retrieval Augmented Generation (RAG)\r\n\r\nRetrieval Augmented Generation (RAG) is a pattern in which you use information from an external source as extra context information when generating a response to a prompt. \r\n\r\nThe RAG pattern is a form of in-context learning we discussed in [#s](#few-shot-learning). You can use RAG for many scenarios. But you\u0027re most likely using RAG to answer users\u0027 questions. [#s](#rag-pattern-architecture) shows the structure of the RAG pattern.\r\n\r\n{#rag-pattern-architecture}\r\n![RAG pattern architecture](rag-pattern-architecture.png)\r\n\r\nThe RAG pattern has two main components:\r\n\r\n1. the retrieval portion is responsible for finding relevant information based on the entered prompt.\r\n2. Next, there\u0027s the generation portion, responsible for generating the final response to the prompt.\r\n\r\nThe basics of the RAG pattern aren\u0027t overly involved, but you can find many variants on the Internet. That\u0027s the beauty of the pattern: You can adapt and extend it to your needs.\r\n\r\nLet\u0027s examine the retrieval component of the RAG pattern more closely to understand how to retrieve information to answer questions.\r\n\r\n{#retrieval-component-architecture}\r\n### Retrieval component architecture\r\n\r\nThe retrieval component of the RAG pattern is usually made out of two subcomponents. You need a method to process information into a format that\u0027s easy for an LLM to answer questions with, and you need a method for the application to retrieve the preprocessed data. [#s](#retrieval-architecture) shows the details of the retrieval portion of the RAG pattern.\r\n\r\n{#retrieval-architecture}\r\n![Retrieval component of the RAG pattern](retrieval-architecture.png)\r\n\r\nLet\u0027s explore the retrieval component of the RAG pattern with a practical use case. Imagine you must explain how to disassemble a washing machine to a mechanic. You could give the whole manual to the mechanic and let them read it. But that\u0027s not efficient because it takes quite a long time. It\u0027s nicer when the mechanic can ask, \u0022How do I remove the electrical board from the front of the Miele W1 washing machine?\u0022 and the LLM answers with specific instructions on how to do that, but nothing more.\r\n\r\nWe could give the LLM the whole washing machine manual and let it determine the answer. But that doesn\u0027t work very well; it might be even worse than the mechanic reading the manual. First, the manual might not fit in the LLM\u0027s context window. Next, the LLM might be unable to grab the correct information because of the limitations in what it can focus on within its context window, as we discussed in [#s](#context-window-limits).",
        "Embedding": []
      },
      {
        "Id": 63,
        "OriginalFileName": "Content\\07-retrieval-augmented-generation.md",
        "Content": "If you want a clear answer, providing the LLM with more focused information that matches the question as closely as possible is essential. And this is what you should solve in the retrieval portion of the RAG pattern.\r\n\r\nIdeally, you want to preprocess the washing machine manual to end up with chunks of information that clearly describe how to perform a specific task. That way, the LLM can almost copy-paste the information into the response. Remember that the LLM can only match patterns and generate tokens based on the pattern.\r\n\r\nPreprocessing content optimally takes a lot of effort. If you want perfection, you will need an army of people who enter the information by hand. That\u0027s not feasible for most projects, so you\u0027ll have to settle for a less ideal solution. But that\u0027s not a massive problem because, in many cases, you can get away with a less optimal solution.\r\n\r\nTo preprocess content for retrieval, you can start by splitting the text in the input documents into chunks of reasonable size. For example, you can create chunks of around 1000 tokens each. You can then store those chunks in a suitable database for retrieval.\r\n\r\n### Storing information for retrieval\r\n\r\nThere are many options for storing the extracted content chunks. For example, you could store the searchable content using a regular search engine. Elastic Search is one of the most widely used search engines and does an excellent job finding documents based on a query.\r\n\r\nRegular search engines will preprocess the chunks into a collection of search terms. They remove stopwords, lowercase the content, and split long words to increase the likelihood of a match during the search. When you send a query, it also gets preprocessed into search terms and matches those terms against the documents in the database. To find a document, one or more search terms must match the text in the document. If you use a synonym or make a spelling mistake, the search match will be less precise.\r\n\r\nThat\u0027s why most people will use a vector database to store chunked Content: it provides better search capabilities than regular search engines.\r\n\r\nA vector database stores chunks based on the embedding vector representations for the chunks. To store content in a vector database for search, you first translate the content into an embedding vector representing the semantic meaning of the content using an embedding model. After creating the embedding vector, you can store it with the original content and metadata in the vector database. When you want to search, you send an embedding vector made with the same embedding representing your query to the vector database and match it against the embedding vectors of the documents in the database. The vector database returns the documents with embedding vectors closest to the query embedding vector. Vector search uses cosine similarity to determine how close vectors are to each other. Imagine an arrow for the document and another for the query. The cosine similarity is the angle between the two arrows. [#s](#cosine-similarity) demonstrates this principle.\r\n\r\n{#cosine-similarity}\r\n![Cosine similarity](cosine-similarity.png)\r\n\r\nThe advantage of the vector-based search method is that you can make spelling mistakes, use synonyms, or even use different words to describe the same thing. As long as the text in the document is semantically similar to the text in the query, you will find it because the embedding vectors will be closely related.\r\n\r\n### Retrieving relevant information\r\n\r\nStoring chunked content in a vector database is a good start for most use cases and will get you a long way. However, there are a few details that we need to discuss when retrieving information from a vector database.",
        "Embedding": []
      },
      {
        "Id": 71,
        "OriginalFileName": "Content\\07-retrieval-augmented-generation.md",
        "Content": "new TextUnitStringMapper(),\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 new TextUnitTextSearchResultMapper());\r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 var searchFunction = textSearch.CreateGetTextSearchResults();\r\n\r\n        kernel.Plugins.AddFromFunctions(\u0022SearchPlugin\u0022, [searchFunction]);\r\n\u00A0 \r\n\u00A0 \u00A0 \u00A0 \u00A0 var citationsFilter = new CitationCapturingFilter();\r\n            kernel.FunctionInvocationFilters.Add(citationsFilter);\r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 // ... Rest of the code\r\n\u00A0 \u00A0 }\r\n}\r\n\u0060\u0060\u0060\r\n\r\nLet\u0027s go over the code to understand the modifications compared to the original\r\nversion of this code: The start of the method is the same as before, but we\u0027ve added a new filter right after configuring the text search plugin.\r\n\r\nIn previous code fragments, we were already using the \u0060TextUnitTextSearchResultMapper\u0060. Let\u0027s discuss what this mapper does because it\u0027s necessary here to make the function filter work. The class looks like this:\r\n\r\n\u0060\u0060\u0060csharp\r\npublic class TextUnitTextSearchResultMapper : ITextSearchResultMapper\r\n{\r\n\u00A0 \u00A0 public TextSearchResult MapFromResultToTextSearchResult(object result)\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 if (result is TextUnit textUnit)\r\n\u00A0 \u00A0 \u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 return new TextSearchResult(value: textUnit.Content)\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 {\r\n                Link = textUnit.OriginalFileName,\r\n                Name = textUnit.Id.ToString()\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 };\r\n\u00A0 \u00A0 \u00A0 \u00A0 }\r\n\r\n\u00A0 \u00A0 \u00A0 \u00A0 throw new ArgumentException(\u0022Invalid result object\u0022);\r\n\u00A0 \u00A0 }\r\n}\r\n\u0060\u0060\u0060\r\n\r\nIn this class, we perform the following steps:\r\n\r\n1. First, we implement the \u0060ISearchResultMapper\u0060 interface, creating the \u0060MapFromResultToTextSearchResult\u0060 method.\r\n2. Then, in the method, we check if the result is a \u0060TextUnit\u0060 instance.\r\n3. Next, we create a new \u0060TextSearchResult\u0060 instance with the Content of the text unit, including the search result\u0027s identifier, link, and value.\r\n\r\nWith the additional filter, you can be sure that you\u0027re capturing the search results found by the vector search instead of relying on the LLM to choose whether something was used in the response. However, you may still get some false positives because the LLM may not actually use a result that you retrieved.\r\n\r\nImplementing a RAG pattern takes effort to get right, and it will not always be 100% perfect. You have to make a choice here: Do you want to have a response where the LLM makes up sources? Or do you want to have a response with a separate set of citations that may not be included in the actual response?\r\n\r\nWhatever you choose, I recommend spending some time to establish a good test strategy for the various parts of your RAG implementation to ensure the highest possible quality. In the next section, we will discuss how to approach testing the RAG pattern.\r\n\r\n## A practical approach to validating the RAG pipeline\r\n\r\nTesting the RAG pattern can be quite complicated. [#s](#rag-evaluation-controls) shows the various aspects of the RAG pattern we discussed and the quality controls that are available to validate them. It\u0027s a lot to get through.\r\n\r\n{#rag-evaluation-controls}\r\n![RAG pattern evaluation controls](rag-evaluation-controls.png)\r\n\r\nWe could go through this step-by-step explaining all the small details of how to validate each component in the RAG pipeline on its own. But I\u0027ve found this to be very unpractical. Every piece in the pipeline interacts with other components and you\u0027ll likely influence one component by changing another.\r\n\r\nLet me help you make sense of RAG pipeline validation by talking you through a practical approach. We\u0027ll use the RAG pipeline from the previous sections based on the content of this book as input for the validation.\r\n\r\nLet\u0027s get started with an overview of the validation process.\r\n\r\n### Overview of the validation process\r\n\r\nWhen testing a RAG pipeline I like working with one or two basic metrics to measure performance. For a RAG pipeline it\u0027s important that the final answer given by the LLM makes sense using as content from the source documents as faithfully as possible.",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "What mindset has been crucial for building practical LLM systems?",
    "Answer": "The crucial mindset for building practical LLM systems involves balancing humans and machines effectively. Specifically:\n\n1. **Clearly Define Problems**: LLMs excel at tasks where there is a clear pattern in the input and a well-defined target structure. The less clear the problem statement, the more issues arise.\n2. **Human Oversight**: Human oversight is essential to ensure the quality and safety of the LLM\u0027s output. Interaction patterns that promote human oversight significantly improve the overall experience.\n3. **Software Engineering Approach**: Treat the development of LLM-based applications as a software engineering problem with an AI aspect, rather than as a pure AI project. This includes:\n   - Following a structured approach for better LLM behavior.\n   - Incorporating automated testing to handle changes in LLM behavior when providers update models.\n\nThis mindset ensures the systems are practical, functional, and maintainable.",
    "Context": [
      {
        "Id": 17,
        "OriginalFileName": "Content\\02-essential-llmops-knowledge.md",
        "Content": "LLMs are. I could write an entire book about this topic, but I\u0027ll have to keep this\r\nshorter because we need to talk about other things. But let me give you the basics so\r\nyou know what you\u0027re getting yourself into.\r\n\r\nThere are three main topics when we talk about security in LLMs:\r\n\r\n- **Data privacy:** Customers and users will talk about data privacy because they\u0027re\r\nworried about the provider stealing their data to train the LLM.\r\n- **Application security:** This is about protecting your application from being hacked.\r\nThis is a big topic because LLMs are very powerful and can be used to generate malicious\r\ncontent.\r\n- **User safety:** This is about protecting your users from being exposed to harmful\r\ncontent generated by the LLM. LLMs can generate harmful content or misleading content\r\nthat can be used to manipulate people.\r\n\r\nLet me go over each of these topics to give you a direction on how to think about them.\r\n\r\n{#llmops-data-privacy} \r\n### Data privacy\r\n\r\nBefore you start using an LLM, you need to think about the data you\u0027re sending to it.\r\n\r\nThe [Samsung incident][SAMSUNG_INCIDENT] teaches us an important lesson: If your data is\r\ngoing to be used for training, it is likely to end up on someone else\u0027s desk at some\r\npoint in the future. Hackers are actively probing with prompts to find out what data is\r\nbeing used to train the LLM.\r\n\r\nSometimes, it\u0027s better not to use an LLM if you\u0027re worried about company secrets being\r\nleaked. For most cases, however, it\u0027s good enough to agree with the LLM provider and ask\r\nit not to use your data for training. All LLM Providers offer this option today and have\r\na section about it in their legal agreements.\r\n\r\nIt is essential to understand that security is a trade-off. You can\u0027t have 100% security\r\nbecause then you wouldn\u0027t use LLMs in the first place and miss out on their value.\r\nHowever, you can\u0027t wholly live without security, either. Have an active discussion with\r\nyour client about the data you\u0027re sending to the LLM and the risks involved, and then\r\ndecide based on that.\r\n\r\nThere\u0027s another closely related topic to data privacy that I want to address here. In\r\nincreasingly increasing countries, some laws require you to be careful with personal\r\nidentifiable information (PII). For example, in Europe, you need to comply with GDPR. In\r\nCalifornia, you need to comply with CCPA. These laws require you to be careful with the\r\ndata you\u0027re processing.\r\n\r\nManaging personal data is a topic you should address alongside the other security\r\nrequirements when planning an LLM-based application.\r\n\r\nMany cloud providers offer standard tools to filter out PII from text. For example,\r\nAzure has a tool called [Text Analytics][PII_FILTER] that can filter out PII from text.\r\nThis way, you can avoid sending PII to the LLM or storing it in your application.\r\nConsider it an extra safety measure on top of ensuring people know they\u0027re not supposed\r\nto send in PII.\r\n\r\n{#llmops-application-security} \r\n### Application Security\r\n\r\nSecuring modern cloud-native applications is challenging. Adding an LLM to the mix opens\r\nup a new chapter in application security. And it\u0027s a relatively new chapter, so there\u0027s\r\nnot much information on securing an LLM-based application. I will cover some LLM\r\nsecurity strategies around security in greater detail in the following chapters, but I\r\nwant to make sure you have a good overview of the topic first.\r\n\r\nApplying a deep defense strategy is the best way to handle application security. You\u0027ll\r\nwant to use multiple layers of defenses to ensure that if a hacker gets through one\r\nlayer, it will have to work hard to get through the next. A layered defense also helps\r\nkeep an attack\u0027s blast radius to a minimum.\r\n\r\nOne of the best moves you can make in terms of application security is not giving your\r\napplication access to resources it shouldn\u0027t have access to. I see many people using\r\nCopilot 365 and oversharing information because of bad defaults and the admin not being",
        "Embedding": []
      },
      {
        "Id": 5,
        "OriginalFileName": "Content\\01-understanding-large-language-models.md",
        "Content": "Once I saw what LLMs could do, I went overboard with ChatGPT. I used it for everything\r\ntext-related. I even messed up two blog posts on my website with negative feedback to\r\nshow for it. LLMs produce flavorless and pretty mediocre content. They\u0027re trained to\r\nrepresent the average of what language has to offer. And that\u0027s pretty average and\r\nflavorless.\r\n\r\nI used LLMs for coding, too, as I am a developer. I wrote a complete application using\r\nonly AI. And it\u0027s used in production today. But it was hard to get there. The LLM\r\nfrequently steered into the wall with weird layouts and useless unit tests. I haven\u0027t\r\nbothered measuring how quickly I built the application. I was quicker but less satisfied\r\nwith the result because writing great code is a skill I\u0027m proud of.\r\n\r\nAfter learning about open-source LLMs, I decided to try them, too. It was very slow,\r\neven on a beefy Intel Core i9 machine with a massive graphics card. I quickly learned\r\nthat you need a lot of power to run an LLM on your machine and in the cloud. And with a\r\nprice tag of 3500 euros for a decent machine, it\u0027s not something you want to do for a\r\nhobby project.\r\n\r\nThere are plenty more experiences where I found the boundaries of what LLMs can do, but\r\nlet me finish with one final example. I tried using an LLM to upgrade program code from\r\na low-code solution to Typescript without human supervision. We quickly had to add human\r\nsupervision because it wouldn\u0027t help us without that.\r\n\r\n### Key lessons learned\r\n\r\nYou might wonder, why am I telling you my experiences? There are three key lessons that\r\nI want you to keep in mind while reading this book:\r\n\r\n1. Be specific in what you ask from the LLM. Don\u0027t just ask for an article about LLMs; provide specific instructions.\r\n2. Always review and understand the output of the LLM. Don\u0027t let your users use the output of the LLM unseen. The output will be wrong in all the weird ways you\u0027ve never thought of.\r\n3. Break big problems down into smaller problems. Instead of asking the LLM to perform 10 steps, ask it for just one step. It will be easier for the LLM to perform and easier for you to debug.\r\n4. Keep track of the context and provide it in focused chunks. LLMs have limited input and output space, so they can\u0027t keep track of a complete book or even a blog post.\r\n\r\n### Evolution of my understanding\r\n\r\nAfter the initial rollercoaster ride with LLMs, my understanding of them evolved. I\r\nstopped seeing them as a silver bullet that could solve all my language-related problems\r\nand started seeing them as a powerful pattern-matching engine capable of transforming\r\ntext.\r\n\r\nOne crucial moment was realizing that LLMs excel at clearly defined tasks that involve\r\nmatching a pattern in the source text and transforming it into other text. If you can\r\nfind a clear pattern in the input and clearly define the target structure, an LLM is\r\nlikely a good solution to your problem. The less clear the problem statement is, the\r\nmore issues you\u0027ll experience.\r\n\r\nYou\u0027ll want to balance humans and machines well to build a practical LLM application.\r\nHuman oversight is essential when using an LLM. Throughout the rest of the book, you\r\nwill find that I\u0027m using interaction patterns that promote human oversight because it\u0027s\r\nnecessary and improves the experience significantly.\r\n\r\n### How I integrated LLMs into real projects\r\n\r\nClearly defined problems and human oversight are essential when you view an LLM-based\r\napplication from a functional perspective. From a technical standpoint, you must\r\nconsider applying LLMs as a software engineering problem with an AI aspect rather than a\r\npure AI project.\r\n\r\nHere are three reasons why you should use a software engineering approach:\r\n\r\n- LLMs will behave better when you follow a structured approach. The more structure, the better.\r\n- LLM behavior changes when providers push new versions of the models; automated testing is your friend.",
        "Embedding": []
      },
      {
        "Id": 14,
        "OriginalFileName": "Content\\02-essential-llmops-knowledge.md",
        "Content": "Let\u0027s dive into each of these aspects in more detail to understand what they mean for your LLM-based application starting with testing LLM-based applications.\r\n\r\n## Testing LLM-based applications\r\n\r\nTesting LLM-based applications is different from testing regular software applications.\r\nIn regular software applications, you can use tests that assert a true or false\r\nstatement. Typically, you\u0027ll write a combination of unit tests, integration tests, and\r\npossibly even acceptance tests to validate that your application is working as expected,\r\nusing a boolean approach to verify that some rules hold for your system.\r\n\r\nYou need to approach testing LLM-based applications differently from regular software.\r\nIt isn\u0027t as black and white. LLMs give you a different response every time because of\r\nwhat we discussed in [#s](#understanding-llms); they use output sampling techniques. You\r\nneed a different testing approach to validate that your LLM-based application is\r\nworking.\r\n\r\nWhere unit and integration tests should already be standard practice, you must apply one\r\nextra layer of tests for LLM-based applications. LLM-based applications need\r\ntests that verify the structure of the LLMs output.\r\n\r\nVerifying the structure of the LLM output is challenging because the output is non-deterministic. You won\u0027t get the same answer every time you run the test. So, instead of testing for hard results, you want the correct answer most of the time. We\u0027re trying to lower our solution\u0027s risk as much as possible. We must accept that, just like regular software, the system sometimes fails for unknown reasons. But we have a good idea of where the problem is coming from for an LLM-based application.\r\n\r\nOne testing strategy that works great in an LLM-based is running multiple samples\r\nthrough the application and then having the test cast a majority vote to determine if\r\nthe call to the LLM produces the correct output.\r\n\r\nAnother great strategy is to use metrics with a value range to determine that the output\r\nis within acceptable limits for your chosen metric.\r\n\r\nI can imagine that knowing just these rules for testing LLM-based applications generates\r\nmore questions than answers. For now, it\u0027s essential to understand that you need to\r\nchange your perspective on testing. In [#s](#prompt-testing-and-monitoring) we\u0027ll cover\r\nhow to test prompts in more detail.\r\n\r\nYou can consider testing as a way to ensure you have a safety net to catch problems\r\nbefore they happen in production. Monitoring is the second safety net you need to catch\r\nproblems while running in production.\r\n\r\n{#llmops-monitoring}\r\n## Monitoring and evaluation of your application\r\n\r\nThere are two essential aspects to monitoring LLM solutions. Unsurprisingly, you must\r\nmonitor for any infrastructure and technical problems with your code. This is more of a\r\nDevOps practice than an LLMOps practice.\r\n\r\nThe LLMOps approach to monitoring focuses on the interaction between your application\r\nand the LLM. Machine-learning models like LLMs tend to fail silently in production.\r\nPeople change their interaction patterns as the model is used, and the model starts to\r\ngenerate less accurate responses. This shift is called model drift or concept drift.\r\n\r\nWhen applying patterns like Retrieval Augmented Generation (RAG), tracking interactions\r\nbetween the LLM and external data sources is essential. As information changes in your\r\ndata sources, the LLM may generate less accurate responses, which you could call data\r\ndrift.\r\n\r\nWe\u0027ll cover monitoring in greater detail in [#s](#prompt-testing-and-monitoring).\r\n\r\nMonitoring interactions is essential to keeping your LLM-based application running smoothly in production. When setting up monitoring, I recommend including monitoring costs as well.\r\n\r\n## Cost management and optimization\r\n\r\nCost management is a big part of LLMOps. When you host an LLM-based application in the\r\ncloud, you can quickly run into high costs if you build just the right kind of logic so",
        "Embedding": []
      }
    ]
  },
  {
    "Question": "What does the \u0060[Theory]\u0060 attribute signify in the test method?",
    "Answer": "The \u0060[Theory]\u0060 attribute in the test method signifies that the method is a parameterized test in **xUnit**. It indicates that the test method will be executed multiple times, each with different sets of input data. The inputs for each test execution are provided using attributes like \u0060[InlineData]\u0060 or other data sources.\n\nIn the context of the provided code, the \u0060[Theory]\u0060 attribute marks the test method \u0060TestGenerateRecipe_ContainsInstructions\u0060 as a parameterized test. For example:\n\n\u0060\u0060\u0060csharp\n[Theory]\n[InlineData(\u0022pizza\u0022, new string[] { \u0022spinach\u0022, \u0022mozzarella\u0022 })]\n\u0060\u0060\u0060\n\nThis means the test will run with the inputs \u0060\u0022pizza\u0022\u0060 as the dish and \u0060[\u0022spinach\u0022, \u0022mozzarella\u0022]\u0060 as the ingredients. You can add more \u0060[InlineData]\u0060 entries to test the same method with different inputs. This approach is particularly useful for testing the same logic across multiple scenarios without duplicating the test code.\n\nIn summary, \u0060[Theory]\u0060 allows for data-driven testing, where the test logic stays the same, but the data varies across test runs.",
    "Context": [
      {
        "Id": 42,
        "OriginalFileName": "Content\\05-testing-and-monitoring-prompts.md",
        "Content": "public RecipeGenerationTests()\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 var configuration = new ConfigurationBuilder()\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 .AddUserSecrets\u003CRecipeGenerationTests\u003E()\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 .Build();\r\n\r\n        _kernel = Kernel.CreateBuilder()\r\n            .AddAzureOpenAIChatCompletion(\r\n                deploymentName: configuration[\u0022LanguageModel:DeploymentName\u0022]!,\r\n                endpoint: configuration[\u0022LanguageModel:Endpoint\u0022]!,\r\n                apiKey: configuration[\u0022LanguageModel:ApiKey\u0022]!\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 ).Build();\r\n\r\n        _prompt = _kernel.CreateFunctionFromPromptYaml(\r\n            EmbeddedResource.Read(\u0022prompt.yaml\u0022), \r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 new HandlebarsPromptTemplateFactory()\r\n\u00A0 \u00A0 \u00A0 \u00A0 );\r\n\u00A0 \u00A0 }\r\n\r\n\u00A0 \u00A0 [Theory]\r\n\u00A0 \u00A0 public async Task TestGenerateRecipe_ContainsInstructions(\r\n\u00A0 \u00A0 \u00A0 \u00A0 string dish, string[] ingredients)\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 // The content of your test.\r\n\u00A0 \u00A0 }\r\n}\r\n\u0060\u0060\u0060\r\n\r\nThis code performs the following steps:\r\n\r\n- First, we define a test class with two variables, \u0060_kernel\u0060 to store the kernel\r\ninstance used in the test, and \u0060_prompt\u0060 to store the prompt function that we\u0027re testing.\r\n- Next, we create a new constructor for the test class that initializes the kernel and the prompt function. We\u0027re using \u0060Microsoft.Extensions.Configuration\u0060 to obtain settings from the user-secrets store. This allows us to keep the API key and other sensitive information from the codebase.\r\n- Finally, we define a test method that inputs a dish name and a list of ingredients. We marked the test method as \u0060[Theory]\u0060 to enable parameterized testing. The test method will be called for each set of input data.\r\n\r\nWith the skeleton for the test in place, we can write the test logic to validate the prompt. The following code shows how to write the test logic:\r\n\r\n\u0060\u0060\u0060csharp\r\n[Theory]\r\n[InlineData(\u0022pizza\u0022, new string[] { \u0022spinach\u0022, \u0022mozzarella\u0022 })]\r\npublic async Task TestGenerateRecipe_ContainsInstructions(\r\n\u00A0 \u00A0 string dish, string[] ingredients)\r\n{\r\n\u00A0 \u00A0 var result = await _prompt.InvokeAsync(_kernel, new KernelArguments\r\n\u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 [\u0022dish\u0022] = dish,\r\n\u00A0 \u00A0 \u00A0 \u00A0 [\u0022ingredients\u0022] = ingredients\r\n\u00A0 \u00A0 });\r\n\r\n    Assert.Contains(\u0022ingredients\u0022, result.ToString().ToLower());\r\n    Assert.Contains(\u0022instructions\u0022, result.ToString().ToLower());\r\n}\r\n\u0060\u0060\u0060\r\n\r\nThe test performs the following steps:\r\n\r\n1. First, we marked the test method with \u0060[InlineData(...)]\u0060 to specify a sample for which we want to run the test. You can use inline data or load data from a test file in your code base. You can learn more about the various methods to load test samples for your data-driven test in [this blog post][XUNIT_DATA_DRIVEN_TESTS].\r\n2. Next, we call the prompt function with the dish name and the list of ingredients as\r\ninput. The result is stored in the \u0060result\u0060 variable.\r\n3. Finally, we use the \u0060Assert.Contains\u0060 method to check if the result contains the\r\nwords *ingredients* and *instructions*. If the result includes these words, the test passes.\r\n\r\nYou can extend this test with more samples as you see fit. The test will run for each provided sample, and separate test results will be reported. You can find the complete source code for the test on [Github][PROMPT_TEST_SAMPLE].\r\n\r\nIt may come as no surprise that running more samples will slow the test, which isn\u0027t fast, to begin with. I highly recommend you mark the test with a separate category and only run the LLM-based tests when you\u0027re about to finish up a new user story in your application or when you change the prompt, the LLM version, or the model configuration.\r\n\r\nYou can mark the test with a category by adding the following attribute to the test method:\r\n\r\n\u0060\u0060\u0060csharp\r\n[Trait(\u0022Category\u0022, \u0022LLM\u0022)]\r\n\u0060\u0060\u0060\r\n\r\nWith this attribute in place, you can filter the tests in the test runner by category. For example, if you use \u0060dotnet test\u0060 to run your tests, you can filter the tests by using the following command:\r\n\r\n\u0060\u0060\u0060bash\r\ndotnet test --filter Category!=LLM\r\n\u0060\u0060\u0060",
        "Embedding": []
      },
      {
        "Id": 44,
        "OriginalFileName": "Content\\05-testing-and-monitoring-prompts.md",
        "Content": "4. Finally, we steer the LLM to output just the score by giving it the start of its output.\r\n\r\nIf you leave the final portion out, the model will yap a lot, and it will be super happy to help you score the output. But that\u0027s not what you need at all. Putting in hints like \u0022Score:\u0022 will help the model steer clear of the yapping and get to the point.\r\n\r\nYou can run this prompt inside a unit test setup with, for example, xunit. The following code demonstrates this:\r\n\r\n\u0060\u0060\u0060csharp\r\n[Fact]\r\npublic async Task TestGenerateRecipe_GeneratesConsistentResults()\r\n{\r\n\u00A0 \u00A0 var result = await _prompt.InvokeAsync(_kernel, \r\n\u00A0 \u00A0 \u00A0 \u00A0 new KernelArguments\r\n\u00A0 \u00A0 \u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 [\u0022dish\u0022] = \u0022pizza\u0022,\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 [\u0022ingredients\u0022] = new string[] \r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 { \r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u0022spinach\u0022, \u0022mozzarella\u0022 \r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 }\r\n\u00A0 \u00A0 \u00A0 \u00A0 });\r\n\r\n\u00A0 \u00A0 var testExecutionSettings = new AzureOpenAIPromptExecutionSettings\r\n\u00A0 \u00A0 {\r\n        ResponseFormat = typeof(EvaluationResult)\r\n\u00A0 \u00A0 };\r\n\r\n\u00A0 \u00A0 var testResult = await _testPrompt.InvokeAsync(_kernel,\r\n\u00A0 \u00A0 \u00A0 \u00A0 new KernelArguments(testExecutionSettings)\r\n\u00A0 \u00A0 \u00A0 \u00A0 {\r\n\u00A0 \u00A0 \u00A0 \u00A0 \u00A0 \u00A0 [\u0022recipe\u0022] = result.ToString()\r\n\u00A0 \u00A0 \u00A0 \u00A0 });\r\n\r\n\u00A0 \u00A0 var evaluationResult = JsonSerializer.Deserialize\u003CEvaluationResult\u003E(\r\n        testResult.ToString())!;\r\n\r\n    Assert.InRange(evaluationResult.Score, 3, 5);\r\n}\r\n\u0060\u0060\u0060\r\n\r\nThe test performs the following steps:\r\n\r\n1. First, we generate a response for the prompt we\u0027re testing. To keep things simple, we use the pizza input from the previous code samples.\r\n2. Next, we take the prompt\u0027s output and put it into the test prompt. We\u0027re using a specific set of execution settings to ensure we\u0027re generating structured output. We haven\u0027t covered this yet, but the \u0060ResponseFormat\u0060 setting tells the LLM we want a structured JSON response with a \u0060Score\u0060 property.\r\n3. Finally, we deserialize the response to JSON and check if the score is between 3 and 5 as a simple check to see if the response is consistent.\r\n\r\nYou can expand this test case with more samples. But unlike the sample in [#s](#prompt-testing-basics), you can\u0027t use the data-driven approach as xunit tests per sample rather than the whole collection. Instead, you\u0027ll want to load a set of samples from CSV or another file format and run them all through the model, collecting the results in a list. Then, you can calculate the average score and check if it\u0027s within a specific range.\r\n\r\nDespite my efforts to reduce the yapping, the model sometimes produces somewhat irrelevant output. You can\u0027t avoid this unless you set the \u0060ResponseFormat\u0060 in the execution settings. We\u0027ll cover structured output in greater detail in chapter 7.\r\n\r\nYou can try the code yourself. The full source code for this sample is on [Github][MODEL_BASED_TEST_SAMPLE].\r\n\r\nTesting LLM output with LLMs looks very powerful, and it is. But before you go wild with this approach, there\u0027s a warning that I need to leave you with.\r\n\r\n## The dangers of the model-based testing approach\r\n\r\nYou should be aware that model-based testing comes with a huge disclaimer. I can\u0027t stress this enough: you\u0027re dealing with a token-predicting pattern-matching machine. So, the scores you\u0027ll see are not actual scores but tokens in a sequence. It\u0027s misleading to present this as a proper evaluation. But still, I\u0027m writing about it because testing with this approach does bring some value.\r\n\r\nWhen you ask a human to evaluate a response and score it on a scale from 1 to 5, you get reasonable-sounding results until you ask the human to explain their scoring. There is no mathematical explanation for the score given by human experts. This is the same for the LLM because the score is the result of a pattern-matching exercise with output sampling. You should be looking for this: Are the LLM and human experts in agreement?",
        "Embedding": []
      },
      {
        "Id": 43,
        "OriginalFileName": "Content\\05-testing-and-monitoring-prompts.md",
        "Content": "This command will run all tests except those marked with the \u0022LLM\u0022 category. You can switch the filter to run only the LLM-based tests.\r\n\r\nOur sample covers just one test to validate that a prompt generates a useful response. This one test isn\u0027t going to be enough, though, because the LLM could generate an inconsistent recipe or a very long response that\u0027s hard to read.\r\n\r\nTo validate the response\u0027s more complex properties, we\u0027ll use the LLM against itself. Let\u0027s look at how you can use model-based testing to validate the prompts\u0027 more complex properties.\r\n\r\n{#model-based-testing}\r\n## Using model-based testing methods to validate prompts\r\n\r\nSo far, we\u0027ve only tested for simple patterns in the output. We\u0027ve checked if the output contains specific keywords, and we could extend this to check for things like the number of items in a list or the number of words. But we can\u0027t check if instructions are consistent or if ingredients use the same kind of units of measurement.\r\n\r\nIt\u0027s typical for LLM-based applications to need more complex tests. You\u0027re automating something that solves a more complex problem, so you need more complex tests to check for correctness. And to be honest, despite your best efforts, the tests aren\u0027t going to cover all the problems that you\u0027ll run into.\r\n\r\nHowever, there\u0027s an interesting method for validating more complex properties of the response by using the LLM itself as an analysis tool. I prefer this method of testing model-based prompt testing.\r\n\r\nThere\u0027s a lot of research into model-based testing approaches for LLM-based applications. For example, there\u0027s [G-Eval][G_EVAL] that proposes a specific test prompt that scores the response on a single metric. Using G-Eval, you can, for example, check if the response is coherent or if it\u0027s consistent with the input. [GPTScore][GPTSCORE] follows a similar pattern where you employ a prompt with instructions to score a response on a specific aspect.\r\n\r\nGPTScore and G-Eval have a common strategy to them:\r\n\r\n1. First, you use the LLM to generate a response to the prompt with test data.\r\n2. Then, put the response into a test prompt with specific instructions to evaluate a single metric for the prompt. In the test instructions, you include a scale to measure the metric.\r\n3. Finally, you record the score generated by the LLM as the test result.\r\n\r\nWhen you run enough samples with this technique, you see how well the LLM handles your prompt. You can assert the score for a single sample and the average score for a set of samples. That\u0027s up to you.\r\n\r\nI recommend running multiple samples through the model and looking at the average score. Since LLMs are inherently indeterministic, so you can\u0027t rely on a single sample to give a helpful answer.\r\n\r\nLet\u0027s explore what a test prompt for the setup we just discussed looks like:\r\n\r\n\u0060\u0060\u0060text\r\n## Instructions \r\n\r\nYou will be given a recipe for a dish based on the user\u0027s ingredients in the fridge. Your task is to rate the\r\nrecipe on a single metric. \r\n\r\n## Recipe\r\n\r\n{{ recipe }}\r\n\r\n## Evaluation criteria\r\n\r\nConsistency (1-5): How consistent is the recipe with the\r\ningredients provided? Are the ingredients used in the recipe?\r\nAre the instructions logical?\r\n\r\n## Evaluation steps\r\n\r\n1. Read the recipe carefully and identify the ingredients.\r\n2. Read the instructions. Check if they are logical.\r\n3. Check if the instructions use the ingredients provided.\r\n3. Assign a score for the evaluation criteria in the evaluation form.\r\n\r\n## Evaluation form (scores ONLY)\r\n\u0060\u0060\u0060\r\n\r\nThe test prompt is quite complicated. Let\u0027s break it down a bit so you understand the general pattern behind it:\r\n\r\n1. In the first section of the prompt, we provide instructions for the task we are working on.\r\n2. In the second section, we\u0027re using the one-shot learning pattern to include the recipe initially generated.\r\n3. In the third section, we provide a chain-of-thought pattern to help produce a more exact response.",
        "Embedding": []
      }
    ]
  }
]