{#working-with-structured-output}
# Working with structured output

Building with LLMs means we have to embrace the instability of AI. You never get the same response and you frequently have to build extra glue code to parse the output of the LLM. Working with unstructured output from an LLM sure is a challenge, but there are ways to limit the chaos by forcing the LLM to generate output that other program code can parse.

In this chapter you'll learn two methods to obtain structured output from an LLM. First, we'll talk about working with JSON output, and after that we'll look at another technique that I like to call sideband communication. This method uses tools to get structured output in addition to regular chat-based output.

We'll cover the following topics in this chapter:

- Why working with structured output is helpful
- Applications that require structured output
- How does structured output work under the hood
- Getting structured output from the LLM
- Working with a sideband channel
- Limitations of structured output

Let's get started by discussing why you want structured output in the first place.

## Why working with structured output is helpful

Working with LLMs can be pure chaos when your applications get bigger. I've run into a lot of problems in particular with parsing the output generated by an LLM.

In a typical chat application you can pass the output from the LLM onto the frontend without doing much to it. So if you're building a chat bot, then you probably don't recognize the challenges of parsing LLM output into a structured format.

However, if you're building a workflow to automate a content related task, then processing the output into a structured format is one of the core operations in your application.

There are numerous ways to process LLM output into a structured format. It usually starts with instructions like: "Output the result in a table", after which you parse the table from the output using a regular expression. Another well-known trick is ask the LLM to output the content in fenced markdown code block. You can then parse the fenced markdown code block.

But what happens if the LLM decides not to generate any of those constructs that you can parse with a regular expression? Well, it breaks, and it's frustrating. Because this it's not the kind of thing that you want to deal with as a developer.

There is a solution though. Most LLMs these days allow you to specify a response format. And all of them support doing so by specifying a JSON schema. Using a JSON schema not only allows you to specify an object structure for the output, you can also describe what each property does so the LLM better understands how to translate the input into structured output.

And there's more good news, in many cases the LLM follows the output structure flawlessly. OpenAI achieves 100% accuracy in this regard. So you can almost blindly rely on this feature.

Eliminating the content parsing aspect of using an LLM reduces the amount of code in your application by a significant amount so you can focus on building that workflow you need to create for processing blog posts into linkedin posts or even more interesting translating a prompt into a structured user interface.

## Applications that require structured output

Speaking of applications that work better based on structured output, I've found that there are quite a few scenarios that benefit from structured output. Anything that involves multi-step workflows benefits from structured output for example.

One application that stands out to me is converting code from one language to another. I've worked on a couple of projects that involved turning old code into modern equivalents. For one client I had to translate some weird XML-based low-code solution into typescript and for another client a Ruby codebase into C#. Now this sounds like: You insert code into the LLM with some instructions and then it generates the desired code. Except that the LLM never does this. Instead it generates a reply similar to this: "Sure, I can help you convert this code. Type in A, then B, and finally C. Good luck!". Not exactly what I was looking for to be honest. But by forcing the LLM to generate the code as a property of a response object in JSON, I was able to reliably convert weird old code into nice looking modern code.

Another great use case involved generating [feature files][FEATURE_FILES] from a prompt. I wanted an application where I could enter a prompt that was translated by the LLM into a feature file. I then wanted to iterate on the results using a chat-based interface with the feature file to the side. The first portion of the application I solved by creating a prompt that returns a structured response just like I used in the code conversion pipeline. The second part of the application was a little more complicated. I used a non-existing function to implement a sideband. I declared a function with the LLM that allowed it to edit the feature file. Whenever the LLM returned a tool call result that should invoke the function, I grabbed the tool call and translate it into an event sent to the web frontend via websockets, updating the content in the feature file editor. The chat content would also be sent to the frontend, but to the chat portion of the screen.

I'm sure there are many more applications out there that benefit from structured output. But these two applications are prime examples showing you why having the option to request structured output is essential.

Now you may be wondering, how does the LLM do this?

## How does structured output work under the hood

In [#s](#llm-output-sampling) we talked about output sampling, and how LLMs use this to sound more natural. This goes into the face of generating structured output, because we can't just sample anything we like if we want valid JSON output. Depending on the kind of LLM you're using there are different ways in which models solve this problem.

### Instruction-based JSON generation

Some LLM providers like Claude and Gemini use instructions to help the LLM produce JSON output. If you tell the LLM to output JSON it will do so in some of the cases. It's not ideal at all.

Claude, for example, produces valid JSON output in 86% of the cases according [to a test by Datachain.ai][JSON_OUTPUT_TEST]. Gemini produces valid JSON output in 10% of the cases they tested. I wouldn't call this reliable enough to build an application with.

There is a way to fix this though. You can use tool calling like we discussed in [#s](#enhancing-llms-with-tools) to increase the odds of getting structured output. You can declare a tool with the LLM specifying a JSON schema of the output and then [force the LLM to call that tool][FORCE_TOOL_CALL]. The LLM will return a tool call result with the data structured according to the JSON schema you specified. It's nasty work, but it can be done.

I like how OpenAI solves this problem much more though.

### Constrained decoding based JSON generation

OpenAI LLMs including Azure OpenAI use constrained output decoding to solve the problem. Here's how that works.

Under normal circumstances when we're generating a response and we need to generate a token, we get to choose from the whole vocabulary subject to the Top-P setting and penalties. However, when applying constrained decoding we must overlay a grammar that marks only certain tokens from the vocabulary as valid. Only the valid tokens are passed through the sampling mechanism and will end up in the response.

There's a reason why OpenAI supports JSON but not much else for generating a structured response. This has to do with the limitations of the LLM and how some more complex grammars work.

Let me take a step into compiler building for the moment. When you're building a parser for a language like JSON or C#, you need to determine at any point in the source code what a character means and if it's valid.

For example, if we were parsing a JSON object we could say that the content should start with a opening bracket `{` followed by a space or double quotes `"`. After that, we expect anything except another double quote to set the name of the property. These constraints we call grammar rules, and you can formally define them to design a full language. The following fragment is a basic representation for a grammar used to parse JSON:

```text
JSON       → Object | Array
Object     → { Members } | { }
Members    → Pair | Pair , Members
Pair       → "string" : Value
Array      → [ Elements ] | [ ]
Elements   → Value | Value , Elements
Value      → "string" 
           | number 
           | true 
           | false 
           | null 
           | Object 
           | Array
```

If you're unfamiliar with grammar rules in general, here's how to interpret the rules. JSON is either an object or an array. An object has members (properties) or is empty.

The members rule is a bit harder to read here. Members is either a single pair of key and value or a key-value pair, followed by more members. So you can have one property in a JSON object or multiple.

A pair has a string to identity the pair and a value. The value could be a string, a number, true, false, null, or another object or array.

Notice how basic these rules are, this is because this language is context-free. We don't need to know the content of the values to determine what rule to apply for parsing basic JSON.

Languages like C# are much more complicated and often require context to work, so they're unusable for an LLM, because it can't look ahead in the response to find out the context needed for each grammar rule.

Constrained output decoding makes it so that LLMs are 100% accurate in following the structure of a JSON schema. But it doesn't mean it won't output garbage in a perfectly valid structure. We're still using AI.

## Getting structured output from the LLM

To get structured output from an OpenAI-based LLM we need to specify the output format using a JSON schema. Let's start by looking at what a JSON schema is and then move on to Semantic Kernel to understand how to configure it to render structured output.

A JSON schema follows a specific structure specified in the [JSON schema standard][JSON_SCHEMA_STANDARD]. The following fragment shows a basic schema:

```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://my-schemas.org/question-generation.schema.json",
  "title": "FeatureFileGenerationResult",
  "description": "A feature file",
  "type": "object",
  "properties": {
    "content": {
        "type": "string"
    }
  },
  "required": ["content"]
}
```

This schema defines an object with a content property. It's not that complicated, but JSON schemas support a lot of different constructions ranging from simple arrays to type unions. It's not the easiest format to work with.

As luck would have it, you don't have to use it when you're working with Semantic Kernel. In Semantic Kernel you can specify a type as the output format for a prompt like so:

```csharp

```

In the prompt execution settings we'll use a `typeof` statement to point Semantic Kernel to the C# output we expect to get from the LLM. Under the covers, the object type is parsed to a JSON schema and sent to the LLM as the expected output format.

A> You can also load a JSON schema from disk and let Semantic Kernel use that, but I found that there's no good way to deserialize the data to a C# class if you do that. Unless of course you generate a C# class from the schema. But it feels like duplication of work. That's why I left it out. If you are interested in this functionality, you can find more about it [in this blogpost][JSON_SCHEMA_POST].

Keep in mind that you can only specify objects as the response format. LLMs don't support arrays as output format and will raise an error if you try to specify an array or list. If you do need a list of items as output, you're going to have to nest it as a a property of the output object.

It's important to tell the LLM that you expect JSON output, otherwise the API will generate an error. It's helpful to include one or two samples in the prompt as well to help the LLM what data should go where in the JSON structure. I find this especially helpful when creating more complicated output structures. For simple outputs I skip this step.

Working with structured output has its limitations. Asking for JSON schema-based output incurs extra latency the first time you submit a request with a new schema. This is because the LLM provider needs to compile the schema down to a grammar to overlay.

The JSON schema is cached on the server, while it usually doesn't contain sensitive business data, it is something you should be aware off. Even if you have a zero storage agreement with OpenAI, this piece of information remains behind.

I should also mention that while the structure of your JSON schema is always followed by the model, the content could be wrong. If you run into issues with invalid values for properties of your output object, I recommend adding samples to the prompt to help the LLM put the right information into the right spot in the response.

I wish that every LLM provider supported the constrained output decoding technique, but they don't. So you may have to resort to other techniques like the one we're discussing in the next section.

## Working with a sideband channel

In the previous section we covered how to get structured output with OpenAI-based models by setting the output format. The output format technique offered by Semantic Kernel doesn't work for models like Gemini and Claude, because they don't allow you to force the LLM into JSON mode like you can with OpenAI.

However, there's another trick that involves tool calling that can help in this regard. Using tool calling to get structured output helps in two scenarios:

1. The LLM support tool calling, but doesn't support constrained output decoding.
2. You want to combine chat with structured output.

Let's explore both scenarios as they require a slightly different approach to using tools with the LLM.

### Using tool calling to get structured output

We can get structured output from an LLM by forcing it to use a tool. It takes two steps to implement this:

1. First, we need to create a tool that only defines metadata. We don't need an implementation for this scenario as we're not truly calling the tool.
2. Next, we need to configure Semantic Kernel to return tool call results instead of calling the function we provide it.

Let's start by setting up a fake tool that defines the structure of the expected output.

## Summary

[FEATURE_FILES]: https://cucumber.io/docs/gherkin/reference/
[JSON_SCHEMA_STANDARD]: https://json-schema.org/
[JSON_OUTPUT_TEST]: https://datachain.ai/blog/enforcing-json-outputs-in-commercial-llms
[FORCE_TOOL_CALL]: https://docs.anthropic.com/en/docs/build-with-claude/tool-use/overview#forcing-tool-use
[JSON_SCHEMA_POST]: https://devblogs.microsoft.com/semantic-kernel/using-json-schema-for-structured-output-in-net-for-openai-models/