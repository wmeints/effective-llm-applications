{#working-with-structured-output}
# Working with structured output

Building with LLMs means we have to embrace the instability of AI. You never get the same response and you frequently have to build extra glue code to parse the output of the LLM. Working with unstructured output from an LLM sure is a challenge, but there are ways to limit the chaos by forcing the LLM to generate output that other program code can parse.

In this chapter you'll learn two methods to obtain structured output from an LLM. First, we'll talk about working with JSON output, and after that we'll look at another technique that I like to call sideband communication. This method uses tools to get structured output in addition to regular chat-based output.

We'll cover the following topics in this chapter:

- Why working with structured output is helpful
- Applications that require structured output
- How does structured output work under the hood
- Getting structured output from the LLM
- Working with a sideband channel
- Limitations of structured output

Let's get started by discussing why you want structured output in the first place.

## Why working with structured output is helpful

Working with LLMs can be pure chaos when your applications get bigger. I've run into a lot of problems in particular with parsing the output generated by an LLM.

In a typical chat application you can pass the output from the LLM onto the frontend without doing much to it. So if you're building a chat bot, then you probably don't recognize the challenges of parsing LLM output into a structured format.

However, if you're building a workflow to automate a content related task, then processing the output into a structured format is one of the core operations in your application.

There are numerous ways to process LLM output into a structured format. It usually starts with instructions like: "Output the result in a table", after which you parse the table from the output using a regular expression. Another well-known trick is ask the LLM to output the content in fenced markdown code block. You can then parse the fenced markdown code block.

But what happens if the LLM decides not to generate any of those constructs that you can parse with a regular expression? Well, it breaks, and it's frustrating. Because this it's not the kind of thing that you want to deal with as a developer.

There is a solution though. Most LLMs these days allow you to specify a response format. And all of them support doing so by specifying a JSON schema. Using a JSON schema not only allows you to specify an object structure for the output, you can also describe what each property does so the LLM better understands how to translate the input into structured output.

And there's more good news, depending on how the LLM provider implemented structured output generation the the LLM follows the output structure flawlessly. OpenAI achieves 100% accuracy in this regard. So you can almost blindly rely on this feature.

Eliminating the content parsing aspect of using an LLM reduces the amount of code in your application by a significant amount so you can focus on building that workflow you need to create for processing blog posts into linkedin posts or even more interesting translating a prompt into a structured user interface.

## Applications that require structured output

Speaking of applications that work better based on structured output, I've found that there are quite a few scenarios that benefit from structured output. Anything that involves multi-step workflows benefits from structured output for example.

One application that stands out to me is converting code from one language to another. I've worked on a couple of projects that involved turning old code into modern equivalents. For one client I had to translate some weird XML-based low-code solution into typescript and for another client a Ruby codebase into C#. Now this sounds like: You insert code into the LLM with some instructions and then it generates the desired code. Except that the LLM never does this. Instead it generates a reply similar to this: "Sure, I can help you convert this code. Type in A, then B, and finally C. Good luck!". Not exactly what I was looking for to be honest. But by forcing the LLM to generate the code as a property of a response object in JSON, I was able to reliably convert weird old code into nice looking modern code.

Another great use case involved generating [feature files][FEATURE_FILES] from a prompt. I wanted an application where I could enter a prompt that was translated by the LLM into a feature file. I then wanted to iterate on the results using a chat-based interface with the feature file to the side. The first portion of the application I solved by creating a prompt that returns a structured response just like I used in the code conversion pipeline. The second part of the application was a little more complicated. I used a non-existing function to implement a sideband. I declared a function with the LLM that allowed it to edit the feature file. Whenever the LLM returned a tool call result that should invoke the function, I grabbed the tool call and translate it into an event sent to the web frontend via websockets, updating the content in the feature file editor. The chat content would also be sent to the frontend, but to the chat portion of the screen.

I'm sure there are many more applications out there that benefit from structured output. But these two applications are prime examples showing you why having the option to request structured output is essential.

Now you may be wondering, how does the LLM do this?

## How does structured output work under the hood

In [#s](#llm-output-sampling) we talked about output sampling, and how LLMs use this to sound more natural. This goes directly against generating structured output, because we can't just sample anything we like if we want valid JSON output. Depending on the kind of LLM you're using there are different ways in which models solve this problem.

### Instruction-based JSON generation

Some LLM providers like Claude only support JSON based structured output by using instructions. If you tell the LLM to output JSON it will do so in most of the cases. It's not ideal though.

Claude, for example, produces valid JSON output in 86% of the cases according [to a test by Datachain.ai][JSON_OUTPUT_TEST]. This percentage is not high enough to be reliable.

There is a way to fix this though. You can use tool calling like we discussed in [#s](#enhancing-llms-with-tools) to increase the odds of getting structured output. You can declare a tool with the LLM specifying a JSON schema of the output and then [force the LLM to call that tool][FORCE_TOOL_CALL]. The LLM will return a tool call result with the data structured according to the JSON schema you specified. It's nasty work, but it can be done. I'll show this in greater detail later in the chapter.

I like how OpenAI solves this problem much more though.

### Constrained decoding based JSON generation

Some LLMs including the [OpenAI based LLMs][OPENAI_JSON_SUPPORT] and [Google Gemini][GEMINI_JSON_SUPPORT] use constrained output decoding to solve the problem. Here's how that works.

Under normal circumstances when we're generating a response and we need to generate a token, we get to choose from the whole vocabulary subject to the Top-P setting and penalties. However, when applying constrained decoding we must overlay a grammar that marks only certain tokens from the vocabulary as valid. Only the valid tokens are passed through the sampling mechanism and will end up in the response.

There's a reason why OpenAI supports JSON but not much else for generating a structured response. This has to do with the limitations of the LLM and how some more complex grammars work.

Let me take a step into compiler building for the moment. When you're building a parser for a language like JSON or C#, you need to determine at any point in the source code what a character means and if it's valid.

For example, if we were parsing a JSON object we could say that the content should start with a opening bracket `{` followed by a space or double quotes `"`. After that, we expect anything except another double quote to set the name of the property. These constraints we call grammar rules, and you can formally define them to design a full language. The following fragment is a basic representation for a grammar used to parse JSON:

```text
JSON       → Object | Array
Object     → { Members } | { }
Members    → Pair | Pair , Members
Pair       → "string" : Value
Array      → [ Elements ] | [ ]
Elements   → Value | Value , Elements
Value      → "string" 
           | number 
           | true 
           | false 
           | null 
           | Object 
           | Array
```

If you're unfamiliar with grammar rules in general, here's how to interpret the rules. JSON is either an object or an array. An object has members (properties) or is empty.

The members rule is a bit harder to read here. Members is either a single pair of key and value or a key-value pair, followed by more members. So you can have one property in a JSON object or multiple.

A pair has a string to identity the pair and a value. The value could be a string, a number, true, false, null, or another object or array.

Notice how basic these rules are, this is because this language is context-free. We don't need to know the content of the values to determine what rule to apply for parsing basic JSON.

Languages like C# are much more complicated and often require context to work, so they're unusable for an LLM, because it can't look ahead in the response to find out the context needed for each grammar rule.

Constrained output decoding makes it so that LLMs are 100% accurate in following the structure of a JSON schema. But it doesn't mean it won't output garbage in a perfectly valid structure. We're still using AI.

## Getting structured output from the LLM

To get structured output from an LLM we need to specify the output format using a JSON schema. Let's start by looking at what a JSON schema is and then move on to Semantic Kernel to understand how to configure it to render structured output.

A JSON schema follows a specific structure specified in the [JSON schema standard][JSON_SCHEMA_STANDARD]. The following fragment shows a basic schema:

```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://my-schemas.org/question-generation.schema.json",
  "title": "FeatureFileGenerationResult",
  "description": "A feature file",
  "type": "object",
  "properties": {
    "content": {
        "type": "string"
    }
  },
  "required": ["content"]
}
```

This schema defines an object with a content property. It's not that complicated, but JSON schemas support a lot of different constructions ranging from simple arrays to type unions. It's not the easiest format to work with.

As luck would have it, you don't have to use it when you're working with Semantic Kernel. In Semantic Kernel you can specify a type as the output format for a prompt like so:

```csharp
var settings = new AzureOpenAIPromptExecutionSettings
{
    ResponseFormat = typeof(ScenarioResult)
};

var response = await kernel.InvokePromptAsync(
    "Generate a scenario with given, when, then statements for the " +
    "following user story: As I user I want to be able to chat to " +
    "customer support",
    new KernelArguments(settings)
);
```

In the prompt execution settings we'll use a `typeof` statement to point Semantic Kernel to the C# output we expect to get from the LLM. Under the covers, the object type is parsed to a JSON schema and sent to the LLM as the expected output format.

Note that for Google Gemini models you need to specify a different prompt execution settings object that looks like this:

```csharp
var settings = new GeminiPromptExecutionSettings
{
    ResponseMimeType = "application/json",
    ResponseSchema = typeof(ScenarioResult)
};

var response = await kernel.InvokePromptAsync(
    "Generate a scenario with given, when, then statements for the " +
    "following user story: As I user I want to be able to chat to " +
    "customer support",
    new KernelArguments(settings)
);
```

It has the same structure as the format specifier for the OpenAI style models in the previous sample.

A> You can also load a JSON schema from disk and let Semantic Kernel use that, but I found that there's no good way to deserialize the data to a C# class if you do that. Unless of course you generate a C# class from the schema. But it feels like duplication of work. That's why I left it out. If you are interested in this functionality, you can find more about it [in this blogpost][JSON_SCHEMA_POST].

Keep in mind that you can only specify objects as the response format. None of the available LLMs don't support arrays as output format and will raise an error if you try to specify an array or list. If you do need a list of items as output, you're going to have to nest it as a a property of the output object.

It's important to tell the LLM that you expect JSON output, otherwise the API will generate an error. It's helpful to include one or two samples in the prompt as well to help the LLM what data should go where in the JSON structure. I find this especially helpful when creating more complicated output structures. For simple outputs I skip this step.

Working with structured output has its limitations. Asking for JSON schema-based output incurs extra latency the first time you submit a request with a new schema. This is because the LLM provider needs to compile the schema down to a grammar to overlay.

The JSON schema is cached on the server, while it usually doesn't contain sensitive business data, it is something you should be aware off. Even if you have a zero storage agreement with OpenAI, this piece of information remains behind. For Google the same rule applies. They cache the JSON schema as well.

I should also mention that while the structure of your JSON schema is always followed by the model, the content could be wrong. If you run into issues with invalid values for properties of your output object, I recommend adding samples to the prompt to help the LLM put the right information into the right spot in the response.

I wish that every LLM provider supported the constrained output decoding technique, but they don't. So you may have to resort to other techniques like the one we're discussing in the next section.

## Working with a sideband channel

In the previous section we covered how to get structured output with OpenAI-based models by setting the output format. The output format technique offered by Semantic Kernel doesn't work for models like Gemini and Claude, because they don't allow you to force the LLM into JSON mode like you can with OpenAI.

However, there's another trick that involves tool calling that can help in this regard. Using tool calling to get structured output helps in two scenarios:

1. The LLM support tool calling, but doesn't support constrained output decoding.
2. You want to combine chat with structured output.

Let's explore both scenarios as they require a slightly different approach to using tools with the LLM.

### Using tool calling to get structured output

We can get structured output from an LLM by forcing it to use a tool. It takes two steps to implement this:

1. First, we need to create a tool that only defines metadata. We don't need an implementation for this scenario as we're not truly calling the tool.
2. Next, we need to configure Semantic Kernel to return tool call results instead of calling the function we provide it.

To demonstrate how to build this setup, we'll use the same sample as before, we're generating a user story using AI. Let's start by setting up a tool that defines the structure of the expected output.

```csharp
public class OutputTool
{
    public string Title { get; set; } = string.Empty;
    public List<string> Steps { get; set; } = new();

    [KernelFunction, Description("Store the created user story")]
    public void CreateUserStory(string title, List<string> steps)
    {
        Title = title;
        Steps = steps;
    }
}
```

In this code we define a tool called `CreateUserStory` that accepts a title, and a list of steps. We're not really creating a story here, we're just storing the values generated by the LLM.

To use the tool, we'll need to set up the prompt invocation in a specific way:

```csharp
var outputTool = new OutputTool();
var outputToolPlugin = kernel.Plugins.AddFromObject(outputTool);

var settings = new AzureOpenAIPromptExecutionSettings
{
    FunctionChoiceBehavior = FunctionChoiceBehavior.Required(
        [outputToolPlugin.First(x => x.Name == "CreateUserStory")])
};

var response = await kernel.InvokePromptAsync(
    "Generate a scenario with given, when, then statements for the " +
    "following user story: As I user I want to be able to chat to " +
    "customer support",
    new KernelArguments(settings)
);

Console.WriteLine(outputTool.Title);

foreach (var step in outputTool.Steps)
{
    Console.WriteLine(step);
}
```

In the code we perform the following steps to use the tool to get structured output:

1. First, we create a new instance of the output tool and turn it into a plugin.
2. Next, we configure the prompt execution settings and force the LLM to use our tool.
3. Finally, we invoke a prompt to generate a user story.

The LLM is required to call a tool, and will invoke the output tool with the contents of the user story. To effectively capture the structured output we'll need to inject the tool instance so it's easily accessible.

Note that by forcing the LLM to always call a tool in Semantic Kernel, it disables the kernel function calling loop we discussed in [#s](#llm-function-calling). This isn't a big problem if you're building a workflow though. Just make sure you don't need to invoke multiple tools in the same call.

This base idea of using tools to get structured output can be extended towards a more complex sideband communication scenario.

### Integrating sideband communication in chat scenarios

If you've used tools like [v0.dev][V0_DEV] and [bolt.new][BOLT_NEW] you're familiar with how AI tools combine chatting to a user with generating content like source code. Often you'll see the application respond to your request in a chat box while generating code on the side of the screen. There's no official name for this style of communication, but I like sideband communication as a nice way to describe this way of working.

Application like v0 use this style of communication to make the use of tools feel more interactive. It looks nice but it's also a powerful way to make sure the LLM outputs usable structured data when it comes to generating content.

You can use this style of communication too in a similar fashion to how we used tools to force the LLM to output structured content. Except this time we can set the function choice behavior for the application to automatic.

```csharp
var outputTool = new OutputTool();
var outputToolPlugin = kernel.Plugins.AddFromObject(outputTool);

var settings = new AzureOpenAIPromptExecutionSettings
{
    FunctionChoiceBehavior = FunctionChoiceBehavior.Auto()
};

var response = await kernel.InvokePromptAsync(
    "Generate a scenario with given, when, then statements for the " +
    "following user story: As I user I want to be able to chat to " +
    "customer support",
    new KernelArguments(settings)
);
```

To get the content from the tool to the frontend you'll need to communicate changes to the frontend the user is using. For web application I recommend looking at [SignalR][SIGNALR_DOCS]. It allows you to build a hub that can push messages to a Javascript frontend. I won't go into much detail on this topic, but here's an example demonstrating how to build a hub for generating user story content in the frontend:

```csharp
public class UserStoryGenerationHub : Hub<IUserStoryGenerationHubClient>
{
    public async Task JoinEditingSessionAsync(string sessionId)
    {
        await Groups.AddToGroupAsync(Context.ConnectionId, sessionId);
    }

    public async Task LeaveEditingSessionAsync(string sessionId)
    {
        await Groups.RemoveFromGroupAsync(Context.ConnectionId, sessionId);
    }

    public async Task UpdateUserStoryContent(
        string sessionId, UserStoryContent userStoryContent)
    {
        var group = Clients.Group(sessionId);
        await group.UpdateUserStoryContent(userStoryContent);
    }
}

public interface IUserStoryGenerationHubClient
{
    public Task UpdateUserStoryContent(UserStoryContent userStoryContent);
}
```

In this code we create a hub that has the following code:

1. First, we declare a new hub class that has an interface called `IUserStoryGenerationHubClient`. This defines the methods that a client needs to handle.
2. Next, we'll allow clients to join and leave editing sessions so we know where to send the update for the user story.
3. Finally, we have a method that allows the application to send updates to the current editing session with the generated content.

We need to modify the tool to use the hub instead of storing the content in properties in the tool class. The following code demonstrates what the tool looks like when you connect it to a SignalR hub:

```csharp
public class OutputTool(IHubContext<
    UserStoryGenerationHub, IUserStoryGenerationHubClient> hub, 
    string clientId)
{
    [KernelFunction, Description("Store the created user story")]
    public async Task CreateUserStory(string title, List<string> steps)
    {
        UserStoryContent userStoryContent = new()
        {
            Title = title,
            Steps = steps
        };

        await hub.Clients
            .Group(clientId)
            .UpdateUserStoryContent(userStoryContent);
    }
}
```

In the tool we made the following changes:

1. First, we inject the hub context for the hub we created.
2. Next, we create a new content object that we stream to the clients by calling the `UpdateUserStoryContent` method. We're using a group to make sure that the content is only streamed to relevant clients.

We're using the groups functionality in SignalR as a way to limit communication to only one or two clients. In production I recommend making sure you add authorization and allow clients to join a group only when they're the owner of a particular conversation (or user story in this scenario). You can find more information about this [in the manual][SIGNALR_AUTH].

To use the hub we created, we must register it in the application startup using the following code:

```csharp
var builder = WebApplication.CreateBuilder(args);

builder.Services.AddSignalR();

var app = builder.Build();

app.MapHub<UserStoryGenerationHub>("/hubs/userstories");

app.Run();
```

First, we need to setup the required components for SignalR after which we can register the hub in the application. I've left out the other configuration code here needed to set up the kernel and the output tool. Let's look at how you can connect the hub to the Semantic Kernel code.

The following code shows a basic chatbot class without conversation history that can respond to user prompts.

```csharp
public class UserStoryGenerationAgent(IHubContext<
    UserStoryGenerationHub, IUserStoryGenerationHubClient> hub, 
    Kernel kernel)
{
    public async Task<string> GenerateResponseAsync(
        string sessionId, string prompt)
    {
        var outputTool = new OutputTool(hub, sessionId);
        var outputToolPlugin = kernel.Plugins.AddFromObject(outputTool);

        var settings = new AzureOpenAIPromptExecutionSettings
        {
            FunctionChoiceBehavior = FunctionChoiceBehavior.Auto()
        };

        var response = await kernel.InvokePromptAsync(
            prompt,
            new KernelArguments(settings)
        );

        // You could return a streaming response, the user 
        // story content is already pushed out to the client 
        // via the hub at this point.

        return response.GetValue<string>()!;
    }
}
```

In this code we perform the following steps:

1. First, we create a class that hooks up the kernel and the hub.
2. Next, we create a method to generate a response, this method requires a session identifier for the editing session we're in and a prompt from the user. Both come from the frontend application.
3. Then, we connect the output tool to the hub with the session identifier.
4. After, we configure the LLM to allow the use of the configured output tool.
5. Next, we generate a response and return it to the caller.

I've gone for a non-streaming response, but you can certainly do that using the code we wrote in [#s](#working-with-chat-completion).

To use the agent in the application we need to use the following code:

```csharp
var builder = WebApplication.CreateBuilder(args);

builder.Services.AddSignalR();

builder.Services.AddKernel()
    .AddAzureOpenAIChatCompletion(
        deploymentName: builder.Configuration["LanguageModel:DeploymentName"]!,
        endpoint: builder.Configuration["LanguageModel:Endpoint"]!,
        apiKey: builder.Configuration["LanguageModel:ApiKey"]!
    );

builder.Services.AddTransient<UserStoryGenerationAgent>();

var app = builder.Build();

app.MapHub<UserStoryGenerationHub>("/hubs/userstories");

app.MapPost("/sessions/{sessionId}/", async (
    string sessionId,
    [FromBody] GenerateUserStoryResponseForm form,
    [FromServices] UserStoryGenerationAgent agent
) => await agent.GenerateResponseAsync(sessionId, form.Prompt));

app.Run();
```

This code looks similar to how we configured the application before. I've added a couple of extra statements:

1. First, I added the kernel initialization code to connect the application with Azure OpenAI.
2. Then, I configured the agent class that we created as a transient dependency. I want a new one every time we have to handle a request.
3. Finally, I created a POST operation that allows the user to submit prompts.

I've left out the join and leave operations for now. You can find these operations in the sample code on [GitHub][GH_SAMPLE_CODE].

Sideband communication makes for some interesting interaction patterns in applications. I love this pattern for chat applications, but wouldn't use it for workflow-based scenarios. You need a lot more infrastructure for sideband communication to work in production especially when your user base grows beyond a few hundred users.

## Summary

In this chapter we discussed two main patterns to generate structured output from an LLM. First we looked at using constrained output decoding to force the LLM to generate valid JSON content. Then we looked at how we can use tool calling as a substitute in case you don't have access to an LLM that supports constrained output decoding.

After looking at the basic patterns for generating structured output, we looked at using sideband communication for generating structured output in chat scenarios.

In the next chapter, we'll look at prompt chaining as way to combine multiple prompts to generate more refined output. We'll use all the techniques we learned in this chapter and previous chapters to build a more complex scenario that involves processing documents into a single summary.

[FEATURE_FILES]: https://cucumber.io/docs/gherkin/reference/
[JSON_SCHEMA_STANDARD]: https://json-schema.org/
[JSON_OUTPUT_TEST]: https://datachain.ai/blog/enforcing-json-outputs-in-commercial-llms
[FORCE_TOOL_CALL]: https://docs.anthropic.com/en/docs/build-with-claude/tool-use/overview#forcing-tool-use
[JSON_SCHEMA_POST]: https://devblogs.microsoft.com/semantic-kernel/using-json-schema-for-structured-output-in-net-for-openai-models/
[GEMINI_JSON_SUPPORT]: https://ai.google.dev/gemini-api/docs/structured-output?lang=python
[OPENAI_JSON_SUPPORT]: https://openai.com/index/introducing-structured-outputs-in-the-api/
[V0_DEV]: https://v0.dev/
[BOLT_NEW]: https://bolt.new
[SIGNALR_DOCS]: https://dotnet.microsoft.com/en-us/apps/aspnet/signalr
[GH_SAMPLE_CODE]: https://github.com/wmeints/effective-llm-applications/tree/publish/samples/chapter-08/csharp/Chapter8.SidebandCommunication/
[SIGNALR_AUTH]: https://learn.microsoft.com/en-us/aspnet/core/signalr/authn-and-authz?view=aspnetcore-9.0