{#working-with-structured-output}
# Working with structured output

Building with LLMs means we have to embrace the instability of AI. You never get the same response and you frequently have to build extra glue code to parse the output of the LLM. Working with unstructured output from an LLM sure is a challenge, but there are ways to limit the chaos by forcing the LLM to generate output that other program code can parse.

In this chapter you'll learn two methods to obtain structured output from an LLM. First, we'll talk about working with JSON output, and after that we'll look at another technique that I like to call sideband communication. This method uses tools to get structured output in addition to regular chat-based output.

We'll cover the following topics in this chapter:

- Why working with structured output is helpful
- Applications that require structured output
- How does structured output work under the hood
- Getting structured output from the LLM
- Working with a sideband channel
- Limitations of structured output

Let's get started by discussing why you want structured output in the first place.

## Why working with structured output is helpful

Working with LLMs can be pure chaos when your applications get bigger. I've run into a lot of problems in particular with parsing the output generated by an LLM.

In a typical chat application you can pass the output from the LLM onto the frontend without doing much to it. So if you're building a chat bot, then you probably don't recognize the challenges of parsing LLM output into a structured format.

However, if you're building a workflow to automate a content related task, then processing the output into a structured format is one of the core operations in your application.

There are numerous ways to process LLM output into a structured format. It usually starts with instructions like: "Output the result in a table", after which you parse the table from the output using a regular expression. Another well-known trick is ask the LLM to output the content in fenced markdown code block. You can then parse the fenced markdown code block.

But what happens if the LLM decides not to generate any of those constructs that you can parse with a regular expression? Well, it breaks, and it's frustrating. Because this it's not the kind of thing that you want to deal with as a developer.

There is a solution though. Most LLMs these days allow you to specify a response format. And all of them support doing so by specifying a JSON schema. Using a JSON schema not only allows you to specify an object structure for the output, you can also describe what each property does so the LLM better understands how to translate the input into structured output.

And there's more good news, in many cases the LLM follows the output structure flawlessly. OpenAI achieves 100% accuracy in this regard. So you can almost blindly rely on this feature.

Eliminating the content parsing aspect of using an LLM reduces the amount of code in your application by a significant amount so you can focus on building that workflow you need to create for processing blog posts into linkedin posts or even more interesting translating a prompt into a structured user interface.

## Applications that require structured output

Speaking of applications that work better based on structured output, I've found that there are quite a few scenarios that benefit from structured output. Anything that involves multi-step workflows benefits from structured output for example.

One application that stands out to me is converting code from one language to another. I've worked on a couple of projects that involved turning old code into modern equivalents. For one client I had to translate some weird XML-based low-code solution into typescript and for another client a Ruby codebase into C#. Now this sounds like: You insert code into the LLM with some instructions and then it generates the desired code. Except that the LLM never does this. Instead it generates a reply similar to this: "Sure, I can help you convert this code. Type in A, then B, and finally C. Good luck!". Not exactly what I was looking for to be honest. But by forcing the LLM to generate the code as a property of a response object in JSON, I was able to reliably convert weird old code into nice looking modern code.

Another great use case involved generating [feature files][FEATURE_FILES] from a prompt. I wanted an application where I could enter a prompt that was translated by the LLM into a feature file. I then wanted to iterate on the results using a chat-based interface with the feature file to the side. The first portion of the application I solved by creating a prompt that returns a structured response just like I used in the code conversion pipeline. The second part of the application was a little more complicated. I used a non-existing function to implement a sideband. I declared a function with the LLM that allowed it to edit the feature file. Whenever the LLM returned a tool call result that should invoke the function, I grabbed the tool call and translate it into an event sent to the web frontend via websockets, updating the content in the feature file editor. The chat content would also be sent to the frontend, but to the chat portion of the screen.

I'm sure there are many more applications out there that benefit from structured output. But these two applications are prime examples showing you why having the option to request structured output is essential.

Now you may be wondering, how does the LLM do this?

## How does structured output work under the hood

In [#s](#llm-output-sampling) we talked about output sampling, and how LLMs use this to sound more natural. This goes into the face of generating structured output, because we can't just sample anything we like if we want valid JSON output. LLMs use constrained output decoding to solve the problem. Here's how that works.

Under normal circumstances when we're generating a response and we need to generate a token, we get to choose from the whole vocabulary subject to the Top-P setting and penalties. However, when applying constrained decoding we must overlay a grammar that marks only certain tokens from the vocabulary as valid. Only the valid tokens are passed through the sampling mechanism and will end up in the response.

There's a reason why all LLM providers support JSON but not much else for generating a structured response. This has to do with the limitations of the LLM and how some more complex grammars work.

Let me take a step into compiler building for the moment. When you're building a parser for a language like JSON or C#, you need to determine at any point in the source code what a character means and if it's valid.

For example, if we were parsing a JSON object we could say that the content should start with a opening bracket `{` followed by a space or double quotes `"`. After that, we expect anything except another double quote to set the name of the property. These constraints we call grammar rules, and you can formally define them to design a full language. The following fragment is a basic representation for a grammar used to parse JSON:

```text
JSON       → Object | Array
Object     → { Members } | { }
Members    → Pair | Pair , Members
Pair       → "string" : Value
Array      → [ Elements ] | [ ]
Elements   → Value | Value , Elements
Value      → "string" 
           | number 
           | true 
           | false 
           | null 
           | Object 
           | Array
```

If you're unfamiliar with grammar rules in general, here's how to interpret the rules. JSON is either an object or an array. An object has members (properties) or is empty.

The members rule is a bit harder to read here. Members is either a single pair of key and value or a key-value pair, followed by more members. So you can have one property in a JSON object or multiple.

A pair has a string to identity the pair and a value. The value could be a string, a number, true, false, null, or another object or array.

Notice how basic these rules are, this is because this language is context-free. We don't need to know the content of the values to determine what rule to apply for parsing basic JSON.

Languages like C# are much more complicated and often require context to work, so they're unusable for an LLM, because it can't look ahead in the response to find out the context needed for each grammar rule.

Constrained output decoding makes it so that LLMs are 100% accurate in following the structure of a JSON schema. But it doesn't mean it won't output garbage in a perfectly valid structure. We're still using AI.

## Getting structured output from the LLM

To get structured output from an LLM we need to specify the output format using a JSON schema. Let's start by looking at what a JSON schema is and then move on to Semantic Kernel to understand how to configure it to render structured output.

A JSON schema follows a specific structure specified in the [JSON schema standard][JSON_SCHEMA_STANDARD]. The following fragment shows a basic schema:

```json
{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://my-schemas.org/question-generation.schema.json",
  "title": "FeatureFileGenerationResult",
  "description": "A feature file",
  "type": "object",
  "properties": {
    "content": {
        "type": "string"
    }
  },
  "required": ["content"]
}

This schema defines an object with a content property. It's not that complicated, but JSON schemas support a lot of different constructions ranging from simple arrays to type unions. It's not the easiest format to work with.

As luck would have it, you don't have to use it when you're working with Semantic Kernel. In Semantic Kernel you can specify a type as the output format for a prompt like so:

```csharp

```

In the prompt execution settings we'll use a `typeof` statement to point Semantic Kernel to the C# output we expect to get from the LLM. Under the covers, the object type is parsed to a JSON schema and sent to the LLM as the expected output format.

Keep in mind that you can only specify objects as the response format. LLMs don't support arrays as output format and will raise an error if you try to specify an array or list. If you do need a list of items as output, you're going to have to nest it as a a property of the output object.

Some LLMs don't automatically generate JSON output unless you tell it to. GPT-4o is known for this. If you want structured output from GPT-4o or any other OpenAI model, you'll have to say so in the prompt.



## Working with a sideband channel

## Limitations of structured output

## Summary

[FEATURE_FILES]: https://cucumber.io/docs/gherkin/reference/
[JSON_SCHEMA_STANDARD]: https://json-schema.org/